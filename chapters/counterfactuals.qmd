---
engine: julia
---

```{julia}
#| echo: false
#| output: false
_code_path = joinpath(dirname(Base.active_project()), "code")
include(joinpath(_code_path, "pkg.jl"))
include(joinpath(_code_path, "minisdm/pipelines.jl"))
include(joinpath(_code_path, "minisdm/counterfactuals.jl"))

JLD2.jldopen(joinpath(_models_path, "sdm-training-data.jld2"), "r") do file
    global X = file["X"]
    global y = file["y"]
    global folds = file["folds"]
    global coordinates = file["coordinates"]
end;

# Load the model
modelpath = joinpath(_models_path, "sdm-baseline.json")
model = SDM(ZScore(), NBC(), 0.5, X, y, [1,12])
train!(model; threshold=true)
```

# Generating counterfactuals

In @sec-explanations, we have introduced a methodology to re-use our model on mock inputs in order to figure out the effect of the specific value of a specific variable on the prediction that is made. This is a powerful approach to formulating explanations. In this chapter, we will introduce a related concept, namely the generation of counterfactuals.

Counterfactuals [@wachter2017] are defined as small perturbations on an actual input instance that yield the *opposite* prediction. In other words, counterfactuals are mock instances that are as close as possible to an observed data point, but that would lead the model to making the inverse recommendation.

```{julia}
#| echo: false
#| output: false
i = rand(findall(!, y))
cnd = X[:,i]
while predict(model, cnd)
    i = rand(findall(!, y))
    cnd = X[:,i]
end
pscore = predict(model, cnd; threshold=false)
```

## Illustration: changing the temperature

Before introducing some elements of theory, it helps to think about what we want to achieve in a more general, intuitive way. Going all the way back to the model of @sec-classification (a NBC with no data transformation, using BIO1 and BIO12 to make predictions), we can input any pair of temperature and precipitation, and get a probability that the site is suitable to the species.

::: column-margin
Note that for this chapter, we have also re-trained the model to optimize its threshold, as opposed to using 0.5 as in @sec-classification.
:::

For example, with a temperature of `{julia} 0.1cnd[1]` degrees, and a precipitation volume of `{julia} cnd[12]`, the model would give a probability of `{julia} pscore`. You can compare with @fig-classification-decision to see that this is associated to a negative prediction.

Is there a temperature at which we would expect the model to make the opposite prediction? Perhaps the simplest approach to this question is to feed our trained model different values of the temperature, and measure the output. What if the location was a degree warmer? A degree colder? Ten?

```{julia}
#| echo: false
#| output: false
temp = cnd[1] .+ LinRange(-50, 50, 200)
npred = zeros(Float64, length(temp))
for i in eachindex(npred)
    tcnd = copy(cnd)
    tcnd[1] = temp[i]
    npred[i] = predict(model, tcnd; threshold=false)
end

lasttemp = temp[findlast(npred .>= model.τ)]
tempdiff = abs(cnd[1]-lasttemp)
```

This approach is both generating perturbations of our initial prediction, and expressing these predictions as a function of whether they lead the model to change its outcome. Conceptually, we are 80% of our way to understanding counterfactuals (in practical terms, we are still on square one, but let's give it time).

The outcome of running this simple simulations is given in @fig-counterfactuals-illustration. From this figure, we can see that the closest temperature that would lead to the model predicting a presence is TODO, which requires a temperature difference of TODO.

```{julia}
#| label: fig-counterfactuals-illustration
#| echo: false
#| fig-scap: What temperature change is required to turn a prediction into the other outcome?
#| fig-cap: Effect of changing the temperature for a single observation (marked as a black dot) on the probability returned by the trained SDM (as a black line). There is a value above which the prediction leads to the positive class (the SDM predicts that the conditions are suitable to the species), marked by a dashed line. The generation of counterfactuals is a way to generalize this approach, by also introducing notions of optimality.

f = Figure(; size=(6, 6).*(600/6))
ax = Axis(f[1,1], xlabel="Temperature", ylabel="P(presence) according to the NBC")

poly!(ax, Point2f[(temp[1], 0), (temp[end], 0), (temp[end], model.τ), (temp[1], model.τ)], color=bkcol.sdm.absentbg)
poly!(ax, Point2f[(temp[1], 1), (temp[end], 1), (temp[end], model.τ), (temp[1], model.τ)], color=bkcol.sdm.presentbg)
hlines!([model.τ], color=bkcol.generic, linestyle=:dash)

lines!(ax, temp, npred, color=bkcol.generic)
scatter!(ax, [cnd[1]], [pscore], color=bkcol.generic, markersize=12)

ylims!(ax, extrema(npred))
xlims!(ax, extrema(temp))

current_figure()
```

Is this a *large* difference? This is a rather subjective question; one we can ask instead is whether this is an *optimal* difference, and specifically whether we can find other alternatives inputs to the model that would be less costly (in terms of distance) and more efficient (in terms of effect on the model).

At this point, it is important to note that we will not be addressing the *feasibility* of making these changes. There are specific algorithms that penalize some variables that are more difficult (or impossible) to change, but they follow the same principles as what we will discuss here. Their inner workings are, quite simply put, orders of magnitude more complex. For an example, and a discussion of why changing features that people can act upon is essential to algorithmic fairness, see @schleich2021.

## Theory on counterfactuals

In this section, we will look at the method introduced by @wachter2017, and spend a bit of time linking it to concepts we have seen in other chapters of the book. In the first section, we have defined two components to the problem of generating counterfactuals: given a vector of features $\mathbf{x}$, we want to generate a new vector of features $\mathbf{x}'$, such that a model $f$ would return $\hat y = f(\mathbf{x})$, and $\hat y' \approx f(\mathbf{x}')$. Because we are working on a classification problem, we *might* assume that we want $\hat y ' = \neg \hat y$, but recall from @sec-learningcurves-probabilistic that we are getting to the binary classification of presence/absence by comparing the probability to an optimized threshold $\tau$, so we can also aim for $\hat y' = \tau + \varepsilon$, where $\varepsilon$ is a very small perturbation (positive or negative) that would make us cross the decision threshold.

### Loss function

In @sec-gradientdescent, we have established the idea of a loss function, that measures how far away a specific instance is to an optimal solution. What does it means for a countefactual to be optimal? We want the distance between $\hat y$ and $\hat y'$ to be as small as possible (we are meeting the criteria of "the prediction has changed"), while also ensuring that the distance between $\mathbf{x}$ and $\mathbf{x}'$ remains small (we are meeting the criteria of parsimonious explanations).

@wachter2017 suggest that the appropriate loss function to use is

$$
L(\mathbf{x}, \mathbf{x}', \hat y', f) = \lambda \times (f(\mathbf{x}') - \hat y')^2 + \text{d}(\mathbf{x}, \mathbf{x}')
$$ {#eq-counterfactuals-loss}

where $\lambda$ is a learning rate (as defined in @sec-gradientdescent-learningrate), and $\text{d}$ is a distance function between the input vectors. In this formulation, the loss function is not handling categorical predictors (which is an acceptable compromise, because we are not using any in this illustration).

In @wachter2017, the distance function for $m$ features is defined as

$$
\text{d}(\mathbf{x}, \mathbf{x}') = \sum_{i=1}^{m}\frac{\|\mathbf{x}_i-\mathbf{x}_i'\|}{\text{MAD}_i}
$$ {#eq-counterfactuals-distance}

where $\text{MAD}_i$ is the median absolute deviation for this feature over the entire dataset. The median absolute deviation for an instance $j$ of a feature $i$ drawn from the entire dataset $\mathbf{X}$ is

$$
\text{median}\left(\|\mathbf{x}_i-\text{median}(\mathbf{X}_{,j})\|\right) \,.
$$

This indicator is very robust to outliers, and returns a scale-free value that allows comparing the distributions of different variables.

### Learning rate

In @eq-counterfactuals-distance, we introduced a learning rate $\lambda$. The interpretation of this term is relatively straightforward, as it measures the *relative* importance of getting close to $\hat y'$ compared to keeping $\mathbf{x}'$ close to $\mathbf{x}$. Therefore, using $\lambda = 2$ means that getting a good switch of the prediction is twice as important as keeping a parsimonious counterfactual.

```{julia}
#| echo: false
#| output: false
learningrate = LinRange(0.0, 100.0, 55)
closs = zeros(Float64, length(temp), length(learningrate))
for i in eachindex(temp)
    tcnd = copy(cnd)
    tcnd[1] = temp[i]
    for j in eachindex(learningrate)
        closs[i,j] = loss(model, cnd, tcnd, model.τ .+ 0.01, learningrate[j]; threshold=false)
    end
end
```

Because the *a priori* value of $\lambda$ may not be intuitive to define, we can rely on the techniques introduced in @sec-tuning, and treat this as an hyper-parameter. Alternatively, @wachter2017 suggest the use of a *threshold* for the distance between the outcome of $f(\mathbf{x}')$ and the desired value of $\hat y'$, which is a trick used in other techniques like acceptance-rejection sampling [@flury1990]. This threshold too must be decided upon before running the algorithm.

```{julia}
#| label: fig-counterfactuals-loss
#| echo: false
#| fig-scap: Loss when changing the temperature for a single observation
#| fig-cap: Consequences of changing only the temperature (as in @fig-counterfactuals-illustration) on the loss function (@eq-counterfactuals-loss) for a single prediction. Notice that changing the value of the learning rate (indicated by the color gradient) to get to a point where we can conclude that the suggested counterfactual is optimal.

f = Figure(; size=(6, 6).*(600/6))
ax = Axis(f[2,1], xlabel="Temperature", ylabel="Loss")

for j in eachindex(learningrate)
    lines!(ax, temp, closs[:,j], color=learningrate[j], colorrange=extrema(learningrate), colormap=bkcol.seq)
end
vlines!(ax, [cnd[1]], color=bkcol.generic, linestyle=:dash)
scatter!(ax, [temp[findlast(npred .>= model.τ)]], [closs[findlast(npred .>= model.τ),1]], color=bkcol.generic, markersize=12)

ylims!(ax, extrema(closs))
xlims!(ax, extrema(temp))

Colorbar(f[1,1], colorrange=extrema(learningrate), colormap=bkcol.seq, vertical=false)

current_figure()
```

In @fig-counterfactuals-loss, we can see the effect of changing the learning rate on the value of the loss function when only changing the temperature as we did for @fig-counterfactuals-illustration. Notice that for values of the learning rate that are too small, there is not counterfactual for which the loss is *lower* than the original datapoint, and therefore we need to increase the learning rate until the optimum (smallest point that brings us above the threshold) can be reached. The shape of the response of the loss function to the change in temperature is also informative. In one direction, it increases linearly: $\mathbf{x}'$ is getting further and further away from $\mathbf{x}$ without the *prediction* getting closer to $\hat y'$. On the other direction, it is more or less hump-shaped, with an optimal point. When we increase the learning rate (possibly by *a lot*), the behavior in which $\mathbf{x}$ is an optimum of this problem disappears.

**TK** the threshold is important because it ensures we will only look at the space of parameters around the optimal solution - but making assumptions about the shape of the loss landscape

### Optimization

In @sec-gradientdescent, we were able to write down an analytical expression of the gradient for our model (and its associated loss function). This may not be the case for the specific problem we are trying to solve. In addition, the surface of the loss function over possibly many parameters may not be smooth; in fact, if we used tree-based classifiers, it would be a guarantee that it would not. For this reason, we will introduce alternative methods to perform the optimization. The general shape of the problem we are tying to solve is the same as in @sec-gradientdescent: given an input $\mathbf{x}$, we are interested in generating $\mathbf{x}'$ that minimizes loss.

The specific details of

@nelder1965

also simulated annealing

```{julia}
#| label: fig-counterfactuals-generation
#| echo: false
#| fig-scap: Generation of a sample of counterfactuals
#| fig-cap: Generation of a number of counterfactual samples for an actual observation (black diamond), color-coded by whether or not the prediction for each counterfactual is positive or negative. The target $\hat y'$ is set to $\tau + 0.015$, which explains why the generated counterfactuals lie on the separation between the positive and the negative class.
f = Figure(; size=(6, 6).*(600/6))
ax = Axis(f[1,1])
scatter!(ax, tuple(cnd[model.v]...), marker=:diamond, color=bkcol.generic, markersize=12)
proposals = [counterfactual(model, cnd, model.τ + 0.015, 100.0; threshold=false) for _ in 1:100]
for i in eachindex(proposals)
    xc = proposals[i]
    ccol = predict(model, xc; threshold=true) ? bkcol.sdm.present : bkcol.sdm.absent
    bcol = predict(model, xc; threshold=true) ? bkcol.sdm.presentbg : bkcol.sdm.absentbg
    scatter!(ax, tuple(xc[model.v]...), strokecolor=ccol, strokewidth=2, color=bcol, markersize=12)
end
#xlims!(ax, extrema(model.X[1,:]))
#ylims!(ax, extrema(model.X[12,:]))
current_figure()
```

## Application: todo

```{julia}
validproposals = filter(p -> predict(model, p; threshold=true), proposals)
L = [loss(model, x, xp, model.τ + 0.015, 100.0; threshold=false) for xp in validproposals]
top5 = validproposals[partialsortperm(L, 1:10)]
```

filter by conditions existing in the dataset

## Conclusion