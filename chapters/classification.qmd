# Supervised classification

In the previous chapters, we have focused on efforts on regression models, which is to say models that predict a continuous response. In this chapter, we will introduce the notion of classification, which is the prediction of a discrete variable representing a category. There are a lot of topics we need to cover before we can confidently come up with a model for classification, and so this chapter is part of a series. We will first introduce the idea of classification; in **TODO**, we will think about predictions of classes as probabilities; in **TODO**, we will generalize these ideas and think about learning curves; finally, in **TODO**, we will finally think about variables a lot more, and introduce elements of model interpretation

## The problem: reindeer distribution

Throughout these chapters, we will be working on a single problem, which is to predict the distribution of the Reindeer, *Rangifer tarandus tarandus*. Species Distribution Modeling (SDM; @elith2009), or Ecological Niche Modeling (ENM), is an excellent instance of ecologist essentially doing applied machine learning already, as @beery2021 rightfully pointed out. In fact, the question of fitness-for-purpose, which we discussed in previous chapters, has been covered in the SDM [@guillera-arroita2015]. In these chapters, we will fully embrace this idea, and look at the problem of predicting where species can be as a data science problem.

These two predictors are important **ECOZONES**, and see *e.g.* [@berteaux2014]

## What is classification?

Classification is the prediction of a qualitative response. In @sec-kmeans, for example, we predicted the class of a pixel, which is a qualitative variable with levels $\{1, 2, \dots, k\}$. This represented an instance of *unsupervised* learning, as we had no *a priori* notion of the correct class of the pixel. When building SDMs, by contrast, we often know where species are, and we can simulate "background points", that represent assumptions about where the species are not [@hanberry2012; @barbet-massin2012].

In short, our response variable has levels $\{0, 1\}$: the species is there, or it is not -- we will challenge this assumption later in the series of chapters, but for now, this will do.

### Separability and the curse of dimensionality

A very important feature of the relationship between the features and the classes is that, broadly speaking, classification is much easier when the classes are separable. Separability (often linear separability) is achieved when, if looking at some projection of the data on two dimensions, you can draw a line that separates the classes (a point in a single dimension, a plane in three dimension, and so on and so forth).

In @fig-classification-separability, we can see the temperature (in degrees) for locations with recorded presences of reindeers (top), and for locations with assumed absences. These two classes are not quite linearly separable alongside this single dimension (but maybe there is a different projection of the data that would change this), but there are still some values at which our guess for a class changes. For example, at a location with a temperature colder than 0°C, presences are far more likely. For a location with a temperature warmer than 5°C, absences become more likely. The locations with a temperature between 0°C and 5°C can go either way.

{{< embed ../notebooks/sm-classification.qmd#fig-classification-separability >}}

It would be tempting to say that adding dimensions should improve our chances to find a feature alongside which the classes become linearly separable. If only!

curse of dimensionality

### The confusion table

The most important element we need in order to evaluate the performance of a classification model is the confusion table.

$$
\begin{pmatrix}
\text{TP} & \text{FP}\\
\text{FN} & \text{TN} 
\end{pmatrix}
$$

## The Naive Bayes Classifier

The Naive Bayes Classifier (NBC) is my all-time favorite classifier. It is build on a very simple intuition, works with almost no data, and more importantly, often provides an annoyingly good baseline for other, more complex classifiers to meet. That NBC works at all is counter-intuitive [@hand2001]. It assumes that all variables are independent, it works when reducing the data to a simpler distribution, and although the numerical estimate of the class probability is remarkably unstable, it generally gives good predictions. NBC is the data science equivalent of saying "eh, I reckon it's probably *this* class" and somehow getting it right 95% of the case.

### Assumptions of the NBC

In @fig-classification-separability, what is the most likely class if the temperature is 2°C?

### How to train the NBC

### Decision rule for prediction

## Application: a baseline model of reindeer presence

### Training and validation strategy

### Performance evaluation of the model

{{< embed ../notebooks/sm-classification.qmd#fig-classification-crossvalidation >}}

### What is an acceptable model?