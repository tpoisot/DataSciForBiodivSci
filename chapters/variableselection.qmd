# Variable selection and feature engineering {#sec-predictors}

In @sec-classification, we introduced a simple classifier trained on a dataset of presence and pseudo-absences of a species (*Sitta whiteheadi*), which we predicted using the mean annual temperature as well as the annual total precipitation. This choice of variables was motivated by our knowledge of the fact that most species tend to have some temperature and precipitation they are best suited to. But we can approach the exercise of selecting predictive variables in a far more formal way, and this will form the core of this chapter. Specifically, we will examine two related techniques: variable selection, and feature engineering.

## The curse of dimensionality {#sec-predictors-curse}

The number of variables we use for prediction is the number of dimensions of a problem. It would be tempting to say that adding dimensions should improve our chances to find a feature alongside which the classes become linearly separable. If only!

The "curse of dimensionality" is the common term of everything breaking down when the dimensions of a problem increase. In our perspective, where we rely on the resemblance between features to make a prediction, increasing the dimensions of a problem means adding features, and it has important consequences on the distance between observations. Picture two points positioned at random on the unit interval: the average distance between them is 1/3. If we add one dimension, keeping two points but turning this line into a cube, the average distance would be about 1/2. For a cube, about 2/3. For $n$ dimensions, we can figure out that the average distance grows like $\sqrt{n/6 + c}$, which is to say that when we add more dimensions, we make the average distance between two points go to infinity. This effect is also affecting ecological studies [*e.g.* @smith2017].

Therefore, we need to approach the problem of "which variables to use" with a specific mindset: we want a lot of information for our model, but not so much that the space in which the predictors exist turns immense. There are techniques for this.

## Step-wise approaches to variable selection

## Removing colinear variables

## Multivariate transformations

Delicious. Finally some good fucking quantitative ecology.

## Code

```{julia}
#| echo: false
#| output: false
_code_path = joinpath(dirname(Base.active_project()), "lib")
include(joinpath(_code_path, "pkg.jl"))
include(joinpath(_code_path, "confusion.jl"))
include(joinpath(_code_path, "mocks.jl"))
include(joinpath(_code_path, "nbc.jl"))
include(joinpath(_code_path, "vif.jl"))
include(joinpath(_code_path, "splitters.jl"))
include(joinpath(_code_path, "palettes.jl"))
include(joinpath(_code_path, "crossvalidate.jl"))
```

```{julia}
#| echo: false
#| output: false
_ptm_path = joinpath(@__DIR__, "..", "ptm")
modelpath = joinpath(_ptm_path, "sdm-initial.jld")
ptm = JLD.load(modelpath)
trainlabels, trainfeatures = ptm["training"]
testlabels, testfeatures = ptm["testing"]
```

```{julia}
#| echo: false
#| output: false
folds = kfold(trainlabels, trainfeatures; k=15, permute=true)
```