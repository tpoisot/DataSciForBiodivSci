# Gradient descent {#gradientdescent-sec-gradientdescent}

As we progress into this book, the process of delivering a trained model is going to become increasingly complex. In @sec-kmeans, we work with a model that did not really require training (but did require to pick the best hyper-parameter). In this chapter, we will only increase complexity very slightly, by considering how we can train a model when we have a reference dataset to compare to.

Doing do will require to introduce several new concepts, and so the "correct" way to read this chapter is to focus on the high-level process. The problem we will try to solve (which is introduced in @sec-gradientdescent-problem) is very simple; in fact, the empirical data looks more fake than many simulated datasets!

## A digression: what is a trained model? {#gradientdescent-sec-gradientdescent-trainedmodel}

## The problem: how many interactions in a food web? {#gradientdescent-sec-gradientdescent-problem}

One of the earliest observation that ecologists made about food webs is that when there are more species, there are more interactions. A remarkably insightful crowd, food web ecologists. Nevertheless, it turns out that this apparently simple question had received a few different answers over the years. In

The initial model was proposed by @cohen1984: the number of interactions $L$ scales linearly with the number of species $S$. After all, we can assume that when averaging over many consumers, there will be an average diversity of resources they consume, and so the number of interactions could be expressed as $L \approx b\times S$.

Not so fast, said @martinez1992. When we start looking a food webs with more species, the increase of $L$ with regards to $S$ is superlinear. Thinking in ecological terms, maybe we can argue that consumers are flexible, and that instead of sampling a set number of resources, they will sample a set proportion of the number of consumer-resource combinations (of which there are $S^2$). In this interpretation, $L \approx b\times S^2$.

But the square term can be relaxed; and there is no reason not to assume a power law, with $L\approx b\times S^a$. This last formulation has long been accepted as the most workable one, because it is possible to approximate values of its parameters using other ecological processes [@brose2004].

The "reality" (*i.e.* the relationship between $S$ and $L$ that correctly accounts for ecological constraints, and fit the data as closely as possible) is a little bit different than this formula [@macdonald2020]. But for the purpose of this chapter, figuring out the values of $a$ and $b$ from empirical data is a very instructive exercise.

## Gradient descent

Gradient descent is built around a remarkably simple intuition: knowing the formula that gives rise to our prediction, and the value of the error we made for each point, we can take the derivative of the error with regards to each parameter, and this tells us how much this parameter contributed to the error. Because we are taking the derivative, we can futher know whether to increase, or decrease, the value of the parameter in order to make a smaller error next time.

In this section, we will use linear regression as an example, because it is the model we have decided to use when exploring our ecological problem in @sec-gradientdescent-problem, and because it is suitably simple to keep track of everything when writing down the gradient by hand.

Before we start assembling the different pieces, we need to decide what our model is. We have settled on a linear model, which will have the form $\hat y = m\times x + b$. The little hat on $\hat y$ indicates that this is a prediction. The input of this model is $x$, and its parameters are $m$ (the slope) and $b$ (the intercept). Using the notation we adopted in @sec-gradientdescent-problem, this would be $\hat l = a \times s + b$, with $l = \text{log} L$ and $s = \text{log} S$.

### Defining the loss function

The loss function is an important concept for anyone attempting to compare predictions to outcomes: it quantifies how far away an ensemble of predictions is from a benchmark of known cases. There are many loss functions we can use, and we will indeed use a few different ones in this book. But for now, we will start with a very general understanding of what these functions *do*.

Think of prediction as throwing a series of ten darts on ten different boards. In this case, we know what the correct outcome is (the center of the board, I assume, although I can be mistaken since I have only played darts once, and lost). A cost function would be any mathematical function that compares the position of each dart on each board, the position of the correct event, and returns a score that informs us about how poorly our prediction lines up with the reality.

In the above example, you may be tempted to say that we can take the Euclidean distance of each dart to the center of each board, in order to know, for each point, how far away we landed. Because there are several boards, and because we may want to vary the number of boards while still retaining the ability to compare our performances, we would then take the average of these measures.

We will note the position of our dart as being $\hat y$, the position of the center as being $y$ (we will call this the *ground truth*), and the number of attempts $n$, and so we can write our loss function as

$$
\frac{1}{n}\sum_{i=1}^{n}(y_i - \hat y_i)^2
$$ {#eq-loss-mse}

This loss function is usually called the MSE (Mean Standard Error), or L2 loss, or the quadratic loss, because the paths to machine learning terminology are many. This is a good example of a loss function for regression (and we will discuss loss functions for classification later in this book). There are alternative loss functions to use for regression problems in @tbl-gradientdescent-regressionloss.

| Measure                       | Expression                                               | Remarks                                                                              |
|------------------------------------|------------------|------------------|
| Mean Squared Error (MSE, L2)  | $\frac{1}{n}\sum_{i=1}^{n}\left(y_i - \hat y_i\right)^2$ | Large errors are (proportionally) more penalized because of the squaring             |
| Mean Absolute Error (MAE, L1) | $\frac{1}{n}\sum_{i=1}^{n}\|y_i - \hat y_i\|$            | Error measured in the units of the response variable                                 |
| Root Mean Square Error (RMSE) | $\sqrt{\text{MSE}}$                                      | Error measured in the units of the response variable                                 |
| Mean Bias Error               | $\frac{1}{n}\sum_{i=1}^{n}\left(y_i - \hat y_i\right)$   | Errors *can* cancel out, but this can be used as a measure of positive/negative bias |

: List of common loss functions for regression problems {#tbl-gradientdescent-regressionloss tbl-colwidths="\[25,25,50\]"}

Throughout this chapter, we will use the L2 loss (@eq-loss-mse), because it has *really* nice properties when it comes to taking derivatives, which we will do a lot of. In the case of a linear model, we can rewrite @eq-loss-mse as

$$
f = \frac{1}{n}\sum\left(y_i - m\times x_i - b\right)^2
$$ {#eq-loss-withmodel}

There is an important change in @eq-loss-withmodel: we have replaced the prediction $\hat y_i$ with a term that is a function of the predictor $x_i$ and the model parameters: this means that we can calculate the value of the loss as a function of a pair of values $(x_i, y_i)$, and the model parameters.

### Calculating the gradient

With the loss function corresponding to our problem in hands (@eq-loss-withmodel), we can calculate the gradient. Given a function that is scalar-valued (it returns a single value), taking several variables, that is differentiable, the gradient of this function is a vector-valued (it returns a vector) function; when evaluated at a specific point, this vectors indicates both the direction and the rate of fastest increase, which is to say the direction in which the function increases away from the point, and how fast it moves.

We can re-state this definition using the terms of the problem we want to solve. At a point $p = [m\quad b]^\top$, the gradient $\nabla f$ of $f$ is given by:

$$
\nabla f\left(
p
\right) = 
\begin{bmatrix}
\frac{\partial f}{\partial m}(p) \\
\frac{\partial f}{\partial b}(p)
\end{bmatrix}\,.
$$ {#eq-gradientdescent-gradientfull}

This indicates how changes in $m$ and $b$ will *increase* the error. In order to have a more explicit formulation, all we have to do is figure out an expression for both of the partial derivatives. In practice, we can let auto-differentiation software calculate the gradient for us [@innes2018]; these packages are now advanced enough that they can take the gradient of code directly.

Solving $(\partial f / \partial m)(p)$ and $(\partial f / \partial c)(p)$ is easy enough:

$$
\nabla f\left(
p
\right) = 
\begin{bmatrix}
-\frac{2}{n}\sum \left[x_i \times (y_i - m\times x_i - b)\right] \\
-\frac{2}{n}\sum \left(y_i - m\times x_i - b\right)
\end{bmatrix}\,.
$$ {#eq-gradientdescent-gradientexplicit}

Note that both of these partial derivatives have a term in $2n^{-1}$. Getting rid of the $2$ in front is very straightforward! We can modify @eq-loss-withmodel to divide by $2n$ instead of $n$. This modified loss function retains the important characteristics: it increases when the prediction gets worse, and it allows comparing the loss with different numbers of points. As with many steps in the model training process, it is important to think about *why* we are doing certain things, as this can enable us to make some slight changes to facilitate the analysis.

With the gradient written down in @eq-gradientdescent-gradientexplicit, we can now think about what it means to *descend* the gradient.

### Descending the gradient

Recall from **TK** that the gradient measures how far we *increase* the function of which we are taking the gradient. Therefore, it measures how much each parameter contributes to the loss value. Our working definition for a trained model is "one that has little loss", and so in an ideal world, we could find a point $p$ for which the gradient is as small as feasible.

Because the gradient measures how far away we increase error, and intuitive way to use it is to take steps in the *opposite* direction. In other words, we can update the value of our parameters using $p := p - \nabla f(p)$, meaning that we substract from the parameter values their contribution to the overall error in the predictions.

But, as we will discuss further in @sec-gradientdescent-learningrate, there is such a thing as "too much learning". For this reason, we will usually not move the entire way, and introduce a term to regulate how much of the way we actually want to descend the gradient. Our actual scheme to update the parameters is

$$
p := p - \eta\times \nabla f(p) \,.
$$

This formula can be *iterated*: with each successive iteration, it will get us closer to the optimal value of $p$, which is to say the combination of $m$ and $b$ that minimizes the loss.

### A note on the learning rate {#gradientdescent-gradientdescent-gradientdescent-gradientdescent-seq-gradientdescent-learningrate}

The error we can make on the first iteration will depend on the value of our initial pick of parameters. If we are *way off*, especially if we did not re-scale our predictors and responses, this error can get very large. And if we make a very large error, we will have a very large gradient, and we will end up making very big steps when we update the parameter values. There is a real risk to end up over-compensating, and correcting the parameters too much.

In order to protect against this, in reality, we update the gradient only a little, where the value of "a little" is determined by an hyper-parameter called the *learning rate*, which we noted $\eta$. This value will be very small (much less than one). Picking the correct learning rate is not simply a way to ensure that we get correct results (though that is always a nice bonus), but can be a way to ensure that we get results *at all*. The representation of numbers in a computer's memory is tricky, and it is possible to create an overflow: a number so large it does not fit within 64 (or 32, or 16, or however many we are using) bits of memory.

The conservative solution of using the smallest possible learning rate is not really effective, either. If we almost do not update our parameters at every epoch, then we will take almost forever to converge on the correct parameters. Figuring out the learning rate is an example of hyper-parameter tuning, which we will get back to later in this book.

## Application: how many links are in a food web?

## Notes on regularization