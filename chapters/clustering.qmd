# Clustering {#sec-clustering}

As we mentioned in the introduction, a core idea of data science is that things that look the same (in that, when described with data, they resemble one another) are likely to be the same. Although this sounds like a simplifying assumption, this can provide the basis for a very powerful technique in which we *create* groups in data that have no labels. This task is called unsupervised clustering: we seek to add a *label* to each observation, in order to form groups, and the data we work from do *not* have a label that we can use to train a model.

## A digression: which birds are red?

Before diving in, it is a good idea to ponder a simple case. We can divide everything in just two categories: things with red feathers, and things without red feathers. An example of a thing with red feathers is the Northern Cardinal (*Cardinalis cardinalis*), and things without red feathers are the iMac G3, Haydn's string quartets, and of course the Northern Cardinal (*Cardinalis cardinalis*).

See, biodiversity data science is complicated, because it tends to rely on the assumption that we can categorize the natural world, and the natural world (mostly in response to natural selection) comes up with ways to be, well, diverse. In the Northern Cardinal, this is shown in males having red feathers, and females having mostly brown feathers. Before moving forward, we need to consider ways to solve this issue, as this issue will come up *all the time.*

The first mistake we have made is that the scope of objects we want to classify, which we will describe as the "domain" of our classification, is much too broad: there are few legitimate applications where we will have a dataset with Northern Cardinals, iMac G3s, and Haydn's string quartets. Picking a reasonable universe of classes would have solved our problem a little. For example, among the things that do not have red feathers are the Mourning Dove, the Kentucky Warbler, and the House Sparrow.

The second mistake that we have made is improperly defining our classes; bird species exhibit sexual dimorphism (not in an interesting way, like wrasses, but let's give them some credit for trying). Assuming that there is such a thing as *a* Northern Cardinal is not necessarily a reasonable assumption! And yet, the assumption that a single label is a valid representation of non-monomorphic populations is a surprisingly common one, with actual consequences for the performance of image classification algorithms [@luccioni2023]. This assumption reveals a lot about our biases: male specimens are over-represented in museum collections, for example [@cooper2019]. In a lot of species, we would need to split the taxonomic unit into multiple groups in order to adequately describe them.

The third mistake we have made is using predictors that are too vague. The "presence of red feathers" is not a predictor that can easily discriminate between the Northen Cardinal (yes for males, sometimes for females), the House Finch (a little for males, no for females), and the Red-Winged Black Bird (a little for males, no for females). In fact, it cannot really capture the difference between red feathers for the male House Finch (head and breast) and the male Red Winged Black Bird (wings, as the name suggests).

The final mistake we have made is in assuming that "red" is relevant as a predictor. In a wonderful paper, @cooney2022 have converted the color of birds into a bird-relevant colorimetric space, revealing a clear latitudinal trend in the ways bird colors, as perceived by other birds, are distributed. This analysis, incidentally, splits all species into males and females. The use of a color space that accounts for the way colors are perceived is a fantastic example of why data science puts domain knowledge front and center.

Deciding which variables are going to be accounted for, how the labels will be defined, and what is considered to be within or outside the scope of the classification problem is *difficult*. It requires domain knowledge (you must know a few things about birds in order to establish criteria to classify birds), and knowledge of how the classification methods operate (in order to have just the right amount of overlap between features in order to provide meaningful estimates of distance).

## The problem: classifying pixels from an image

Throughout this chapter, we will work on a single image -- we may initially balk at the idea that an image is data, but it is! Specifically, an image is a series of instances (the pixels), each described by their position in a multidimensional colorimetric space. Greyscale images have one dimension, and images in color will have three: their red, green, and blue channels. Not only are images data, this specific dataset is going to be far larger than many of the datasets we will work on in practice: the number of pixels we work with is given by the product of the width and height of the image!

In fact, we are going to use an image with a lot more dimensions: the data in this chapter are coming from a Landsat 9 image [@vermote2016], for which we have access to 7 different bands (the full data product has more bands, but we will not use them all in this example).

| Band number | Information                  |
|-------------|------------------------------|
| 1           | Aerosol                      |
| 2           | Visible blue                 |
| 3           | Visible red                  |
| 4           | Visible green                |
| 5           | Near-infrared (NIR)          |
| 6           | Short wavelength IR (SWIR 1) |
| 7           | SWIR 2                       |

From these channels, we can reconstruct an approximation of what the landscape looked like (by using the red, green, and blue channels).

Or can we? If we were to invent a time machine, and go stand directly under Landsat 9 at the exact center of this scene, and look around, what would we see? We would see colors, and they would admit a representation as a three-dimensional vector of red, green, and blue. But we would see so much more than that! And even if we were to stand within a pixel, we would see a *lot* of colors. And texture. And depth. We would see something entirely different from this map; and we would be able to draw a lot more inferences about our surroundings than what is possible by knowing the average color of a 30x30 meters pixel. But just like we can get more information that Landsat 9, so to can Landsat 9 out-sense us when it comes to getting information. In the same way that we can extract a natural color composite out of the different channels, we can extract a fake color one to highlight differences in the landscape.

{{< embed ../notebooks/sm-clustering.qmd#fig-kmeans-composites >}}

In @fig-kmeans-composites, we compare the natural color reconstruction to two different false color composites. All of the panels in @fig-kmeans-composites represent the same physical place at the same moment in time; but through them, we are looking at this place with very different purposes. This is not an idle observation, but a core notion in data science: what we measure defines what we can see. In order to tell something meaningful about this place, we need to look at it in the "right" way.

::: column-margin
We will revisit the issue of variable selection and feature engineering in @sec-variable-selection.
:::

So far, we have looked at this area by combining the raw data. Depending on the question we have in mind, they may not be the *right* data. In fact, they may not hold information that is relevant to our question *at all*; or worse, they can hold more noise than signal. Looking at @fig-kmeans-composites, we might wonder, "where are the areas with vegetation?". And based on our knowledge of what plants do, we can start thinking about this question in a different way. Specifically, "is there a series of features of ares with vegetation that are not shared by area without vegetation?". But this a complicated question to answer, and so we can simplify this by asking, "how can I guess that there is a plant?". Thankfully, ecologists, whose hobbies include (i) guessing even in the presence of data and (ii) plants, have ways to answer this question rather accurately.

One way to do this is to calculate the normalized difference vegetation index, or NDVI [@kennedy2020]. NDVI is derived from the band data (we will see how in a minute), and is an adequate heuristic to make a difference between vegetation, barren soil, and water. Because we are specifically thinking about fields, we can also consider the NDWI (water) and NDMI (moisture) dimensions: taken together, these information will represent every pixel in a three-dimensional space, telling us whether there are plants (NDVI), whether they are stressed (NDMI), and whether this pixel is a water body (NDWI).

Because there are a few guidelines (educated guesses, in truth, and the jury is still out on the "educated" part) about the values, we can look at the relationship between the NDVI and NDMI data [@fig-kmeans-hexbin]. For example, NDMI values around -0.1 (note how there is a strong cluster of points here) are [low-canopy cover with low water stress](https://eos.com/make-an-analysis/ndmi/); NDVI values from 0.2 to 0.5 are good candidates for moderately dense crops.

{{< embed ../notebooks/sm-clustering.qmd#fig-kmeans-hexbin >}}

By picking these three values, instead of simply looking at the clustering of all the bands in the raw data, we are starting to refine what the algorithm see, through the lens of what we know is important about the system.

## The theory behind *k*-means clustering

In order to understand the theory underlying *k*-means, we will work backwards from its output. As a method for unsupervised clustering, *k*-means will return a vector of *class memberships*, which is to say, a list that maps each observation (pixel, in our case) to a class (tentatively, a cohesive landscape unit). What this means is that *k*-means is a transformation, taking as its input a vector with three dimensions (red, green, blue), and returning a scalar (an integer, even!), giving the class to which this pixel belongs. These are the input and output of our blackbox, and now we can start figuring out its internals.

### Inputs and parameters

::: column-margin
Throughout this book, we will use $\mathbf{X}$ to note the matrix of features, and $\mathbf{y}$ to note the vector of labels. Instances are columns of the features matrix.
:::

In *k*-means, a set of observations $\mathbf{x}_i$ are assigned to a set of classes $\mathbf{C}$, also called the clusters. All $\mathbf{x}_i$ are vectors with the same dimension (we will call it $f$, for *features*), and we can think of our observations as a matrix of features $\mathbf{X}$ of size $(f, n)$, with $f$ features and $n$ observations (the columns of this matrix).

The number of classes of $\mathbf{C}$ is $|\mathbf{C}| = k$, and $k$ is an hyper-parameter of the model, as it needs to bet fixed before we start running the algorithm. Each class is defined by its centroid, a vector $\mathbf{c}$ with $f$ dimensions (*i.e.* the centroid corresponds to a potential "idealized" observation of this class in the space of the features).

### Assigning instances to classes

::: column-margin
Of course, the correct distance measure to use depends on what is appropriate for the data!
:::

Instances are assigned to the class when the distance between themselves and the centroid of this class is lower than the distance between themselves and any other class. To phrase it differently, the class membership of an instance $\mathbf{x}_i$ is given by

$$
\text{argmin}_j \left\|\mathbf{x}_i-\mathbf{c}_j\right\|_2 \,,
$$ {#eq-clustering-onepoint}

which is the value of $j$ that minimizes the $L^2$ norm (the Euclidean distance) between the instance and the centroid, and $\text{argmin}_j$ is the function returning the value of $j$ that minimizes its argument.

### Optimizing the centroids

Of course, what we really care about is the assignment of *all* instances to the classes. For this reason, the configuration (the disposition of the centroid) that solves our specific problem is the one that leads to the lowest possible variance within the clusters. As it turns out, it is not that difficult to go from @eq-clustering-onepoint to a solution for the entire problem: we simply have to sum over all points!

This leads to a measure of the variance, which we want to minimize.

$$
\sum_{i=1}^k \sum_{\mathbf{x}\in \mathbf{C}_i} \|\mathbf{x} - \mathbf{c}_i\|_2 \,.
$$ {#eq-clustering-variance}

The part that is non-trivial is now to decide on the value of $\mathbf{c}$ for each class. This is the heart of the *k*-means algorithm. From @eq-clustering-onepoint, we have a criteria to decide to which class each instance belongs. Of course, there is nothing that prevents us from using this in the opposite direction, to define the instance by the points that form it! In this approach, the membership of class $\mathbf{C}_j$ is the list of points that satisfy the condition in @eq-clustering-onepoint. But there is no guarantee that the *current* position of $\mathbf{c}_j$ in the middle of all of these points is optimal, *i.e.* that it minimizes the within-class variance.

To ensure that this is the case, we can re-define the value of $\mathbf{c}_j$ as

$$
\mathbf{c}_j = \frac{1}{|\mathbf{C}_j|}\sum\mathbf{C}_j \,,
$$ {#eq-clustering-centroid-update}

where $|\cdot|$ is the cardinality of (number of istances in) $\mathbf{C}_j$, and $\sum \mathbf{C}_j$ is the sum of each feature in $\mathbf{C}_j$. To put it plainly: we update the centroid of $\mathbf{C}_j$ so that it takes, for each feature, the average value of all the points that form $\mathbf{C}_j$.

### Updating the classes

::: column-margin
Repeating a step multiple times in a row is called an iterative process, and we will see a *lot* of them.
:::

Once we have applied @eq-clustering-centroid-update to all classes, there is a good chance that we have moved the centroids in a way that moved them away from some of the points: the membership of the instances has likely changed. Therefore, we need to re-start the process again, in an iterative way.

But until when? Finding the optimal solution for a set of points is an NP-hard problem [@aloise2009], which means that we will need to rely on a little bit of luck. The simplest way to deal with iterative processes is to let them run for a long time, as after a little while they should converge onto an optimum (here, a set of centroids for which the variance is as good as it gets), and hope that this optimum is *global* and not *local*.

A global optimum is easy to define: it is the state of the solution that gives the best possible result. For this specific problem, a global optimum means that there are no other combinations of centroids that give a lower variance. A local optimum is a little bit more subtle: it means that we have found a combination of centroids that we cannot improve without first making the variance worse. Because the algorithm as we have introduced it in the previous sections is *greedy*, in that it makes the moves that give the best short-term improvement, it will not provide a solution that temporarily makes the variance higher, and therefore is susceptible to being trapped in a local optimum.

In order to get the best possible solution, it is common to run *k*-means multiple times for a given $k$, and to pick the positions of the centroids that give the best overall fit.

### Identification of the optimal number of clusters

## Application: optimal clustering of the satellite image data

### Initial run

{{< embed ../notebooks/sm-clustering.qmd#fig-kmeans-initial-landscape >}}

### Optimal number of pixels

::: column-margin
We will revisit the issue of tuning the hyper-parameters in more depth in @sec-tuning.
:::

In order to produce @fig-kmeans-initial-landscape, we had to guess at a number of classes we wanted to split the landscape into. This introduces two important steps in coming up with a model: starting with initial parameters in order to iterate rapidly, and then refining these parameters to deliver a model that is fit for purpose.

We will use the method from @davies1979, which ... .

{{< embed ../notebooks/sm-clustering.qmd#fig-kmeans-tuning >}}

### Clustering with optimal number of classes

{{< embed ../notebooks/sm-clustering.qmd#fig-kmeans-optimal-landscape >}}

### Partition of the pixels

{{< embed ../notebooks/sm-clustering.qmd#fig-kmeans-barplot-classes >}}

{{< embed ../notebooks/sm-clustering.qmd#fig-kmeans-clustering >}}

::: column-margin
In fact, this behavior makes *k*-means excellent at creating color palettes from images! Cases in point, [Karthik Ram's Wes Anderson palettes](https://github.com/karthik/wesanderson), and [David Lawrence Miller's Beyoncé palettes](https://github.com/dill/beyonce). Let it never again be said that ecologists should not be trusted with machine learning methods.
:::

The result in @fig-kmeans-clustering is not surprising, if we spend some time thinking about how *k*-means work. Because our criteria to assign a point to a cluster is based on the being closest to its centroid than to any other centroid, we are essentially creating Voronoi cells, with sharp boundaries between them. By opposition to a model based on, for example, mixtures of Gaussians, the assignment of a point to a cluster in *k*-means is independent of the current composition of the cluster (modulo the fact that the current composition of the cluster is used to update the centroids). In fact, this makes *k*-means closer to (or at least most efficient as) a method for vector quantization [@gray1984].

## Alternatives and improvements

EM

k-median

k-medoids