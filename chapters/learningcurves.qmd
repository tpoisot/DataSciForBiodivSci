# Hyper-parameters tuning {#sec-tuning}

In @sec-gradientdescent, we represented the testing and training loss of a model as a function of the number of gradient descent steps we had made. This sort of representation is very useful to figure out how well our model is learning, and is called, appropriately enough, a learning curve. We further discussed that the learning rate (and possibly the regularization rate), and the number of epochs, where *hyper*-parameters of the model. An hyper-parameter is usually defined as a parameter of the model that is *controlling* the learning process, but is not itself modified through learning [@yang2020]. Hyper-parameters usually need to be determined *before* the training starts [@claesen2015], but there are various strategies to optimize them. In this chapter, we will produce learning curves to find the optimal values of hyper-parameters of the model we developed in Â­@sec-classification and @sec-predictors.

We will illustrate this using an approach called moving-threshold classification, and additionally explore how we can conduct searches to tune several hyper-parameters at once. There are many techniques to sample multiple parameters at the same time, including Latin hypercube sampling [@huntington1998], orthogonal sampling [@mckay1979], and grid searches. The common point to all of these approaches are that they generate a combination of hyper-parameters, which are used to train the model, and measures of performance are then used to pick the best possible combination of hyper-parameters. In the process of doing this, we will also revisit the question of why the MCC is a good measure of the classification performance, as well as examine tools to investigate the "right" balance between false/true positive rates. At the end of this chapter, we will have produced a very good model for the distribution of the Corsican nuthatch, which we will then try to *explain* in @sec-explanations.

```{julia}
#| echo: false
#| output: false
_code_path = joinpath(dirname(Base.active_project()), "lib")
include(joinpath(_code_path, "pkg.jl"))
include(joinpath(_code_path, "confusion.jl"))
include(joinpath(_code_path, "mocks.jl"))
include(joinpath(_code_path, "bioclim.jl"))
include(joinpath(_code_path, "nbc.jl"))
include(joinpath(_code_path, "vif.jl"))
include(joinpath(_code_path, "splitters.jl"))
include(joinpath(_code_path, "palettes.jl"))
include(joinpath(_code_path, "crossvalidate.jl"))
include(joinpath(_code_path, "variableselection.jl"))
```

## Classification based on probabilities

When first introducing classification in @sec-classification and @sec-predictors, we used a model that returned a deterministic answer, which is to say, the name of a class (in our case, this class was either "present" or "absent"). But a lot of classifiers return quantitative values, that correspond to (proxies for) the probability of the different classes. Nevertheless, because we are interested in solving a classification problem, we need to end up with a confusion table, and so we need to turn a number into a class. In the context of binary classification (we model a yes/no variable), this can be done using a threshold for the probability.

::: column-margin
Note that the quantitative value returned by the classifier does not *need* to be a probability; it simply needs to be on an interval or ratio scale.
:::

The idea behind the use of thresholds is simple: if the classifier output $\hat y$ is larger than (or equal to) the threshold value $\tau$, we consider that this prediction corresponds to the positive class (the event we want to detect, for example the presence of a species). In the other case, this prediction corresponds to the negative class. Note that we do not, strictly, speaking, require that the value $\hat y$ returned by the classifier be a probability. We can simply decide to pick $\tau$ somewhere in the support of the distribution of $\hat y$.

The threshold to decide on a positive event is an hyper-parameter of the model. In the NBC we built in @sec-classification, our decision rule was that $p(+) > p(-)$, which when all is said and done (but we will convince ourselves of this in **TODO**), means that we used $\tau = 0.5$. But there is no reason to assume that the threshold needs to be one half. Maybe the model is overly sensitive to negatives. Maybe there is a slight issue with our training data that bias the model predictions. And for this reason, we have to look for the optimal value of $\tau$.

There are two important values for the threshold, at which we know the behavior of our model. The first is $\tau = \text{min}(\hat y)$, for which the model *always* returns a negative answer; the second is, unsurprisingly, $\tau = \text{max}(\hat y)$, where the model *always* returns a positive answer. Thinking of this behavior in terms of the measures on the confusion matrix, as we have introduced them in **TODO**, the smallest possible threshold gives only negatives, and the largest possible one gives only positives: they respectively maximize the false negatives and false positives rates.

### The ROC curve

This is a behavior we can exploit, as increasing the threshold away from the minimum will lower the false negatives rate and increase the true positive rate, while decreasing the threshold away from the maximum will lower the false positives rate and increase the true negative rate. If we cross our fingers and knock on wood, there will be a point where the false events rates have decreased as much as possible, and the true events rates have increased as much as possible, and this corresponds to the optimal value of $\tau$ for our problem.

We have just described the Receiver Operating Characteristic (ROC; @fawcett2006) curve! The ROC curve visualizes the false positive rate on the $x$ axis, and the true positive rate on the $y$ axis. The area under the curve (the ROC-AUC) is a measure of the overall performance of the classifier [@hanley1982]; a model with ROC-AUC of 0.5 performs at random, and values moving away from 0.5 indicate better (close to 1) or worse (close to 0) performance.The ROC curve is a description of the model performance across all of the possible threshold values we investigated!

### The PR curve

One very common issue with ROC curves, is that they are overly optimistic about the performance of the model, especially when the problem we work on suffers from class imbalance, which happens when observations of the positive class are much rarer than observations of the negative class. In ecology, this is a common feature of data on species interactions [@poisot2023]. For this reason, it is always advised to ...

### Cross-entropy loss and other classification loss functions

## How to optimize the threshold?

In order to understand the optimization of the threshold, we first need to understand how a model with thresholding works. When we run such a model on multiple input features, it will return a list of probabilities, for example $[0.2, 0.8, 0.1, 0.5, 1.0]$. We then compare all of these values to an initial threshold, for example $\tau = 0.05$, giving us a vector of Boolean values, in this case $[+, +, +, +, +]$. We can then compare this classified output to a series of validation labels, *e.g.* $[-, +, -, -, +]$, and report the performance of our model. In this case, the very low thresholds means that we accept any probability as a positive case, and so our model is very strongly biased. We then increase the threshold, and start again.

As we have discussed in **TODO previous**, moving the threshold is essentially a way to move in the space of true/false rates. As the measures of classification performance capture information that is relevant in this space, there should be a value of the threshold that maximizes one of these measures. Alas, no one agrees on which measure this should be [@Perkins2006; @Unal2017]. The usual recommendation is to use the True Skill Statistic, also known as Youden's $J$ [@youden1950]. The biomedical literature, which is quite naturally interested in getting the interpretation of tests right, has established that maximizing this value brings us very close to the optimal threshold for a binary classifier [@perkins2005]. In a simulation study, using the True Skill Statistic gave good predictive performance for models of species interactions [@poisot2023a].

Some authors have used the MCC as a measure of optimality [@zhou2013], as it is maximized *only* when a classifier gets a good score for the basic rates of the confusion matrix. Based on this information, @chicco2023 recommend that MCC should be used to pick the optimal threshold *regardless of the question*, and I agree with their assessment. A high MCC is always associated to a high ROC-AUC, TSS, etc., but the opposite is not necessarily true. This is because the MCC can only reach high values when the model is good at *everything*, and therefore it is not possible to trick it. In fact, previous comparisons show that MCC even outperform measures of classification loss [@Jurman2012].

For once, and after over 15 years of methodological discussion, it appears that we have a conclusive answer! In order to pick the optimal threshold, we find the value that maximizes the MCC. Note that in previous chapters, we already used the MCC as a our criteria for the best model, and now you know why.

## Application: improved Corsican nuthatch model

### Making the NBC explicitely probabilistic

In @sec-classification, we have expressed the probability that the NBC recommends a positive outcome as

$$
    P(+|x) = \frac{P(+)}{P(x)}P(x|+)\,,
$$

and noted that because $P(x)$ is constant across all classes, we could simplify this model as $P(+|x) \propto P(+)P(x|+)$. But because we know the only two possible classes are $+$ and $-$, we can figure out the expression for $P(x)$. Because we are dealing with probabilities, we know that $P(+|x)+P(-|x) = 1$. We can therefore re-write this as

$$
\frac{P(+)}{P(x)}P(x|+)+\frac{P(-)}{P(x)}P(x|-) = 1\,
$$

which after some reorganisation (and note that $P(-) = 1-P(+)$), results in 

$$
P(x) = P(+) P(x|+)+P(-) P(x|-) \,.
$$

This value $P(x)$ is the "evidence" in Bayesian parlance, and we can use this value explicitely to get the prediction for the probability associated to the class $+$ using the NBC.

Note that we can see that using the approximate version we used so far (the prediction is positive if $P(+) P(x|+) > P(-) P(x|-)$) is equivalent to saying that the prediction is positive whenever $P(+|x) > \tau$ with $\tau = 0.5$. In the next sections, we will challenge the assumption that $0.5$ is the optimal value of $\tau$.

```{julia}
#| echo: false
#| output: false
_ptm_path = joinpath(dirname(Base.active_project()), "checkpoints")
modelpath = joinpath(_ptm_path, "sdm-step-1.jld")
ptm = JLD.load(modelpath)
y = ptm["labels"]
X = ptm["features"][:,ptm["variables"]]
folds = ptm["folds"]
v = ptm["variables"]
tf = ptm["transformation"]
untuned = naivebayes(y, X; transformation=tf)
```

```{julia}
#| echo: false
#| output: false
T = LinRange(0.0, 1.0, 250)
CT = hcat([first(crossvalidate(naivebayes, y, X, folds, Ï; transformation=tf)) for Ï in T]...)
Ï = T[last(findmax(vec(mean(mcc.(CT); dims=1))))]
```

```{julia}
#| echo: false
#| label: fig-tuning-threshold
#| fig-cap: learning curve for the threshold
fig = Figure(; resolution=(600, 300))
ax = Axis(fig[1,1]; xlabel="Threshold", ylabel="MCC")
series!(ax, T, mcc.(CT), solid_color=light[1])
lines!(ax, T, vec(mean(mcc.(CT); dims=1)), color=light[2], linewidth=2, linestyle=:dash)
scatter!(ax, Ï, maximum(vec(mean(mcc.(CT); dims=1))), color=light[2], markersize=20)
xlims!(ax, (0., 1.))
ylims!(ax, (0., 1.))
current_figure()
```

```{julia}
#| echo: false
#| label: fig-tuning-roc-pr
#| fig-cap: ROC and PR
f = Figure()

roc = Axis(f[1,1], xlabel="False Positive Rate", ylabel="True Positive Rate", aspect=1)
pr = Axis(f[1,2], xlabel="True Positive Rate", ylabel="Positive Pred. Value", aspect=1)

scatter!(roc, [0.0], [1.0], color=light[8], alpha=0.2, markersize=300)
scatter!(pr, [1.0], [1.0], color=light[8], alpha=0.2, markersize=300)

arrows!(roc, [0.5], [0.5], [-0.2], [0.2], color=light[8], linewidth=4)
arrows!(roc, [0.5], [0.5], [0.2], [-0.2], color=light[3], linewidth=2, linestyle=:dash)

lines!(roc, [0.0, 1.0], [0.0, 1.0], color=:black, linestyle=:dash)
lines!(pr, [0.0, 1.0], [0.5, 0.5], color=:black, linestyle=:dash)

arrows!(pr, [0.5], [0.5], [0.2], [0.2], color=light[8], linewidth=4)
arrows!(pr, [0.5], [0.5], [-0.2], [-0.2], color=light[3], linewidth=2, linestyle=:dash)

for i in axes(CT, 1)
    lines!(roc, fpr.(CT[i,:]), tpr.(CT[i,:]), color=vibrant[1])
    lines!(pr, tpr.(CT[i,:]), ppv.(CT[i,:]), color=vibrant[1])
end

for ax in [pr, roc]
    xlims!(ax, (0, 1))
    ylims!(ax, (0, 1))
    scatter!(ax, 0.5, 0.5, color=:black)
end

current_figure()
```

### Fine-tuning the NBC prior

In the previous section, we have assumed that the prior on occurrences $P(+)$ was one half, which is a decision we can revisit. But changing this value would probably require that we also change the threshold, and for this reason we need to optimize both hyperparameters at the same time. We present the results of a simple grid search in @fig-tuning-both.

::: column-margin
A grid search in an exhaustive sweep of all possible combinations of parameter values. In order to make the process more efficient, refined approaches like successive halvings [@jamieson2016] can be used.
:::

Based on these results, it appears that changing the value of the prior has very little impact on the best MCC we can achieve: the threshold is simply adjusted to reflect the fact that we assume occurrences to be increasingly likely. In this example, there is very little incentive for us to change the value of the prior, as it would have a very small effect on the overall performance of the model. For this reason, we will keep the previous model ($P(+) = 0.5$ and $\tau \approx  `{julia} round(Ï; digits=2)`$) as the best one.

Just because we decided to use a learning curve does not mean we *have* to change the hyper-parameters. Sometimes, this approach reveals that the value of an hyper-parameter is not really important to model performance, and we need to make a decision on what to do next. Here, although there are marginal changes in the value of the MCC, they do not feel significant enough to change the value of the prior.

```{julia}
#| echo: false
#| label: fig-tuning-both
#| fig-cap: learning curve for the threshold and prior
fig = Figure(; resolution=(600, 400))
ax = Axis(fig[1,1]; xlabel="P(+)", ylabel="MCC")

T = LinRange(0.0, 1.0, 8)
Pr = LinRange(0.2, 0.8, 3)
Threads.@threads for pr in Pr
    CT = hcat([first(crossvalidate(naivebayes, y, X, folds, Ï; transformation=tf, presence=pr)) for Ï in T]...)
    mccs = vec(mean(mcc.(CT); dims=1))
    bm, bi = findmax(mccs)
    scatter!(ax, pr, bm, color=T[bi], colorrange=(0., 1.), colormap=cgrad(iridescent), strokecolor=:black, strokewidth=1)
end

Colorbar(fig[1,2], colorrange=(0, 1), colormap=cgrad(iridescent), label="Optimal threshold")

xlims!(ax, (0., 1.))
ylims!(ax, (0.87, 0.89))

current_figure()
```

### Comparison with another classifier

```{julia}
#| echo: false
#| output: false
T = LinRange(0.0, 1.0, 150)
BT = hcat([first(crossvalidate(bioclim, y, ptm["features"], folds, Ï)) for Ï in T]...)
Ïb = T[last(findmax(vec(mean(mcc.(BT); dims=1))))]
```

```{julia}
#| echo: false
#| output: false
Cv, Ct = crossvalidate(naivebayes, y, X, folds, Ï; transformation=tf)
uCv, uCt = crossvalidate(naivebayes, y, X, folds; transformation=tf)
bCv, bCt = crossvalidate(bioclim, y, ptm["features"], folds, Ïb)
```

```{julia}
#| echo: false
#| output: false
model = naivebayes(y, X; transformation=tf)
_layer_path = joinpath(dirname(Base.active_project()), "data", "general", "layers.tiff")
bio = [SpeciesDistributionToolkit._read_geotiff(_layer_path, SimpleSDMResponse; bandnumber=i) for i in v]

pr = convert(Float64, similar(first(bio)))
bs = convert(Float64, similar(first(bio)))
en = convert(Float64, similar(first(bio)))

submodels = [naivebayes(y, X[b,:]; transformation=tf) for b in bootstrap(y, X; n=500)]

Threads.@threads for k in keys(pr)
    x = [bio[i][k] for i in axes(v, 1)]
    pr[k] = model(x)
    en[k] = entropy(pr[k])
    bs[k] = iqr([m(x) for m in submodels])
end
```

### Testing the final model

As we are now considering that our model is adequately trained, we can finally apply it to the testing data we had set aside early in @sec-classification. Applying the trained model to this data provides a fair estimate of the expected model performance, and relaying this information to people who are going to use the model is important. For this reason, we will also compare the model to BIOCLIM PLUS compare to validation performance on old model

We are *not* applying the older versions of the model to the testing data, as we had decided against this. We had established the rule of "we pick the best model as the one with the highest validation MCC", and this is what we will stick to. To do otherwise would be the applied machine learning equivalent of $p$-hacking.

```{julia}
#| echo: false
#| output: false
ytest, Xtest = JLD.load(joinpath(_ptm_path, "sdm-step-0.jld"))["testing"]
preds = vec(mapslices(model, Xtest[:,v], dims=2))
C = ConfusionMatrix(preds, ytest, Ï)
```

We can start by taking a look at the confusion matrix on the testing data:

$$
\begin{pmatrix}
`{julia} C.tp` & `{julia} C.fp` \\
`{julia} C.fn` & `{julia} C.tn`
\end{pmatrix}
$$

This is very promising! There are far more predictions on the diagonal (`{julia} C.tp + C.tn`) than outside of it (`{julia} C.fp + C.fn`), which suggests an accurate classifier. But in order to really evaluate the model, we will report a little more information: TSS FOM FDI F1

### Reporting the final predictions


```{julia}
#| echo: false
#| output: false
f = Figure()
ax = Axis(f[1,1], aspect=DataAspect())
heatmap!(ax, pr .>= Ï, colormap=vibrant[1:2])
current_figure()
```

## Conclusion

In this chapter, we...

In @sec-explanations, we will start asking "why"? Specifically, we will see a series of tools to evaluate why the model was making a specific prediction at a specific place, and look at the relationship between the importance of variables for model performance and for actual predictions.

```{julia}
#| output: false
#| echo: false
_ptm_path = joinpath(dirname(Base.active_project()), "checkpoints")
modelpath = joinpath(_ptm_path, "sdm-step-2.jld")
JLD.save(
    modelpath,
    "labels", y,
    "features", X,
    "folds", folds,
    "variables", v,
    "transformation", Whitening,
    "threshold", Ï
)
```