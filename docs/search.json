[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning for Biodiversity Scientists",
    "section": "",
    "text": "Preface\nMachine learning is now an established methodology to study biodiversity, and this is a problem.\nThis may be an opportunity when it comes to advancing our knowledge of biodiversity, and in particular when it comes to translating this knowledge into action (Tuia et al. 2022); but make no mistake, this is a problem for us, biodiversity scientists, as we suddenly need to develop competences in an entirely new field in order to remain professionally relevant (Ellwood et al. 2019). And as luck would have it, there are easier fields to master than machine learning. The point of this book, therefore, is to provide an introduction to fundamental concepts in data science, from the perspective of a biodiversity scientist, by using examples corresponding to real-world use-cases of these techniques.\n\n\nThink of data science as being its own epistemology (Desai et al. 2022), and machine learning as one methodology we can apply to work within this context.\nBut what do we mean by machine learning and data science? Most science, after all, relies on data in some capacity. What falls under the umbrella of data science is, in short, embracing in equal measure quantitative skills (mathematics, machine learning, statistics), programming, and domain expertise, in order to solve well-defined problems. Machine learning is a series of techniques (or, more precisely, a high-level approach to these techniques) through which we conduct our data science activities. A core tenet of data science is that, when using it, we seek to “deliver actionable insights”, which is MBA-speak for “figuring out what to do next”. One of the ways in which this occurs is by letting the data speak, after they have been, of course, properly cleaned and transformed and engineered. This entire process is driven by (or, even, subject to) domain knowledge. There is no such thing as data science, at least not in a vacuum: there is data science as a methodology applied to a specific domain.\nBefore we embark into a journey of discovery on the applications of data science to biodiversity, allow me to let you in on a little secret: data science is a little bit of a misnomer. In order to understand why, I need (or at least, I really want) to talk about cooking.\nTo become a good cook, there are general techniques one must master, which we apply to specific steps in recipes; these recipes draw from a common cultural or local repertoire and cultural specifics (but the evolution of recipes is remarkably convergent – most cuisines have a mirepoix, bread, and beer). Finally, there is the product, i.e. the unique dish that you have cooked. And so it is for data science too: we can abstract a series of processes and guidelines, think about their application within the context of our specific field, study system, or line and research, and all of this will shape the final data product we can serve.\nWhen writing this preface, I turned to my shelf of cookbooks, and picked my two favorites: Robuchon’s The Complete Robuchon (a no-nonsense list of hundreds of recipes with no place for improvisation), and Bianco’s Pizza, Pasta, and Other Food I Like (a short volume with very few pizza and pasta, and wonderful discussions about the importance of humility, creativity, and generosity). Data science, if it were cooking, would feel a lot like the second. Deviation from the rules is often justifiable if you feel like it. But this improvisation requires good skills, a clear mental map of the problem, a defined vision of what these deviations will let you achieve, and a library of patterns that you can draw from.\nThis book will not get you here. But it will speed up the process, by framing the practice of data science as a natural way to conduct research on biodiversity.\n\n\n\n\n\n\n\nReferences\n\n\nDesai, J., Watson, D., Wang, V., Taddeo, M. & Floridi, L. (2022). The epistemological foundations of data science: a critical review. Synthese, 200.\n\n\nEllwood, E.R., Sessa, J.A., Abraham, J.K., Budden, A.E., Douglas, N., Guralnick, R., et al. (2019). Biodiversity Science and the Twenty-First Century Workforce. BioScience, 70, 119–121.\n\n\nTuia, D., Kellenberger, B., Beery, S., Costelloe, B.R., Zuffi, S., Risse, B., et al. (2022). Perspectives in machine learning for wildlife conservation. Nature Communications, 13, 792.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 Core concepts in data science\nThis book started as a collection of notes from several classes I taught in the Department of Biological Sciences at the Université de Montréal, as well as a few workshops I ran for the Québec Centre for Biodiversity Sciences. When teaching data synthesis, data science, and machine learning to biology students, I realized that the field was missing resources that could serve as stepping stones to proficiency.\nThere are excellent manuals covering the mathematics of data science and machine learning (I will list a few later on). These are important to read, because the field of machine learning is an offshoot of mathematics and computer science, and it is important to become familiar with the core concepts. A little bit of calculus and a whole lot of linear algebra should be more of the same for many ecologists. But these resources are usually less useful as practical guides to the field.\nThere are many good papers giving overviews of some applications of data science to biological problems (a lot of them are cited in this book). These are important to read, because any attempt to adopt a new methodology (new to us, not new to the field, or new in absolute terms!) must proceed alongside some familiarity of how it has been used by our colleagues. But these articles, although good at showing how these tools are actually used, usually make it difficult to establish more general recommendations.\nThere are, finally, thousands of tutorials about how to write code to perform any machine learning algorithm you can think of. Some of them are even good. But these tutorials usually suffer (in our case) from being disconnected from the field of biodiversity science, and of course are limited by the language they use, the version of the packages they ran with, and again do not allow for much generalization.\nWhen navigating these resources, one thing that students commonly called for was an attempt to tie concepts together, and to explain when and how human decisions were required in ML approaches (Sulmont et al. 2019). This is particularly true of students with strong domain knowledge that want to understand how machine learning fits with their ability to do research.\nThis is book is this attempt.\nThere are, broadly speaking, two situations in which reading this book is useful. The first is when you are done reading some general books about machine learning, and want to see how it can be applied to problems that are more specific to biodiversity research; the second is when you have a working understanding of biodiversity research, and want a stepping stone into the machine learning literature. Note that there is no scenario where you stop after reading this book – this is by design. The purpose of this book is to give a practical overview of “how data science for biodiversity happens”, and this needs to be done in parallel to even more fundamental readings.\nA wonderful introduction to the mathematics behind machine learning can be found in Deisenroth et al. (2020), which provides stunning visualization of mathematical concepts. Yau (2015) is a particularly useful book about the ways to visualize data in a meaningful way. Watt et al. (2020) is a solid introduction to the underlying theory of applied machine learning. For ecologists, Dietze (2017) is a comprehensive, and still highly readable, treaty on the problems associated to forecasting. The best way to decide on which book to read is often to look at the books that your colleagues have also read; being able to work through material collectively is useful, and knowing that you can practice the craft of data science within a community will make your learning more effective.\nWhen reading this book, I encourage you to read the chapters in order. They have been designed to be read in order, because each chapter introduces the least possible amount of new concepts, but often requires to build on the previous chapters. This is particularly true of the second half of this book.\nG\n\n\n\ntraining\n\n\nTraining data\n\n\n\nmodel\n\n\nModel training\n\n\n\ntraining-&gt;model\n\n\n\n\n\ncrossval\n\n\nCross-validation\n\n\n\ntraining-&gt;crossval\n\n\n\n\n\ntesting\n\n\nTesting data\n\n\n\ntest\n\n\nPerformance test\n\n\n\ntesting-&gt;test\n\n\n\n\n\nprediction\n\n\nPrediction\n\n\n\nmodel-&gt;prediction\n\n\n\n\n\nprediction-&gt;crossval\n\n\n\n\n\ncvplus\n✓\n\n\n\ncrossval-&gt;cvplus\n\n\n\n\n\ncvminus\n✗\n\n\n\ncrossval-&gt;cvminus\n\n\n\n\n\ncvplus-&gt;test\n\n\n\n\n\ntestplus\n✓\n\n\n\ntest-&gt;testplus\n\n\n\n\n\ntestminus\n✗\n\n\n\ntest-&gt;testminus\n\n\n\n\n\nuse\n\nUsable model\n\n\n\ntestplus-&gt;use\n\n\n\n\n\n\n\n\nFlowchart 1.1: An overview of the process of coming up with a usable model. The process of creating a model starts with a trainig dataset made of predictors and responses, which is used to train a model. This model is cross-validated on its training data, to estimate whether it can be fully retrained. The fully trained model is that applied to an independent testing dataset, and the evaluation of the performance determines whether it will be used.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#core-concepts-in-data-science",
    "href": "intro.html#core-concepts-in-data-science",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1.1 EDA\n\n\n1.1.2 Clustering and regression\n\n\n1.1.3 Supervised and unsupervised\n\n\n1.1.4 Training, testing, and validation\n\n\n1.1.5 Transformations and feature engineering",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#an-overview-of-the-content",
    "href": "intro.html#an-overview-of-the-content",
    "title": "1  Introduction",
    "section": "1.2 An overview of the content",
    "text": "1.2 An overview of the content\nIn ?sec-clustering, we introduce some fundamental questions in data science, by working on the clustering of pixels in Landsat data. The point of this chapter is to question the way we think about data, and to start a discussion about an “optimal” model, hyper-parameters, and what a “good” model is.\nIn ?sec-gradientdescent, we revisit well-trodden statistical ground, by fitting a linear model to linear data, but uisng gradient descent. This provides us with an opportunity to think about what a “fitted” model is, whether it is possible to learn too much from data, and why being able to think about predictions in the unit of our problem helps.\nIn ?sec-crossvalidation, we start introducing one of the most important bit element of data science practice, in the form of cross-validation. We apply this technique to the prediction of plant phenology over a millenia, and think about the central question of “what kind of decision-making can we justify with a model”.\nIn ?sec-classification, we introduce the task of classification, and spend a lot of time thinking about biases in predictions, which are acceptable, and which are not. We start building a model for the distribution of the Reindeer, which we will improve over a few chapters.\nIn ?sec-predictors, we explore ways to perform variable selection, think of this task as being part of the training process, and introduce ideas related to dimensionality reduction. In ?sec-leakage, we discuss data leakage, where it comes from, and how to prevent it. This leads us to introducing the concept of data transformations as a model, which will establish some best practices we will keep on using throughout this book.\nIn ?sec-tuning, we conclude story arcs that had been initiated in a few previous chapters, and explore training curves, the tuning of hyper-parameters, and moving-threshold classification. We provide the final refinements to out model of the Reindeer distribution.\nIn Chapter 2, we will shift our attention from prediction to understanding, and explore techniques to quantify the importance of variables, as well as ways to visualize their contribution to the predictions. In doing so, we will introduce concepts of model interpretation and explainability.\nIn ?sec-bagging, …",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#sec-introduction-colors",
    "href": "intro.html#sec-introduction-colors",
    "title": "1  Introduction",
    "section": "1.3 A note on colors",
    "text": "1.3 A note on colors\n\n\n\nType\nMeaning\nColor\n\n\n\n\nAll\ngeneric\n\n\n\n\nno data\n\n\n\nCross-validation\ntraining\n\n\n\n\nvalidation\n\n\n\n\ntesting\n\n\n\nSpecies range\npresence\n\n\n\n\nabsence\n\n\n\nRange change\nloss\n\n\n\n\nno change\n\n\n\n\ngain\n\n\n\n\nIn addition, there are three important color palettes. Information that is sequential is nature, which is to say it increases on a continuous scale without a logical midpoint, is rendered with these colors (from low to the left, to high values to the right):\n\n\n\n\n\nThe diverging palette is used for values that have a clear midpoint (usually values centered on 0). The midpoint will always correspond to the central color, and this palette is symmetrical:\n\n\n\n\n\nFinally, the categorical data are represented using the following palette:",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#some-rules-about-this-book",
    "href": "intro.html#some-rules-about-this-book",
    "title": "1  Introduction",
    "section": "1.4 Some rules about this book",
    "text": "1.4 Some rules about this book\nWhen I started aggregating these notes, I decided on a series of four rules. No code, no simulated data, no long list of model, and above all, no iris dataset. In this section, I will go through why I decided to adopt these rules, and how it should change the way you interact with the book.\n\n1.4.1 No code\nThis is, maybe, the most surprising rule, because data science is programming (in a sense). But sometimes there is so much focus on programming that we lose track of the other, important aspects of the practice of data science: abstractions, relationship with data, and domain knowledge.\nThis book did involve a lot of code. Specifically, this book was written using Julia (Bezanson et al. 2017), and every figure is generated by a notebook, and they are part of the material I use when teaching from this content in the classroom. But code is not a universal language, and unless you are really familiar with the language, code can obfuscate. I had no intention to write a Julia book (or an R book, or a Python book). The point is to think about data science applied to ecological research, and I felt like it would be more inclusive to do this in a language agnostic way.\nAnd finally, code rots. Code with more dependencies rots faster. It take a single change in the API of a package to break the examples, and then you are left with a very expensive monitor stand. With a few exceptions, the examples in this book do not use complicated packages either.\n\n\n1.4.2 No simulated data\nI have nothing against simulated data. I have, in fact, generated simulated data in many different contexts, for training or for research. But the limit of simulated is that we almost inevitably fail to include what makes real data challenging: noise, incomplete or uneven sampling, data representation artifacts. And so when it is time to work on real data, everything seems suddenly more difficult.\nSimulated data have immense training value; but it is also important to engage with the imperfect actual data, as we will overwhelmingly apply the concepts from this book to them. For this reason, there are no simulated data in this book. Everything that is presented correspond to an actual use case that proceeds from a question we could reasonably ask in the context, paired with a dataset that could be used to answer this question.\n\n\n1.4.3 No model zoo\nMy favorite machine learning package is MLJ (Blaom et al. 2020). When given a table of labels and a table of features, it will give back a series of models that match with these data. It speeds up the discovery of models considerably, and is generally a lot more informative than trying to read from a list of possible techniques. If I have questions about an algorithm from this list, I can start reading more documentation about how it works.\nReading a long enumeration of things is boring; unless it’s sung by Yakko Warner, I’m not interested, and I refuse to inflict it on people. But more importantly, these enumerations of models often distract from thinking about the problem we want to solve in more abstract terms. I rarely wake up in the morning and think “oh boy I can’t wait to train a SVM today”; chances are, my thought process will be closer to “I need to tell the mushroom people where I think the next good foraging locations will be”. The rest, is implementation details.\nIn fact, 90% of this book uses only two models: linear regression, and the Naïve Bayes Classifier. Some other models are involved in a few chapters, but these two models are breathtakingly simple, work surprisingly well, run fast, and can be tweaked to allow us to build deep intuitions about how machines learn. They are perfect for the classroom, and give us the freedom to spent most of our time thinking about how we interact with models, and why, and how we make methodological decisions.\n\n\n1.4.4 No iris dataset\nFrom a teaching point of view, the iris dataset is like hearing Smash Mouth in a movie trailer, in that it tells you two things with absolute certainty. First, that you are indeed watching a movie trailer. Second, that you could be watching Shrek instead. There are datasets out there that are infinitely more exciting to use than iris.\nBut there is a far more important reason not to use iris: eugenics.\nListen, we made it several hundred words in a text about quantitative techniques in life sciences without encountering a sad little man with racist ideas that academia decided to ignore because “he just contributed so much to the field, and these were different times, maybe we shouldn’t be so quick to judge?”. Ronald Aylmer Fisher, statistics’ most racist nerd, was such a man; and there are, of course, those who want to consider the possibility that you can be outrageously racist as long as you are an outstanding scientist (Bodmer et al. 2021).\nThe iris dataset was first published by Fisher (1936) in the Annals of Eugenics (so, there’s a bit of a red flag there already), and draws from several publications by Edgar Anderson, starting with Anderson (1928); Unwin & Kleinman (2021) have an interesting historiographic deep-dive into the correspondence between the two. Judging by the dates, you may think that Fisher was a product of his time. But this could not be further from the truth. Fisher was dissatisfied with his time, to the point where his contributions to statistics were done in service of his views, in order to provide the appearance of scientific rigor to his bigotry.\nFisher advocated for forced sterilization for the “defectives” (which he estimated at, oh, roughly 10% of the population), argued that not all races had equal capacity for intellectual and emotional development, and held a host of related opinions. There is no amount of contribution to science that pardon these views. Coming up with the idea of the null hypothesis does not even out lending “scientific” credibility to ideas whose logical (and historical) conclusion is genocide. That Ronald Fisher is still described as a polymath and a genius is infuriating, and we should use every alternative to his work that we have.\nThankfully, there are alternatives!\nThe most broadly known alternative to the iris dataset is penguins, which was collected by ecologists (Gorman et al. 2014), and published as a standard dataset (Horst et al. 2020) so that we can train students without engaging with the “legacy” of eugenicists. The penguins dataset is also genuinely good! The classes are not so obviously separable, there are some missing data that reflect the reality of field work, and the data about sex and spatial location have been preserved, which increases the diversity of questions we can ask. We won’t use penguins either. It’s a fine dataset, but at this point there is little that we can write around it that would be new, or exciting. But if you want to apply some of the techniques in this book? Go penguins.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/explanations.html",
    "href": "chapters/explanations.html",
    "title": "2  Explaining predictions",
    "section": "",
    "text": "2.0.1 Partial responses\nIn this chapter, we will\nnavigate the accuracy-explainability for public policy Bell et al. (2022)\nwhat is explainable differs between stakeholders Amarasinghe et al. (2023)\nbiodiversity need sustained model uptake Weiskopf et al. (2022)\nvalues of variable against mean of all others",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Explaining predictions</span>"
    ]
  },
  {
    "objectID": "chapters/explanations.html#application",
    "href": "chapters/explanations.html#application",
    "title": "2  Explaining predictions",
    "section": "2.1 Application",
    "text": "2.1 Application\n\n2.1.1 Partial responses\n\n\n\n\n\n\n\nFigure 2.1: TODO\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.2: TODO\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.3: TODO\n\n\n\n\n\n\n2.1.2 Shapley values\n\nS = zeros(Float64, (length(variables(model)), length(labels(model))))\nfor (vidx, vpos) in enumerate(variables(model))\n    S[vidx,:] = explain(model, vpos; threshold=false, samples=200)\nend\nP = features(model, variables(model))\n\n4×2230 Matrix{Float64}:\n  6.15002   8.75   8.45001  13.75  …  13.85  14.65   9.25  14.05  13.85\n 26.3      22.1   25.2      19.6      21.7   18.6   26.4   18.8   19.3\n 42.2      40.7   44.3      45.0      49.6   44.9   47.1   43.6   49.6\n 23.15     22.95  24.85     26.45     28.15  26.85  26.35  26.35  26.35\n\n\nTODO redraw the stemplot from the variable selection chapter to compare prediction v. explanation\n\n\n\nTable 2.1: blah blah blah\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nImp. (Shapley)\nImp. (bootstrap)\nMin.\nMed.\nMax.\n\n\n\n\nBIO 8\n39.59%\n35.25%\n-0.29\n-0.09\n0.56\n\n\nBIO 7\n33.74%\n25.45%\n-0.40\n-0.05\n0.30\n\n\nBIO 15\n14.91%\n8.88%\n-0.38\n0.00\n0.18\n\n\nBIO 5\n11.76%\n6.61%\n-0.24\n-0.01\n0.20\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.4: Effect of each variable (sorted by importance as in Table 2.1) on the change of the score for a single prediction. Recall that this is expressed as the change from the average prediction made by the model.\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.5: Effect of each variable (sorted by importance as in Table 2.1) on the change of the score for a single prediction. Recall that this is expressed as the change from the average prediction made by the model.\n\n\n\n\n\nf = Figure(; size=(600, 400))\nargs = (color=predict(model), markersize=5, colorrange=(0., 1.))\n\nax1 = Axis(f[1,1]; xlabel=\"BIO $(model.v[varord[1]])\")\nscatter!(ax1, P[varord[1],:], S[varord[1],:]; args...)\nax2 = Axis(f[1,2]; xlabel=\"BIO $(model.v[varord[2]])\")\nscatter!(ax2, P[varord[2],:], S[varord[2],:]; args...)\nax3 = Axis(f[2,1]; xlabel=\"BIO $(model.v[varord[3]])\")\nscatter!(ax3, P[varord[3],:], S[varord[3],:]; args...)\nax4 = Axis(f[2,2]; xlabel=\"BIO $(model.v[varord[4]])\")\nscatter!(ax4, P[varord[4],:], S[varord[4],:]; args...)\n\nxmin, xmax = extrema(S)\nfor ax in [ax1, ax2, ax3, ax4]\n    hlines!(ax, [0.0], color=:black, linestyle=:dash)\nend\n\ncurrent_figure()\n\n\n\n\n\n\n2.1.3 Spatial partial effects\n\n_layer_path = joinpath(dirname(Base.active_project()), \"data\", \"occurrences\", \"layers.tiff\")\nbio = [SimpleSDMLayers._read_geotiff(_layer_path; bandnumber=i) for i in 1:19]\n\n19-element Vector{SDMLayer{Float32}}:\n SDMLayer{Float32}(Float32[Inf Inf … Inf Inf; Inf Inf … Inf Inf; … ; Inf Inf … Inf Inf; Inf Inf … Inf Inf], Bool[0 0 … 0 0; 0 0 … 0 0; … ; 0 0 … 0 0; 0 0 … 0 0], (8.533193690349995, 9.549860352949993), (41.33319391914997, 43.00819391245001), \"+proj=longlat +datum=WGS84 +no_defs\")\n SDMLayer{Float32}(Float32[Inf Inf … Inf Inf; Inf Inf … Inf Inf; … ; Inf Inf … Inf Inf; Inf Inf … Inf Inf], Bool[0 0 … 0 0; 0 0 … 0 0; … ; 0 0 … 0 0; 0 0 … 0 0], (8.533193690349995, 9.549860352949993), (41.33319391914997, 43.00819391245001), \"+proj=longlat +datum=WGS84 +no_defs\")\n SDMLayer{Float32}(Float32[Inf Inf … Inf Inf; Inf Inf … Inf Inf; … ; Inf Inf … Inf Inf; Inf Inf … Inf Inf], Bool[0 0 … 0 0; 0 0 … 0 0; … ; 0 0 … 0 0; 0 0 … 0 0], (8.533193690349995, 9.549860352949993), (41.33319391914997, 43.00819391245001), \"+proj=longlat +datum=WGS84 +no_defs\")\n SDMLayer{Float32}(Float32[Inf Inf … Inf Inf; Inf Inf … Inf Inf; … ; Inf Inf … Inf Inf; Inf Inf … Inf Inf], Bool[0 0 … 0 0; 0 0 … 0 0; … ; 0 0 … 0 0; 0 0 … 0 0], (8.533193690349995, 9.549860352949993), (41.33319391914997, 43.00819391245001), \"+proj=longlat +datum=WGS84 +no_defs\")\n SDMLayer{Float32}(Float32[Inf Inf … Inf Inf; Inf Inf … Inf Inf; … ; Inf Inf … Inf Inf; Inf Inf … Inf Inf], Bool[0 0 … 0 0; 0 0 … 0 0; … ; 0 0 … 0 0; 0 0 … 0 0], (8.533193690349995, 9.549860352949993), (41.33319391914997, 43.00819391245001), \"+proj=longlat +datum=WGS84 +no_defs\")\n SDMLayer{Float32}(Float32[Inf Inf … Inf Inf; Inf Inf … Inf Inf; … ; Inf Inf … Inf Inf; Inf Inf … Inf Inf], Bool[0 0 … 0 0; 0 0 … 0 0; … ; 0 0 … 0 0; 0 0 … 0 0], (8.533193690349995, 9.549860352949993), (41.33319391914997, 43.00819391245001), \"+proj=longlat +datum=WGS84 +no_defs\")\n SDMLayer{Float32}(Float32[Inf Inf … Inf Inf; Inf Inf … Inf Inf; … ; Inf Inf … Inf Inf; Inf Inf … Inf Inf], Bool[0 0 … 0 0; 0 0 … 0 0; … ; 0 0 … 0 0; 0 0 … 0 0], (8.533193690349995, 9.549860352949993), (41.33319391914997, 43.00819391245001), \"+proj=longlat +datum=WGS84 +no_defs\")\n SDMLayer{Float32}(Float32[Inf Inf … Inf Inf; Inf Inf … Inf Inf; … ; Inf Inf … Inf Inf; Inf Inf … Inf Inf], Bool[0 0 … 0 0; 0 0 … 0 0; … ; 0 0 … 0 0; 0 0 … 0 0], (8.533193690349995, 9.549860352949993), (41.33319391914997, 43.00819391245001), \"+proj=longlat +datum=WGS84 +no_defs\")\n SDMLayer{Float32}(Float32[Inf Inf … Inf Inf; Inf Inf … Inf Inf; … ; Inf Inf … Inf Inf; Inf Inf … Inf Inf], Bool[0 0 … 0 0; 0 0 … 0 0; … ; 0 0 … 0 0; 0 0 … 0 0], (8.533193690349995, 9.549860352949993), (41.33319391914997, 43.00819391245001), \"+proj=longlat +datum=WGS84 +no_defs\")\n SDMLayer{Float32}(Float32[Inf Inf … Inf Inf; Inf Inf … Inf Inf; … ; Inf Inf … Inf Inf; Inf Inf … Inf Inf], Bool[0 0 … 0 0; 0 0 … 0 0; … ; 0 0 … 0 0; 0 0 … 0 0], (8.533193690349995, 9.549860352949993), (41.33319391914997, 43.00819391245001), \"+proj=longlat +datum=WGS84 +no_defs\")\n SDMLayer{Float32}(Float32[Inf Inf … Inf Inf; Inf Inf … Inf Inf; … ; Inf Inf … Inf Inf; Inf Inf … Inf Inf], Bool[0 0 … 0 0; 0 0 … 0 0; … ; 0 0 … 0 0; 0 0 … 0 0], (8.533193690349995, 9.549860352949993), (41.33319391914997, 43.00819391245001), \"+proj=longlat +datum=WGS84 +no_defs\")\n SDMLayer{Float32}(Float32[Inf Inf … Inf Inf; Inf Inf … Inf Inf; … ; Inf Inf … Inf Inf; Inf Inf … Inf Inf], Bool[0 0 … 0 0; 0 0 … 0 0; … ; 0 0 … 0 0; 0 0 … 0 0], (8.533193690349995, 9.549860352949993), (41.33319391914997, 43.00819391245001), \"+proj=longlat +datum=WGS84 +no_defs\")\n SDMLayer{Float32}(Float32[Inf Inf … Inf Inf; Inf Inf … Inf Inf; … ; Inf Inf … Inf Inf; Inf Inf … Inf Inf], Bool[0 0 … 0 0; 0 0 … 0 0; … ; 0 0 … 0 0; 0 0 … 0 0], (8.533193690349995, 9.549860352949993), (41.33319391914997, 43.00819391245001), \"+proj=longlat +datum=WGS84 +no_defs\")\n SDMLayer{Float32}(Float32[Inf Inf … Inf Inf; Inf Inf … Inf Inf; … ; Inf Inf … Inf Inf; Inf Inf … Inf Inf], Bool[0 0 … 0 0; 0 0 … 0 0; … ; 0 0 … 0 0; 0 0 … 0 0], (8.533193690349995, 9.549860352949993), (41.33319391914997, 43.00819391245001), \"+proj=longlat +datum=WGS84 +no_defs\")\n SDMLayer{Float32}(Float32[Inf Inf … Inf Inf; Inf Inf … Inf Inf; … ; Inf Inf … Inf Inf; Inf Inf … Inf Inf], Bool[0 0 … 0 0; 0 0 … 0 0; … ; 0 0 … 0 0; 0 0 … 0 0], (8.533193690349995, 9.549860352949993), (41.33319391914997, 43.00819391245001), \"+proj=longlat +datum=WGS84 +no_defs\")\n SDMLayer{Float32}(Float32[Inf Inf … Inf Inf; Inf Inf … Inf Inf; … ; Inf Inf … Inf Inf; Inf Inf … Inf Inf], Bool[0 0 … 0 0; 0 0 … 0 0; … ; 0 0 … 0 0; 0 0 … 0 0], (8.533193690349995, 9.549860352949993), (41.33319391914997, 43.00819391245001), \"+proj=longlat +datum=WGS84 +no_defs\")\n SDMLayer{Float32}(Float32[Inf Inf … Inf Inf; Inf Inf … Inf Inf; … ; Inf Inf … Inf Inf; Inf Inf … Inf Inf], Bool[0 0 … 0 0; 0 0 … 0 0; … ; 0 0 … 0 0; 0 0 … 0 0], (8.533193690349995, 9.549860352949993), (41.33319391914997, 43.00819391245001), \"+proj=longlat +datum=WGS84 +no_defs\")\n SDMLayer{Float32}(Float32[Inf Inf … Inf Inf; Inf Inf … Inf Inf; … ; Inf Inf … Inf Inf; Inf Inf … Inf Inf], Bool[0 0 … 0 0; 0 0 … 0 0; … ; 0 0 … 0 0; 0 0 … 0 0], (8.533193690349995, 9.549860352949993), (41.33319391914997, 43.00819391245001), \"+proj=longlat +datum=WGS84 +no_defs\")\n SDMLayer{Float32}(Float32[Inf Inf … Inf Inf; Inf Inf … Inf Inf; … ; Inf Inf … Inf Inf; Inf Inf … Inf Inf], Bool[0 0 … 0 0; 0 0 … 0 0; … ; 0 0 … 0 0; 0 0 … 0 0], (8.533193690349995, 9.549860352949993), (41.33319391914997, 43.00819391245001), \"+proj=longlat +datum=WGS84 +no_defs\")\n\n\n\nV = explain(model, bio; threshold=false, samples=30)\n\n4-element Vector{SDMLayer{Float64}}:\n SDMLayer{Float64}([0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Bool[0 0 … 0 0; 0 0 … 0 0; … ; 0 0 … 0 0; 0 0 … 0 0], (8.533193690349995, 9.549860352949993), (41.33319391914997, 43.00819391245001), \"+proj=longlat +datum=WGS84 +no_defs\")\n SDMLayer{Float64}([0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Bool[0 0 … 0 0; 0 0 … 0 0; … ; 0 0 … 0 0; 0 0 … 0 0], (8.533193690349995, 9.549860352949993), (41.33319391914997, 43.00819391245001), \"+proj=longlat +datum=WGS84 +no_defs\")\n SDMLayer{Float64}([0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Bool[0 0 … 0 0; 0 0 … 0 0; … ; 0 0 … 0 0; 0 0 … 0 0], (8.533193690349995, 9.549860352949993), (41.33319391914997, 43.00819391245001), \"+proj=longlat +datum=WGS84 +no_defs\")\n SDMLayer{Float64}([0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Bool[0 0 … 0 0; 0 0 … 0 0; … ; 0 0 … 0 0; 0 0 … 0 0], (8.533193690349995, 9.549860352949993), (41.33319391914997, 43.00819391245001), \"+proj=longlat +datum=WGS84 +no_defs\")\n\n\n\nf = Figure()\na1 = Axis(f[1,1])\na2 = Axis(f[1,2])\nheatmap!(a1, V[varord[1]], colormap=bkcol.div, colorrange=(-0.5,0.5))\nheatmap!(a2, partialresponse(model, bio, variables(model)[varord[1]]; threshold=false))\ncurrent_figure()\n\n\n\n\n\n\n2.1.4 Most important variable locally\n\n\n\n\n\n\n\nFigure 2.6: TODO",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Explaining predictions</span>"
    ]
  },
  {
    "objectID": "chapters/explanations.html#conclusion",
    "href": "chapters/explanations.html#conclusion",
    "title": "2  Explaining predictions",
    "section": "2.2 Conclusion",
    "text": "2.2 Conclusion",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Explaining predictions</span>"
    ]
  }
]