[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning for Biodiversity Scientists",
    "section": "",
    "text": "Preface\nData science is now an established methodology to study biodiversity, and this is a problem.\nThis may be an opportunity when it comes to advancing our knowledge of biodiversity, and in particular when it comes to translating this knowledge into action (Tuia et al. 2022); but make no mistake, this is a problem for us, biodiversity scientists, as we suddenly need to develop competences in an entirely new field. And as luck would have it, there are easier fields to master than data science. The point of this book, therefore, is to provide an introduction to fundamental concepts in data science, from the perspective of a biodiversity scientist, by using examples corresponding to real-world use-cases of these techniques.\nBut what do we mean by data science? Most science, after all, relies on data in some capacity. What falls under the umbrella of data science is, in short, embracing in equal measure quantitative skills (mathematics, machine learning, statistics), programming, and domain expertise, in order to solve well-defined problems. A core tenet of data science is that, when using it, we seek to “deliver actionable insights”, which is MBA-speak for “figuring out what to do next”. One of the ways in which this occurs is by letting the data speak, after they have been, of course, properly cleaned and transformed and engineered beyond recognition. This entire process is driven by (or subject to, even) domain knowledge. There is no such thing as data science, at least not in a vacuum: there is data science as a methodology applied to a specific domain.\nBefore we embark into a journey of discovery on the applications of data science to biodiversity, allow me to let you in on a little secret: data science is a little bit of a misnomer.\nTo understand why, it helps to think of science (the application of the scientific method, that is) as cooking. There are general techniques one must master, and specific steps and cultural specifics, and there is a final product. When writing this preface, I turned to my shelf of cookbooks, and picked my two favorites: Robuchon’s The Complete Robuchon (a no-nonsense list of hundreds of recipes with no place for improvisation), and Bianco’s Pizza, Pasta, and Other Food I Like (a short volume with very few pizza and pasta, and wonderful discussions about the importance of humility, creativity, and generosity). Data science, if it were cooking, would feel a lot like the second. Deviation from the rules (they are mostly recommendations, in fact) is often justifiable if you feel like it. But this improvisation requires good skills, a clear mental map of the problem, and a library of patterns that you can draw from.\nThis book will not get you here. But it will speed up the process, by framing the practice of data science as a natural way to conduct research on biodiversity.\n\n\n\n\n\n\n\nReferences\n\n\nTuia, D., Kellenberger, B., Beery, S., Costelloe, B.R., Zuffi, S., Risse, B., et al. (2022). Perspectives in machine learning for wildlife conservation. Nature Communications, 13, 792.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 Core concepts in data science\nG\n\n\n\ntraining\n\n\nTraining data\n\n\n\nmodel\n\n\nModel training\n\n\n\ntraining-&gt;model\n\n\n\n\n\ncrossval\n\n\nCross-validation\n\n\n\ntraining-&gt;crossval\n\n\n\n\n\ntesting\n\n\nTesting data\n\n\n\ntest\n\n\nPerformance test\n\n\n\ntesting-&gt;test\n\n\n\n\n\nprediction\n\n\nPrediction\n\n\n\nmodel-&gt;prediction\n\n\n\n\n\nprediction-&gt;crossval\n\n\n\n\n\ncvplus\n✓\n\n\n\ncrossval-&gt;cvplus\n\n\n\n\n\ncvminus\n✗\n\n\n\ncrossval-&gt;cvminus\n\n\n\n\n\ncvplus-&gt;test\n\n\n\n\n\ntestplus\n✓\n\n\n\ntest-&gt;testplus\n\n\n\n\n\ntestminus\n✗\n\n\n\ntest-&gt;testminus\n\n\n\n\n\nuse\n\nUsable model\n\n\n\ntestplus-&gt;use\n\n\n\n\n\n\n\n\nFlowchart 1.1: An overview of the process of coming up with a usable model. The process of creating a model starts with a trainig dataset made of predictors and responses, which is used to train a model. This model is cross-validated on its training data, to estimate whether it can be fully retrained. The fully trained model is that applied to an independent testing dataset, and the evaluation of the performance determines whether it will be used.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#core-concepts-in-data-science",
    "href": "intro.html#core-concepts-in-data-science",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1.1 EDA\n\n\n1.1.2 Clustering and regression\n\n\n1.1.3 Supervised and unsupervised\n\n\n1.1.4 Training, testing, and validation\n\n\n1.1.5 Transformations and feature engineering",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#an-overview-of-the-content",
    "href": "intro.html#an-overview-of-the-content",
    "title": "1  Introduction",
    "section": "1.2 An overview of the content",
    "text": "1.2 An overview of the content\nIn Chapter 2, we introduce some fundamental questions in data science, by working on the clustering of pixels in Landsat data. The point of this chapter is to question the way we think about data, and to start a discussion about an “optimal” model, hyper-parameters, and what a “good” model is.\nIn ?sec-gradientdescent, we revisit well-trodden statistical ground, by fitting a linear model to linear data, but uisng gradient descent. This provides us with an opportunity to think about what a “fitted” model is, whether it is possible to learn too much from data, and why being able to think about predictions in the unit of our problem helps.\nIn ?sec-crossvalidation, we start introducing one of the most important bit element of data science practice, in the form of cross-validation. We apply this technique to the prediction of plant phenology over a millenia, and think about the central question of “what kind of decision-making can we justify with a model”.\nIn ?sec-leakage, we discuss data leakage, where it comes from, and how to prevent it. This leads us to introducing the concept of data transformations as a model, which will establish some best practices we will keep on using throughout this book.\nIn ?sec-classification, we introduce the task of classification, and spend a lot of time thinking about biases in predictions, which are acceptable, and which are not. We start building a model for the distribution of the Reindeer, which we will improve over a few chapters.\nIn ?sec-variable-selection, we explore ways to perform variable selection, think of this task as being part of the training process, and introduce ideas related to dimensionality reduction. We further improve our distribution model.\nIn ?sec-tuning, we conclude story arcs that had been initiated in a few previous chapters, and explore training curves, the tuning of hyper-parameters, and moving-threshold classification. We provide the final refinements to out model of the Reindeer distribution.\nIn ?sec-explanations, we will shift our attention from prediction to understanding, and explore techniques to quantify the importance of variables, as well as ways to visualize their contribution to the predictions. In doing so, we will introduce concepts of model interpretation and explainability.\nIn ?sec-bagging, …",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#some-rules-about-this-book",
    "href": "intro.html#some-rules-about-this-book",
    "title": "1  Introduction",
    "section": "1.3 Some rules about this book",
    "text": "1.3 Some rules about this book\nWhen I started aggregating these notes, I decided on a series of four rules. No code, no simulated data, no long list of model, and above all, no iris dataset. In this section, I will go through why I decided to adopt these rules, and how it should change the way you interact with the book.\n\n1.3.1 No code\nThis is, maybe, the most surprising rule, because data science is programming (in a sense). But sometimes there is so much focus on programming that we lose track of the other, important aspects of the practice of data science: abstractions, relationship with data, and domain knowledge.\nThis book did involve a lot of code. Specifically, this book was written using Julia (Bezanson et al. 2017), and every figure is generated by a notebook, and they are part of the material I use when teaching from this content in the classroom. But code is not a universal language, and unless you are really familiar with the language, code can obfuscate. I had no intention to write a Julia book (or an R book, or a Python book). The point is to think about data science applied to ecological research, and I felt like it would be more inclusive to do this in a language agnostic way.\nAnd finally, code rots. Code with more dependencies rots faster. It take a single change in the API of a package to break the examples, and then you are left with a very expensive monitor stand. With a few exceptions, the examples in this book do not use complicated packages either.\n\n\n1.3.2 No simulated data\nI have nothing against simulated data. I have, in fact, generated simulated data in many different contexts, for training or for research. But the limit of simulated is that we almost inevitably fail to include what makes real data challenging: noise, incomplete or uneven sampling, data representation artifacts. And so when it is time to work on real data, everything seems suddenly more difficult.\nSimulated data have immense training value; but it is also important to engage with the imperfect actual data, as we will overwhelmingly apply the concepts from this book to them. For this reason, there are no simulated data in this book. Everything that is presented correspond to an actual use case that proceeds from a question we could reasonably ask in the context, paired with a dataset that could be used to answer this question.\n\n\n1.3.3 No model zoo\nMy favorite machine learning package is MLJ (Blaom et al. 2020). When given a table of labels and a table of features, it will give back a series of models that match with these data. It speeds up the discovery of models considerably, and is generally a lot more informative than trying to read from a list of possible techniques. If I have questions about an algorithm from this list, I can start reading more documentation about how it works.\nReading a long enumeration of things is boring; unless it’s sung by Yakko Warner, I’m not interested, and I refuse to inflict it on people. But more importantly, these enumerations of models often distract from thinking about the problem we want to solve in more abstract terms. I rarely wake up in the morning and think “oh boy I can’t wait to train a SVM today”; chances are, my thought process will be closer to “I need to tell the mushroom people where I think the next good foraging locations will be”. The rest, is implementation details.\nIn fact, 90% of this book uses only two models: linear regression, and the Naïve Bayes Classifier. Some other models are involved in a few chapters, but these two models are breathtakingly simple, work surprisingly well, run fast, and can be tweaked to allow us to build deep intuitions about how machines learn. They are perfect for the classroom, and give us the freedom to spent most of our time thinking about how we interact with models, and why, and how we make methodological decisions.\n\n\n1.3.4 No iris dataset\nFrom a teaching point of view, the iris dataset is like hearing Smash Mouth in a movie trailer, in that it tells you two things with absolute certainty. First, that you are indeed watching a movie trailer. Second, that you could be watching Shrek instead. There are datasets out there that are infinitely more exciting to use than iris.\nBut there is a far more important reason not to use iris: eugenics.\nListen, we made it several hundred words in a text about quantitative techniques in life sciences without encountering a sad little man with racist ideas that academia decided to ignore because “he just contributed so much to the field, and these were different times, maybe we shouldn’t be so quick to judge?”. Ronald Aylmer Fisher, statistics’ most racist nerd, was such a man; and there are, of course, those who want to consider the possibility that you can be outrageously racist as long as you are an outstanding scientist (Bodmer et al. 2021).\nThe iris dataset was first published by Fisher (1936) in the Annals of Eugenics (so, there’s a bit of a red flag there already), and draws from several publications by Edgar Anderson, starting with Anderson (1928); Unwin & Kleinman (2021) have an interesting historiographic deep-dive into the correspondence between the two. Judging by the dates, you may think that Fisher was a product of his time. But this could not be further from the truth. Fisher was dissatisfied with his time, to the point where his contributions to statistics were done in service of his views, in order to provide the appearance of scientific rigor to his bigotry.\nFisher advocated for forced sterilization for the “defectives” (which he estimated at, oh, roughly 10% of the population), argued that not all races had equal capacity for intellectual and emotional development, and held a host of related opinions. There is no amount of contribution to science that pardon these views. Coming up with the idea of the null hypothesis does not even out lending “scientific” credibility to ideas whose logical (and historical) conclusion is genocide. That Ronald Fisher is still described as a polymath and a genius is infuriating, and we should use every alternative to his work that we have.\nThankfully, there are alternatives!\nThe most broadly known alternative to the iris dataset is penguins, which was collected by ecologists (Gorman et al. 2014), and published as a standard dataset (Horst et al. 2020) so that we can train students without engaging with the “legacy” of eugenicists. The penguins dataset is also genuinely good! The classes are not so obviously separable, there are some missing data that reflect the reality of field work, and the data about sex and spatial location have been preserved, which increases the diversity of questions we can ask. We won’t use penguins either. It’s a fine dataset, but at this point there is little that we can write around it that would be new, or exciting. But if you want to apply some of the techniques in this book? Go penguins.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/clustering.html",
    "href": "chapters/clustering.html",
    "title": "2  Clustering",
    "section": "",
    "text": "2.1 A digression: which birds are red?\nBefore diving in, it is a good idea to ponder a simple case. We can divide everything in just two categories: things with red feathers, and things without red feathers. An example of a thing with red feathers is the Northern Cardinal (Cardinalis cardinalis), and things without red feathers are the iMac G3, Haydn’s string quartets, and of course the Northern Cardinal (Cardinalis cardinalis).\nSee, biodiversity data science is complicated, because it tends to rely on the assumption that we can categorize the natural world, and the natural world (mostly in response to natural selection) comes up with ways to be, well, diverse and hard to categorize. In the Northern Cardinal, this is shown in males having red feathers, and females having mostly brown feathers. Before moving forward, we need to consider ways to solve this issue, as this issue will come up all the time.\nThe first mistake we have made is that the scope of objects we want to classify, which we will describe as the “domain” of our classification, is much too broad: there are few legitimate applications where we will have a dataset with Northern Cardinals, iMac G3s, and Haydn’s string quartets. Picking a reasonable universe of classes would have solved our problem a little. For example, among the things that do not have red feathers are the Mourning Dove, the Kentucky Warbler, and the House Sparrow.\nThe second mistake that we have made is improperly defining our classes; bird species exhibit sexual dimorphism (not in an interesting way, like wrasses, but let’s give them some credit for trying). Assuming that there is such a thing as a Northern Cardinal is not necessarily a reasonable assumption! And yet, the assumption that a single label is a valid representation of non-monomorphic populations is a surprisingly common one, with actual consequences for the performance of image classification algorithms (Luccioni & Rolnick 2023). This assumption reveals a lot about our biases: male specimens are over-represented in museum collections, for example (Cooper et al. 2019). In a lot of species, we would need to split the taxonomic unit into multiple groups in order to adequately describe them.\nThe third mistake we have made is using predictors that are too vague. The “presence of red feathers” is not a predictor that can easily discriminate between the Northen Cardinal (yes for males, sometimes for females), the House Finch (a little for males, no for females), and the Red-Winged Black Bird (a little for males, no for females). In fact, it cannot really capture the difference between red feathers for the male House Finch (head and breast) and the male Red Winged Black Bird (wings, as the name suggests).\nThe final mistake we have made is in assuming that “red” is relevant as a predictor. In a wonderful paper, Cooney et al. (2022) have converted the color of birds into a bird-relevant colorimetric space, revealing a clear latitudinal trend in the ways bird colors, as perceived by other birds, are distributed. This analysis, incidentally, splits all species into males and females. The use of a color space that accounts for the way colors are perceived is a fantastic example of why data science puts domain knowledge front and center.\nDeciding which variables are going to be accounted for, how the labels will be defined, and what is considered to be within or outside the scope of the classification problem is difficult. It requires domain knowledge (you must know a few things about birds in order to establish criteria to classify birds), and knowledge of how the classification methods operate (in order to have just the right amount of overlap between features in order to provide meaningful estimates of distance).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Clustering</span>"
    ]
  },
  {
    "objectID": "chapters/clustering.html#a-digression-which-birds-are-red",
    "href": "chapters/clustering.html#a-digression-which-birds-are-red",
    "title": "2  Clustering",
    "section": "2.1 A digression: which birds are red?",
    "text": "2.1 A digression: which birds are red?\nBefore diving in, it is a good idea to ponder a simple case. We can divide everything in just two categories: things with red feathers, and things without red feathers. An example of a thing with red feathers is the Northern Cardinal (Cardinalis cardinalis), and things without red feathers are the iMac G3, Haydn’s string quartets, and of course the Northern Cardinal (Cardinalis cardinalis).\nSee, biodiversity data science is complicated, because it tends to rely on the assumption that we can categorize the natural world, and the natural world (mostly in response to natural selection) comes up with ways to be, well, diverse and hard to categorize. In the Northern Cardinal, this is shown in males having red feathers, and females having mostly brown feathers. Before moving forward, we need to consider ways to solve this issue, as this issue will come up all the time.\nThe first mistake we have made is that the scope of objects we want to classify, which we will describe as the “domain” of our classification, is much too broad: there are few legitimate applications where we will have a dataset with Northern Cardinals, iMac G3s, and Haydn’s string quartets. Picking a reasonable universe of classes would have solved our problem a little. For example, among the things that do not have red feathers are the Mourning Dove, the Kentucky Warbler, and the House Sparrow.\nThe second mistake that we have made is improperly defining our classes; bird species exhibit sexual dimorphism (not in an interesting way, like wrasses, but let’s give them some credit for trying). Assuming that there is such a thing as a Northern Cardinal is not necessarily a reasonable assumption! And yet, the assumption that a single label is a valid representation of non-monomorphic populations is a surprisingly common one, with actual consequences for the performance of image classification algorithms (Luccioni and Rolnick 2023). This assumption reveals a lot about our biases: male specimens are over-represented in museum collections, for example (Cooper et al. 2019). In a lot of species, we would need to split the taxonomic unit into multiple groups in order to adequately describe them.\nThe third mistake we have made is using predictors that are too vague. The “presence of red feathers” is not a predictor that can easily discriminate between the Northen Cardinal (yes for males, sometimes for females), the House Finch (a little for males, no for females), and the Red-Winged Black Bird (a little for males, no for females). In fact, it cannot really capture the difference between red feathers for the male House Finch (head and breast) and the male Red Winged Black Bird (wings, as the name suggests).\nThe final mistake we have made is in assuming that “red” is relevant as a predictor. In a wonderful paper, Cooney et al. (2022) have converted the color of birds into a bird-relevant colorimetric space, revealing a clear latitudinal trend in the ways bird colors, as perceived by other birds, are distributed. This analysis, incidentally, splits all species into males and females. The use of a color space that accounts for the way colors are perceived is a fantastic example of why data science puts domain knowledge front and center.\nDeciding which variables are going to be accounted for, how the labels will be defined, and what is considered to be within or outside the scope of the classification problem is difficult. It requires domain knowledge (you must know a few things about birds in order to establish criteria to classify birds), and knowledge of how the classification methods operate (in order to have just the right amount of overlap between features in order to provide meaningful estimates of distance).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Clustering</span>"
    ]
  },
  {
    "objectID": "chapters/clustering.html#the-problem-classifying-pixels-from-an-image",
    "href": "chapters/clustering.html#the-problem-classifying-pixels-from-an-image",
    "title": "2  Clustering",
    "section": "2.2 The problem: classifying pixels from an image",
    "text": "2.2 The problem: classifying pixels from an image\nThroughout this chapter, we will work on a single image – we may initially balk at the idea that an image is data, but it is! Specifically, an image is a series of instances (the pixels), each described by their position in a multidimensional colorimetric space. Greyscale images have one dimension, and images in color will have three: their red, green, and blue channels. Not only are images data, this specific dataset is going to be far larger than many of the datasets we will work on in practice: the number of pixels we work with is given by the product of the width, height, and depth of the image!\nIn fact, we are going to use an image with many dimensions: the data in this chapter are coming from a Landsat 9 scene (Vermote et al. 2016), for which we have access to 9 different bands.\n\nOverview of the bands in a Landsat 9 scene. The data from this chapter were downloaded from LandsatLook.\n\n\nBand\nMeasure\nNotes\n\n\n\n\n1\nAerosol\nGood proxy for Chl. in oceans\n\n\n2\nVisible blue\n\n\n\n3\nVisible green\n\n\n\n4\nVisible red\n\n\n\n5\nNear-infrared (NIR)\nReflected by healthy plants\n\n\n6, 7\nShort wavelength IR (SWIR 1)\nGood at differentiating wet earth and dry earth\n\n\n8\nPanchromatic\nHigh-resolution monochrome\n\n\n9\nCirrus band\nCan pick up high and thin clouds\n\n\n10, 11\nThermal infrared\n\n\n\n\nBy using the data present in the channels, we can reconstruct an approximation of what the landscape looked like (by using the red, green, and blue channels).\nOr can we?\nIf we were to invent a time machine, and go stand directly under Landsat 9 at the exact center of this scene, and look around, what would we see? We would see colors, and they would admit a representation as a three-dimensional vector of red, green, and blue. But we would see so much more than that! And even if we were to stand within a pixel, we would see a lot of colors. And texture. And depth. We would see something entirely different from this map; and we would be able to draw a lot more inferences about our surroundings than what is possible by knowing the average color of a 30x30 meters pixel. But just like we can get more information that Landsat 9, so too can Landsat 9 out-sense us when it comes to getting information. In the same way that we can extract a natural color composite out of the different channels, we can extract a fake color one to highlight differences in the landscape.\n\n\n\n\n\n\n\n\nFigure 2.1: The Landsat 9 data are combined into the “Natural Color” image, in which the red, green, and blue bands are mapped to their respective channels (left). The other composites is a 6-5-4 image meant to show differences between urban areas, vegetations, and crops. Note that the true-color composite is slightly distored compared to the colors of the landscape we expect; this is because natural colors are difficult to reproduce accurately.\n\n\n\n\n\nIn Figure 2.1, we compare the natural color reconstruction (top) to a false color composite. All of the panels in Figure 2.1 represent the same physical place at the same moment in time; but through them, we are looking at this place with very different purposes. This is not an idle observation, but a core notion in data science: what we measure defines what we can see. In order to tell something ecologically meaningful about this place, we need to look at it in the “right” way. Of course, although remote sensing offers a promising way to collect data for biodiversity monitoring at scale (Gonzalez et al. 2023), there is no guarantee that it will be the right approach for all problems. More (fancier) data is not necessarily right for all problems.\n\n\nWe will revisit the issue of variable selection and feature engineering in ?sec-variable-selection.\nSo far, we have looked at this area by combining the raw data. Depending on the question we have in mind, they may not be the right data. In fact, they may not hold information that is relevant to our question at all; or worse, they can hold more noise than signal. The area we will work on in this chapter is a very small crop of a Landsat 9 scene, taken on path 14 and row 28, early in late June 2023. It shows the western tip of the island of Montréal, as well as Lake Saint-Louis to the south (not actually a lake), Lake Deux-Montages to the north (not actually a lake either), and a small part of Oka national park. This is an interesting area because it has a high variety of environments: large bodies of water, forested areas (bright green in the composite), densely urbanized places (bright purple and white in the composite), less densely urbanized (green-brown), and cropland to the western tip of the island.\nBut can we classify these different environments starting in an ecologically relevant way? Based on our knowledge of plants, we can start thinking about this question in a different way. Specifically, “can we guess that a pixel contains plants?”, and “can we guess at how much water there is in a pixel?”. Thankfully, ecologists, whose hobbies include (i) guesswork and (ii) plants, have ways to answer these questions rather accurately.\nOne way to do this is to calculate the normalized difference vegetation index, or NDVI (Kennedy & Burbach 2020). NDVI is derived from the band data (NIR - Red), and there is an adequate heuristic using it to make a difference between vegetation, barren soil, and water. Because plants are immediately tied to water, we can also consider the NDWI (water; Green - NIR) and NDMI (moisture; NIR - SWIR1) dimensions: taken together, these information will represent every pixel in a three-dimensional space, telling us whether there are plants (NDVI), whether they are stressed (NDMI), and whether this pixel is a water body (NDWI). Other commonly used indices based on Landsat 9 data include the NBR (Normalized Burned Ratio), for which high values are suggestive of a history of intense fire (Roy et al. 2006 have challenged the idea that this measure is relevant immediately post-fire), and the NDBI (Normalized Difference Built-up Index) for urban areas.\nWe can look at the relationship between the NDVI and NDMI data Figure 2.2. For example, NDMI values around -0.1 are low-canopy cover with low water stress; NDVI values from 0.2 to 0.5 are good candidates for moderately dense crops. Notice that there is a strong (linear) relationship between NDVI and NDMI. Indeed, none of these indices are really independent; this implies that they are likely to be more informative taken together than when looking at them one at a time (Zheng et al. 2021). Indeed, urban area tend to have high values of the NDWI, which makes the specific task of looking for swimming pools (for mosquito control) more challenging than it sounds (McFeeters 2013).\n\n\n\n\n\n\n\n\nFigure 2.2: The pixels acquired from Landsat 9 exist in a space with many different dimensions (one for each band). Because we are interested in a landscape classification based on water and vegetation data, we use the NDVI, NDMI, and NDWI combinations of bands. These are derived data, and represent the creation of new features from the raw data. Darker colors indicate more pixels in this bin.\n\n\n\n\n\nBy picking these four transformed values, instead of simply looking at the clustering of all the bands in the raw data, we are starting to refine what the algorithm sees, through the lens of what we know is important about the system. With these data in hands, we can start building a classification algorithm.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Clustering</span>"
    ]
  },
  {
    "objectID": "chapters/clustering.html#the-theory-behind-k-means-clustering",
    "href": "chapters/clustering.html#the-theory-behind-k-means-clustering",
    "title": "2  Clustering",
    "section": "2.3 The theory behind k-means clustering",
    "text": "2.3 The theory behind k-means clustering\nIn order to understand the theory underlying k-means, we will work backwards from its output. As a method for clustering, k-means will return a vector of class memberships, which is to say, a list that maps each observation (pixel, in our case) to a class (tentatively, a cohesive landscape unit). What this means is that k-means is a transformation, taking as its input a vector with three dimensions (NDVI, NDMI, NDWI), and returning a scalar (an integer, even!), giving the class to which this pixel belongs. Pixels only belongs to one class. These are the input and output of our blackbox, and now we can start figuring out its internals.\n\n2.3.1 Inputs and parameters\n\n\nThroughout this book, we will use \\(\\mathbf{X}\\) to note the matrix of features, and \\(\\mathbf{y}\\) to note the vector of labels. Instances are columns of the features matrix, noted \\(\\mathbf{x}_i\\).\nIn k-means, a set of observations \\(\\mathbf{x}_i\\) are assigned to a set of classes \\(\\mathbf{C}\\), also called the clusters. All \\(\\mathbf{x}_i\\) are vectors with the same dimension (we will call it \\(f\\), for features), and we can think of our observations as a matrix of features \\(\\mathbf{X}\\) of size \\((f, n)\\), with \\(f\\) features and \\(n\\) observations (the columns of this matrix).\nThe number of classes of \\(\\mathbf{C}\\) is \\(|\\mathbf{C}| = k\\), and \\(k\\) is an hyper-parameter of the model, as it needs to fixed before we start running the algorithm. Each class is defined by its centroid, a vector \\(\\mathbf{c}\\) with \\(f\\) dimensions (i.e. the centroid corresponds to a potential “idealized” observation of this class in the space of the features), which k-means progressively refines.\n\n\n2.3.2 Assigning instances to classes\n\n\nOf course, the correct distance measure to use depends on what is appropriate for the data!\nInstances are assigned to the class for which the distance between themselves and the centroid of this class is lower than the distance between themselves and the centroid of any other class. To phrase it differently, the class membership of an instance \\(\\mathbf{x}_i\\) is given by\n\\[\n\\text{argmin}_j \\left\\|\\mathbf{x}_i-\\mathbf{c}_j\\right\\|_2 \\,,\n\\tag{2.1}\\]\nwhich is the value of \\(j\\) that minimizes the \\(L^2\\) norm (\\(\\|\\cdot\\|_2\\), the Euclidean distance) between the instance and the centroid; \\(\\text{argmin}_j\\) is the function returning the value of \\(j\\) that minimizes its argument. For example, \\(\\text{argmin}(0.2,0.8,0.0)\\) is \\(3\\), as the third argument is the smallest. There exists an \\(\\text{argmax}\\) function, which works in the same way.\n\n\n2.3.3 Optimizing the centroids\nOf course, what we really care about is the assignment of all instances to the classes. For this reason, the configuration (the disposition of the centroids) that solves our specific problem is the one that leads to the lowest possible variance within the clusters. As it turns out, it is not that difficult to go from Equation 2.1 to a solution for the entire problem: we simply have to sum over all points!\nThis leads to a measure of the variance, which we want to minimize, expressed as\n\\[\n\\sum_{i=1}^k \\sum_{\\mathbf{x}\\in \\mathbf{C}_i} \\|\\mathbf{x} - \\mathbf{c}_i\\|_2 \\,.\n\\tag{2.2}\\]\nThe part that is non-trivial is now to decide on the value of \\(\\mathbf{c}\\) for each class. This is the heart of the k-means algorithm. From Equation 2.1, we have a criteria to decide to which class each instance belongs. Of course, there is nothing that prevents us from using this in the opposite direction, to define the instance by the points that form it! In this approach, the membership of class \\(\\mathbf{C}_j\\) is the list of points that satisfy the condition in Equation 2.1. But there is no guarantee that the current position of \\(\\mathbf{c}_j\\) in the middle of all of these points is optimal, i.e. that it minimizes the within-class variance.\nThis is easily achieved, however. To ensure that this is the case, we can re-define the value of \\(\\mathbf{c}_j\\) as\n\\[\n\\mathbf{c}_j = \\frac{1}{|\\mathbf{C}_j|}\\sum\\mathbf{C}_j \\,,\n\\tag{2.3}\\]\nwhere \\(|\\cdot|\\) is the cardinality of (number of istances in) \\(\\mathbf{C}_j\\), and \\(\\sum \\mathbf{C}_j\\) is the sum of each feature in \\(\\mathbf{C}_j\\). To put it plainly: we update the centroid of \\(\\mathbf{C}_j\\) so that it takes, for each feature, the average value of all the instances that form \\(\\mathbf{C}_j\\).\n\n\n2.3.4 Updating the classes\n\n\nRepeating a step multiple times in a row is called an iterative process, and we will see a lot of them.\nOnce we have applied Equation 2.3 to all classes, there is a good chance that we have moved the centroids in a way that moved them away from some of the points, and closer to others: the membership of the instances has likely changed. Therefore, we need to re-start the process again, in an iterative way.\nBut until when?\nFinding the optimal solution for a set of points is an NP-hard problem (Aloise et al. 2009), which means that we will need to rely on a little bit of luck, or a whole lot of time. The simplest way to deal with iterative processes is to let them run for a long time, as after a little while they should converge onto an optimum (here, a set of centroids for which the variance is as good as it gets), and hope that this optimum is global and not local.\nA global optimum is easy to define: it is the state of the solution that gives the best possible result. For this specific problem, a global optimum means that there are no other combinations of centroids that give a lower variance. A local optimum is a little bit more subtle: it means that we have found a combination of centroids that we cannot improve without first making the variance worse. Because the algorithm as we have introduced it in the previous sections is greedy, in that it makes the moves that give the best short-term improvement, it will not provide a solution that temporarily makes the variance higher, and therefore is susceptible to being trapped in a local optimum.\nIn order to get the best possible solution, it is therefore common to run k-means multiple times for a given \\(k\\), and to pick the positions of the centroids that give the best overall fit.\n\n\n2.3.5 Identification of the optimal number of clusters\nOne question that is left un-answered is the value of \\(k\\). How do we decide on the number of clusters?\nThere are two solutions here. One is to have an a priori knowledge of the number of classes. For example, if the purpose of clustering is to create groups for some specific task, there might be an upper/lower bound to the number of tasks you are willing to consider. The other solution is to run the algorithm in a way that optimizes the number of clusters for us.\nThis second solution turns out to be rather simple with k-means. We need to change the value of \\(k\\), run it on the same dataset several times, and then pick the solution that was optimal. But this is not trivial. Simply using Equation 2.2 would lead to always preferring many clusters. After all, each point in its own cluster would get a pretty low variance!\nFor this reason, we use measures of optimality that are a little more refined. One of them is the Davies & Bouldin (1979) method, which is built around a simple idea: an assignment of instances to clusters is good if the instances within a cluster are not too far away from the centroids, and the centroids are as far away from one another as possible.\nThe Davies-Bouldin measure is striking in its simplicity. From a series of points and their assigned clusters, we only need to compute two things. The first is a vector \\(\\mathbf{s}\\), which holds the average distance between the points and their centroids (this is the \\(\\left\\|\\mathbf{x}_i-\\mathbf{c}_j\\right\\|_2\\) term in Equation 2.1, so this measure still relates directly to the variance); the second is a matrix \\(\\mathbf{M}\\), which measures the distances between the centroids.\nThese two information are combined in a matrix \\(\\mathbf{R}\\), wherein \\(\\mathbf{R}_{ij} = (s_i + s_j)/\\mathbf{M}_{ij}\\). The interpretation of this term is quite simply: is the average distance within clusters \\(i\\) and \\(j\\) much larger compared to the distance between these clusters. This is, in a sense, a measure of the stress that these two clusters impose on the entire system. In order to turn this matrix into a single value, we calculate the maximum value (ignoring the diagonal!) for each row: this is a measure of the maximal amount of stress in which a cluster is involved. By averaging these values across all clusters, we have a measure of the quality of the assignment, that can be compared for multiple values of \\(k\\).\nNote that this approach protects us against the each-point-in-its-cluster situation: in this scenario, the distance between clusters would decrease really rapidly, meaning that the values in \\(\\mathbf{R}\\) would increase; the Davies-Bouldin measure indicates a better clustering when the values are lower.\n\n\nIn fact, there is very little enumeration of techniques in this book. The important point is to understand how all of the pieces fit together, not to make a census of all possible pieces.\nThere are alternatives to this method, including silhouettes (Rousseeuw 1987) and the technique of Dunn (1974). The question of optimizing the number of clusters goes back several decades (Thorndike 1953), and it still actively studied. What matter is less to give a comprehensive overview of all the measures: the message here is to pick one that works (and can be justified) for your specific problem!",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Clustering</span>"
    ]
  },
  {
    "objectID": "chapters/clustering.html#application-optimal-clustering-of-the-satellite-image-data",
    "href": "chapters/clustering.html#application-optimal-clustering-of-the-satellite-image-data",
    "title": "2  Clustering",
    "section": "2.4 Application: optimal clustering of the satellite image data",
    "text": "2.4 Application: optimal clustering of the satellite image data\n\n2.4.1 Initial run\nBefore we do anything else, we need to run our algorithm with a random pick of hyper-parameters, in order to get a sense of how hard the task ahead is. In this case, using \\(k = 3\\), we get the results presented in Figure 2.3.\n\n\n\n\n\n\n\n\nFigure 2.3: caption\n\n\n\n\n\n\nAfter iterating the k-means algorithm, we obtain a classification for every pixel in the landscape. This classification is based on the values of NDVI, NDMI, and NDWI indices, and therefore groups pixels based on specific assumptions about vegetation and stress. This clustering was produced using \\(k=3\\), i.e. we want to see what the landscape would look like when divided into three categories.\n\n\n\nIn fact, take some time to think about how you would use \\(k\\)-means to come up with a way to remove pixels with only water from this image!\nIt is always a good idea to look at the first results and state the obvious. Here, for example, we can say that water is easy to identify. In fact, removing open water pixels from images is an interesting image analysis challenge (Mondejar & Tongco 2019), and because we used an index that specifically identifies water bodies (NDWI), it is not surprising that there is an entire cluster that seems to be associated with water. But if we take a better look, it appears that there groups of pixels that represent dense urban areas that are classified with the water pixels. When looking at the landscape in a space with three dimensions, it looks like separating densely built-up environment and water is difficult.\nThis might seem like an idle observation, but this is not the case! It means that when working on vegetation-related questions, we will likely need at least one cluster for water, and one cluster for built-up areas. This is helpful information, because we can already think about how many classes of vegetation we are willing to accept, and add (at least) two clusters to capture other types of cover.\n\n\n2.4.2 Optimal number of pixels\n\n\nWe will revisit the issue of tuning the hyper-parameters in more depth in ?sec-tuning.\nIn order to produce Figure 2.3, we had to guess at a number of classes we wanted to split the landscape into. This introduces two important steps in coming up with a model: starting with initial parameters in order to iterate rapidly, and then refining these parameters to deliver a model that is fit for purpose. Our discussion in Section 2.4.1, where we concluded that we needed to keep (maybe) two classes for water and built-up is not really satisfying, as we do not yet have a benchmark to evaluate the correct value of \\(k\\); we know that it is more than 3, but how much more?\nWe will now change the values of \\(k\\) and use the Davies & Bouldin (1979) measure introduced in Section 2.3.5 to identify the optimal value of \\(k\\). The results are presented in Figure 2.4. Note that we only explore \\(k \\in [3, 10]\\). More than 8 categories is probably not very actionable, and therefore we can make the decision to only look at this range of parameters. Sometimes (always!) the best solution is the one that gets your job done.\n\n\n\n\n\n\n\n\nFigure 2.4: Results of running the k-means algorithm ten times for each number of clusters between 3 and 8. The average Davies-Bouldin and cost are reported, as well as the standard deviation. As expected, the total cost decreases with more clusters, but this is not necessarily the sign of a better clustering.\n\n\n\n\n\nThere are two interesting things in Figure 2.4. First, note that for \\(k=\\{3,4\\}\\), there is almost no dispersal: all of the assignments have the exact same score, which is unlikely to happen except if the assignments are the same every time! This is a good sign, and, anecdotally, something that might suggest a really information separation of the points. Second, \\(k = 3\\) has by far the lowest Davies-Bouldin index of all values we tried, and is therefore strongly suggestive of an optimal hyper-parameter. But in Figure 2.3, we already established that one of these clusters was capturing both water and built-up environments, so although it may look better from a quantitative point of view, it is not an ideal solution for the specific problem we have.\nIn this specific case, it makers very little sense not to use \\(k = 4\\) or \\(k = 5\\). They have about the same performance, but this gives us potentially more classes that are neither water nor built-up. This image is one of many cases where it is acceptable to sacrifice a little bit of optimality in order to present more actionable information. Based on the results in this section, we will pick the largest possible \\(k\\) that does not lead to a drop in performance, which in our case is \\(k=5\\).\n\n\n2.4.3 Clustering with optimal number of classes\nThe clustering of pixels using \\(k = 5\\) is presented in Figure 2.5. Unsurprisingly, k-means separated the open water pixels, the dense urban areas, as well as the more forested/green areas. Now is a good idea to start thinking about what is representative of these clusters: one is associated with very high NDWI value (these are the water pixels), and two classes have both high NDVI and high NDMI (suggesting different categories of vegetation).\n\n\n┌ Warning: The clustering cost increased at iteration #16\n└ @ Clustering ~/.julia/packages/Clustering/JwhfU/src/kmeans.jl:191\n┌ Warning: The clustering cost increased at iteration #18\n└ @ Clustering ~/.julia/packages/Clustering/JwhfU/src/kmeans.jl:191\n┌ Warning: The clustering cost increased at iteration #21\n└ @ Clustering ~/.julia/packages/Clustering/JwhfU/src/kmeans.jl:191\n┌ Warning: The clustering cost increased at iteration #24\n└ @ Clustering ~/.julia/packages/Clustering/JwhfU/src/kmeans.jl:191\n┌ Warning: The clustering cost increased at iteration #27\n└ @ Clustering ~/.julia/packages/Clustering/JwhfU/src/kmeans.jl:191\n\n\n\n\n\n\n\n\nFigure 2.5: Results of the landscape clustering with k=5 clusters. This number of clusters gives us a good separation between different groups of pixels, and seems to capture features of the landscape as revealed with the false-color composites.\n\n\n\n\n\n\n\nWe will revisit the issue of understanding how a model makes a prediction in ?sec-explanations.\nThe relative size of the clusters (as well as the position of their centroids) is presented in Table 2.1. There is a good difference in the size of the clusters, which is an important thing to note. Indeed, a common myth about k-means is that it gives clusters of the same size. This “size” does not refer to the cardinality of the clusters, but to the volume that they cover in the space of the parameters. If an area of the space of parameters is more densely packed with instances, the cluster covering the area will have more points!\n\n\n\n\n\n\nCluster\nCover\nNDVI\nNDWI\nNDMI\n\n\n\n\n2\n38\n-0.018\n0.012\n0.006\n\n\n5\n10\n0.096\n-0.152\n0.005\n\n\n3\n19\n0.224\n-0.262\n0.08\n\n\n1\n17\n0.32\n-0.343\n0.139\n\n\n4\n16\n0.439\n-0.443\n0.223\n\n\n\n\n\nTable 2.1: Summary of the values for the centers of the optimal clusters found in this image. The cover column gives the percentage of all pixels associated to this class. The clusters are sorted by the NDVI of their centroid.\n\n\n\n\n\nIn fact, this behavior makes k-means excellent at creating color palettes from images! Cases in point, Karthik Ram’s Wes Anderson palettes, and David Lawrence Miller’s Beyoncé palettes. Let it never again be said that ecologists should not be trusted with machine learning methods.\nThe area of the space of parameters covered by each cluster in represented in Figure 2.6, and this result is actually not surprising, if we spend some time thinking about how k-means work. Because our criteria to assign a point to a cluster is based on the being closest to its centroid than to any other centroid, we are essentially creating Voronoi cells, with linear boundaries between them.\nBy opposition to a model based on, for example, mixtures of Gaussians, the assignment of a point to a cluster in k-means is independent of the current composition of the cluster (modulo the fact that the current composition of the cluster is used to update the centroids). In fact, this makes k-means closer to (or at least most efficient as) a method for quantization (Gray 1984).\n\n\n\n\n\n\n\n\nFigure 2.6: Visualisation of the clustering output as a function of the NDVI and NDMI values. Note that the limits between the clusters are lines (planes), and that each cluster covers about the same volume in the space of parameters.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Clustering</span>"
    ]
  },
  {
    "objectID": "chapters/clustering.html#conclusion",
    "href": "chapters/clustering.html#conclusion",
    "title": "2  Clustering",
    "section": "2.5 Conclusion",
    "text": "2.5 Conclusion\nIn this chapter, we have used the k-means algorithm to create groups in a large dataset that had no labels, i.e. the points were not assigned to a class. By picking the features we wanted to cluster the point, we were able to highlight specific aspects of the landscape. In ?sec-gradientdescent, we will start adding labels to our data, and shift our attention from classification to regression problems.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Clustering</span>"
    ]
  },
  {
    "objectID": "chapters/gradientdescent.html",
    "href": "chapters/gradientdescent.html",
    "title": "3  Gradient descent",
    "section": "",
    "text": "References",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Gradient descent</span>"
    ]
  },
  {
    "objectID": "chapters/gradientdescent.html#sec-gradientdescent-trainedmodel",
    "href": "chapters/gradientdescent.html#sec-gradientdescent-trainedmodel",
    "title": "3  Gradient descent",
    "section": "3.1 A digression: what is a trained model?",
    "text": "3.1 A digression: what is a trained model?\nModels are data. When a model is trained, it represents a series of measurements (its parameters), taken on a representation of the natural world (the training data), through a specific instrument (the model itself, see e.g. Morrison and Morgan 1999). A trained model is, therefore, capturing our understanding of a specific situation we encountered. We need to be very precise when defining what, exactly, a model describes. In fact, we need to take a step back and try to figure out where the model stops.\nAs we will see in this chapter, then in Chapter 4, and finally in Chapter 7, the fact of training a model means that there is a back and forth between the algorithm we train, the data we use for training, and the criteria we set to define the performance of the trained model. The algorithm bound to its dataset is the machine we train in machine learning.\nTherefore, a trained model is never independent from its training data: they describe the scope of the problem we want to address with this model. In Chapter 2, we ended up with a machine (the trained k-means algorithm) whose parameters (the centroids of the classes) made sense in the specific context of the training data we used; applied to a different dataset, there are no guarantees that our model would deliver useful information.\nFor the purpose of this book, we will consider that a model is trained when we have defined the algorithm, the data, the measure through which we will evaluate the model performance, and then measured the performance on a dataset built specifically for this task. All of these elements are important, as they give us the possibility to explain how we came up with the model, and therefore, how we made the predictions. This is different from reasoning about why the model is making a specific prediction (we will discuss this in Chapter 8), and is more related to explaining the process, the “outer core” of the model. As you read this chapter, pay attention to these elements: what algorithm are we using, on what data, how do we measure its performance, and how well does it perform?",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Gradient descent</span>"
    ]
  },
  {
    "objectID": "chapters/gradientdescent.html#sec-gradientdescent-problem",
    "href": "chapters/gradientdescent.html#sec-gradientdescent-problem",
    "title": "3  Gradient descent",
    "section": "3.2 The problem: how many interactions in a food web?",
    "text": "3.2 The problem: how many interactions in a food web?\nOne of the earliest observation that ecologists made about food webs is that when there are more species, there are more interactions. A remarkably insightful crowd, food web ecologists. Nevertheless, it turns out that this apparently simple question had received a few different answers over the years.\nThe initial model was proposed by Cohen and Briand (1984): the number of interactions \\(L\\) scales linearly with the number of species \\(S\\). After all, we can assume that when averaging over many consumers, there will be an average diversity of resources they consume, and so the number of interactions could be expressed as \\(L \\approx b\\times S\\).\nNot so fast, said Martinez (1992). When we start looking a food webs with more species, the increase of \\(L\\) with regards to \\(S\\) is superlinear. Thinking in ecological terms, maybe we can argue that consumers are flexible, and that instead of sampling a set number of resources, they will sample a set proportion of the number of consumer-resource combinations (of which there are \\(S^2\\)). In this interpretation, \\(L \\approx b\\times S^2\\).\nBut the square term can be relaxed; and there is no reason not to assume a power law, with \\(L\\approx b\\times S^a\\). This last formulation has long been accepted as the most workable one, because it is possible to approximate values of its parameters using other ecological processes (Brose et al. 2004).\nThe “reality” (i.e. the relationship between \\(S\\) and \\(L\\) that correctly accounts for ecological constraints, and fit the data as closely as possible) is a little bit different than this formula (MacDonald, Banville, and Poisot 2020). But for the purpose of this chapter, figuring out the values of \\(a\\) and \\(b\\) from empirical data is a very instructive exercise.\nIn Figure 3.1, we can check that there is a linear relationship between the natural log of the number of species and the natural log of the number of links. This is not surprising! If we assume that \\(L \\approx b\\times S^a\\), then we can take the log of both sides, and we get \\(\\text{log}\\, L \\approx a \\times \\text{log}\\, S + \\text{log}\\,b\\). This is linear model, and so we can estimate its parameters using linear regression!\n\n\n\n\n\n\n\n\nFigure 3.1: We have assumed that the relationship between \\(L\\) and \\(S\\) could be represented by \\(L \\approx b\\times S^a\\), which gave us a reason to take the natural log of both variables. On this figure, we see that the relationship between the logs look linear, which means that linear regression has a good chance of estimating the values of the parameters.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Gradient descent</span>"
    ]
  },
  {
    "objectID": "chapters/gradientdescent.html#sec-gradientdescent-explanation",
    "href": "chapters/gradientdescent.html#sec-gradientdescent-explanation",
    "title": "3  Gradient descent",
    "section": "3.3 Gradient descent",
    "text": "3.3 Gradient descent\nGradient descent is built around a remarkably simple intuition: knowing the formula that gives rise to our prediction, and the value of the error we made for each point, we can take the derivative of the error with regards to each parameter, and this tells us how much this parameter contributed to the error. Because we are taking the derivative, we can futher know whether to increase, or decrease, the value of the parameter in order to make a smaller error next time.\nIn this section, we will use linear regression as an example, because it is the model we have decided to use when exploring our ecological problem in Section 3.2, and because it is suitably simple to keep track of everything when writing down the gradient by hand.\nBefore we start assembling the different pieces, we need to decide what our model is. We have settled on a linear model, which will have the form \\(\\hat y = m\\times x + b\\). The little hat on \\(\\hat y\\) indicates that this is a prediction. The input of this model is \\(x\\), and its parameters are \\(m\\) (the slope) and \\(b\\) (the intercept). Using the notation we adopted in Section 3.2, this would be \\(\\hat l = a \\times s + b\\), with \\(l = \\text{log} L\\) and \\(s = \\text{log} S\\).\n\n3.3.1 Defining the loss function\nThe loss function is an important concept for anyone attempting to compare predictions to outcomes: it quantifies how far away an ensemble of predictions is from a benchmark of known cases. There are many loss functions we can use, and we will indeed use a few different ones in this book. But for now, we will start with a very general understanding of what these functions do.\nThink of prediction as throwing a series of ten darts on ten different boards. In this case, we know what the correct outcome is (the center of the board, I assume, although I can be mistaken since I have only played darts once, and lost). A cost function would be any mathematical function that compares the position of each dart on each board, the position of the correct event, and returns a score that informs us about how poorly our prediction lines up with the reality.\nIn the above example, you may be tempted to say that we can take the Euclidean distance of each dart to the center of each board, in order to know, for each point, how far away we landed. Because there are several boards, and because we may want to vary the number of boards while still retaining the ability to compare our performances, we would then take the average of these measures.\nWe will note the position of our dart as being \\(\\hat y\\), the position of the center as being \\(y\\) (we will call this the ground truth), and the number of attempts \\(n\\), and so we can write our loss function as\n\\[\n\\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat y_i)^2\n\\tag{3.1}\\]\n\n\nIn data science, things often have multiple names. This is true of loss functions, and this will be even more true on other things later.\nThis loss function is usually called the MSE (Mean Standard Error), or L2 loss, or the quadratic loss, because the paths to machine learning terminology are many. This is a good example of a loss function for regression (and we will discuss loss functions for classification later in this book). There are alternative loss functions to use for regression problems in Table 3.1.\n\n\n\n\n\n\n\n\n\n\n\nMeasure\nExpression\nRemarks\n\n\n\n\nMean Squared Error (MSE, L2)\n\\(\\frac{1}{n}\\sum_{i=1}^{n}\\left(y_i - \\hat y_i\\right)^2\\)\nLarge errors are (proportionally) more penalized because of the squaring\n\n\nMean Absolute Error (MAE, L1)\n\\(\\frac{1}{n}\\sum_{i=1}^{n}\\|y_i - \\hat y_i\\|\\)\nError measured in the units of the response variable\n\n\nRoot Mean Square Error (RMSE)\n\\(\\sqrt{\\text{MSE}}\\)\nError measured in the units of the response variable\n\n\nMean Bias Error\n\\(\\frac{1}{n}\\sum_{i=1}^{n}\\left(y_i - \\hat y_i\\right)\\)\nErrors can cancel out, but this can be used as a measure of positive/negative bias\n\n\n\n\n\nTable 3.1: List of common loss functions for regression problems\n\n\n\nThroughout this chapter, we will use the L2 loss (Equation 3.1), because it has really nice properties when it comes to taking derivatives, which we will do a lot of. In the case of a linear model, we can rewrite Equation 3.1 as\n\\[\nf = \\frac{1}{n}\\sum\\left(y_i - m\\times x_i - b\\right)^2\n\\tag{3.2}\\]\nThere is an important change in Equation 3.2: we have replaced the prediction \\(\\hat y_i\\) with a term that is a function of the predictor \\(x_i\\) and the model parameters: this means that we can calculate the value of the loss as a function of a pair of values \\((x_i, y_i)\\), and the model parameters.\n\n\n3.3.2 Calculating the gradient\nWith the loss function corresponding to our problem in hands (Equation 3.2), we can calculate the gradient. Given a function that is scalar-valued (it returns a single value), taking several variables, that is differentiable, the gradient of this function is a vector-valued (it returns a vector) function; when evaluated at a specific point, this vectors indicates both the direction and the rate of fastest increase, which is to say the direction in which the function increases away from the point, and how fast it moves.\nWe can re-state this definition using the terms of the problem we want to solve. At a point \\(p = [m\\quad b]^\\top\\), the gradient \\(\\nabla f\\) of \\(f\\) is given by:\n\\[\n\\nabla f\\left(\np\n\\right) =\n\\begin{bmatrix}\n\\frac{\\partial f}{\\partial m}(p) \\\\\n\\frac{\\partial f}{\\partial b}(p)\n\\end{bmatrix}\\,.\n\\tag{3.3}\\]\nThis indicates how changes in \\(m\\) and \\(b\\) will increase the error. In order to have a more explicit formulation, all we have to do is figure out an expression for both of the partial derivatives. In practice, we can let auto-differentiation software calculate the gradient for us (Innes 2018); these packages are now advanced enough that they can take the gradient of code directly.\nSolving \\((\\partial f / \\partial m)(p)\\) and \\((\\partial f / \\partial c)(p)\\) is easy enough:\n\\[\n\\nabla f\\left(\np\n\\right) =\n\\begin{bmatrix}\n-\\frac{2}{n}\\sum \\left[x_i \\times (y_i - m\\times x_i - b)\\right] \\\\\n-\\frac{2}{n}\\sum \\left(y_i - m\\times x_i - b\\right)\n\\end{bmatrix}\\,.\n\\tag{3.4}\\]\nNote that both of these partial derivatives have a term in \\(2n^{-1}\\). Getting rid of the \\(2\\) in front is very straightforward! We can modify Equation 3.2 to divide by \\(2n\\) instead of \\(n\\). This modified loss function retains the important characteristics: it increases when the prediction gets worse, and it allows comparing the loss with different numbers of points. As with many steps in the model training process, it is important to think about why we are doing certain things, as this can enable us to make some slight changes to facilitate the analysis.\nWith the gradient written down in Equation 3.4, we can now think about what it means to descend the gradient.\n\n\n3.3.3 Descending the gradient\nRecall from Section 3.3.2 that the gradient measures how far we increase the function of which we are taking the gradient. Therefore, it measures how much each parameter contributes to the loss value. Our working definition for a trained model is “one that has little loss”, and so in an ideal world, we could find a point \\(p\\) for which the gradient is as small as feasible.\nBecause the gradient measures how far away we increase error, and intuitive way to use it is to take steps in the opposite direction. In other words, we can update the value of our parameters using \\(p := p - \\nabla f(p)\\), meaning that we subtract from the parameter values their contribution to the overall error in the predictions.\nBut, as we will discuss further in Section 3.3.4, there is such a thing as “too much learning”. For this reason, we will usually not move the entire way, and introduce a term to regulate how much of the way we actually want to descend the gradient. Our actual scheme to update the parameters is\n\\[\np := p - \\eta\\times \\nabla f(p) \\,.\n\\tag{3.5}\\]\nThis formula can be iterated: with each successive iteration, it will get us closer to the optimal value of \\(p\\), which is to say the combination of \\(m\\) and \\(b\\) that minimizes the loss.\n\n\n3.3.4 A note on the learning rate\nThe error we can make on the first iteration will depend on the value of our initial pick of parameters. If we are way off, especially if we did not re-scale our predictors and responses, this error can get very large. And if we make a very large error, we will have a very large gradient, and we will end up making very big steps when we update the parameter values. There is a real risk to end up over-compensating, and correcting the parameters too much.\nIn order to protect against this, in reality, we update the gradient only a little, where the value of “a little” is determined by an hyper-parameter called the learning rate, which we noted \\(\\eta\\). This value will be very small (much less than one). Picking the correct learning rate is not simply a way to ensure that we get correct results (though that is always a nice bonus), but can be a way to ensure that we get results at all. The representation of numbers in a computer’s memory is tricky, and it is possible to create an overflow: a number so large it does not fit within 64 (or 32, or 16, or however many we are using) bits of memory.\nThe conservative solution of using the smallest possible learning rate is not really effective, either. If we almost do not update our parameters at every epoch, then we will take almost forever to converge on the correct parameters. Figuring out the learning rate is an example of hyper-parameter tuning, which we will get back to later in this book.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Gradient descent</span>"
    ]
  },
  {
    "objectID": "chapters/gradientdescent.html#sec-gradientdescent-application",
    "href": "chapters/gradientdescent.html#sec-gradientdescent-application",
    "title": "3  Gradient descent",
    "section": "3.4 Application: how many links are in a food web?",
    "text": "3.4 Application: how many links are in a food web?\nWe will not get back to the problem exposed in Figure 3.1, and use gradient descent to fit the parameters of the model defined as \\(\\hat y \\approx \\beta_0 + \\beta_1 \\times x\\), where, using the notation introduced in Section 3.2, \\(\\hat y\\) is the natural log of the number of interactions (what we want to predict), \\(x\\) is the natural log of the species richness (our predictor), and \\(\\beta_0\\) and \\(\\beta_1\\) are the parameters of the model.\n\n3.4.1 The things we won’t do\nAt this point, we could decide that it is a good idea to transform our predictor and our response, for example using the z-score. But this is not really required here; we know that our model will give results that make sense in the units of species and interactions (after dealing with the natural log, of course). In addition, as we will see in Section 6.2, applying a transformation to the data too soon can be a dangerous thing. We will have to live with raw features for a few more chapters.\nIn order to get a sense of the performance of our model, we will remove some of the data, meaning that the model will not learn on these data points. We will get back to this practice (cross-validation) in a lot more details in Chapter 4, but for now it is enough to say that we hide 20% of the dataset, and we will use them to evaluate how good the model is as it trains. The point of this chapter is not to think too deeply about cross-validation, but simply to develop intuitions about the way a machine learns.\n\n\n3.4.2 Starting the learning process\nIn order to start the gradient descent process, we need to decide on an initial value of the parameters. There are many ways to do it. We could work our way from our knowledge of the system; for example \\(b &lt; 1\\) and \\(a = 2\\) would fit relatively well with early results in the food web literature. Or we could draw a pair of values \\((a, b)\\) at random. Looking at Figure 3.1, it is clear that our problem is remarkably simple, and so presumably either solution would work.\n\n\n3.4.3 Stopping the learning process\nThe gradient descent algorithm is entirely contained in Equation 3.5 , and so we only need to iterate several times to optimize the parameters. How long we need to run the algorithm for depends on a variety of factors, including our learning rate (slow learning requires more time!), our constraints in terms of computing time, but also how good we need to model to be.\n\n\nThe number of iterations over which we train the model is usually called the number of epochs, and is an hyper-parameter of the model.\nOne usual approach is to decide on a number of iterations (we need to start somewhere), and to check how rapidly the model seems to settle on a series of parameters. But more than this, we also need to ensure that our model is not learning too much from the data. This would result in over-fitting, in which the models gets better on the data we used to train it, and worse on the data we kept hidden from the training! In Table 3.2, we present the RMSE loss for the training and testing datasets, as well as the current estimates of the values of the parameters of the linear model.\n\n\n\n\n\n\n\n\nStep\nLoss (training)\nLoss (testing)\nβ₀\nβ₁\n\n\n\n\n1\n3.92114\n3.18785\n0.4\n0.2\n\n\n10\n2.99934\n2.3914\n0.487395\n0.226696\n\n\n30\n1.72775\n1.31211\n0.640263\n0.271814\n\n\n100\n0.536207\n0.373075\n0.907004\n0.337644\n\n\n300\n0.392477\n0.308264\n1.03855\n0.311011\n\n\n1000\n0.326848\n0.253939\n1.11195\n0.110083\n\n\n3000\n0.225623\n0.167897\n1.25704\n-0.311373\n\n\n10000\n0.164974\n0.119597\n1.4487\n-0.868105\n\n\n20000\n0.162808\n0.118899\n1.48864\n-0.984121\n\n\n\n\n\n\nTable 3.2: This table shows the change in the model, as measured by the loss and by the estimates of the parameters, after an increasing amount of training epochs. The loss drops sharply in the first 500 iterations, but even after 20000 iterations, there are still some changes in the values of the parameters.\n\n\n\n\nIn order to protect against over-fitting, it is common to add a check to the training loop, to say that after a minimum number of iterations has been done, we stop the training when the loss on the testing data starts increasing. In order to protect against very long training steps, it is also common to set a tolerance (absolute or relative) under which we decide that improvements to the loss are not meaningful, and which serves as a stopping criterion for the training.\n\n\n3.4.4 Detecting over-fitting\nAs we mentioned in the previous section, one risk with training that runs for too long is to start seeing over-fitting. The usual diagnosis for over-fitting is an increase in the testing loss, which is to say, in the loss measured on the data that were not used for training. In Figure 3.2, we can see that the RMSE loss decreases at the same rate on both datasets, which indicates that the model is learning from the data, but not to a point where its ability to generalize suffers.\n\n\nUnderfitting is also a possible scenario, where the model is not learning from the data, and can be detected by seeing the loss measures remain high or even increase.\n\n\n\n\n\n\n\n\nFigure 3.2: This figures shows the change in the loss for the training and testing dataset. As the two curves converge on low values at the same rate, this suggests that the model is not over-fitting, and is therefore suitable for use.\n\n\n\n\n\nWe are producing the loss over time figure after the training, as it is good practice – but as we mentioned in the previous section, it is very common to have the training code look at the dynamics of these two values in order to decide whether to stop the training early.\nBefore moving forward, let’s look at Figure 3.2 a little more closely. In the first steps, the loss decreases very rapidly – this is because we started from a value of \\(\\mathbf{\\beta}\\) that is, presumably, far away from the optimum, and therefore the gradient is really strong. Despite the low learning rate, we are making long steps in the space of parameters. After this initial rapid increase, the loss decreases much more slowly. This, counter-intuitively, indicates that we are getting closer to the optimum! At the exact point where \\(\\beta_0\\) and \\(\\beta_1\\) optimally describe our dataset, the gradient vanishes, and our system would stop moving. And as we get closer and closer to this point, we are slowing down. In the next section, we will see how the change in loss over times ties into the changes with the optimal parameter values.\n\n\n3.4.5 Visualizing the learning process\nFrom Figure 3.3, we can see the change in \\(\\beta_0\\) and \\(\\beta_1\\), as well as the movement of the current best estimate of the parameters (right panel). The sharp decrease in loss early in the training is specifically associated to a rapid change in the value of \\(\\beta_0\\). Further note that the change in parameters values is not monotonous! The value of \\(\\beta_1\\) initially increases, but when \\(\\beta_0\\) gets closer to the optimum, the gradient indicates that we have been moving \\(\\beta_1\\) in the “wrong” direction.\n\n\n\n\n\n\n\n\nFigure 3.3: This figure shows the change in the parameters values over time. Note that the change is very large initially, because we make large steps when the gradient is strong. The rate of change gets much lower as we get nearer to the “correct” value.\n\n\n\n\n\nThis is what gives rise to the “elbow” shape in the right panel of Figure 3.3. Remember that the gradient descent algorithm, in its simple formulation, assumes that we can never climb back up, i.e. we never accept a costly move. The trajectory of the parameters therefore represents the path that brings them to the lowest point they can reach without having to temporarily recommend a worse solution.\nBut how good is the solution we have reached?\n\n\n3.4.6 Outcome of the model\nWe could read the performance of the model using the data in Figure 3.2, but what we really care about is the model’s ability to tell us something about the data we initially gave it. This is presented in Figure 3.4. As we can see, the model is doing a rather good job at capturing the relationship between the number of species and the number of interactions.\n\n\n\n\n\n\n\n\nFigure 3.4: Overview of the fitted model. The residuals (top panel) are mostly centered around 0, which suggests little bias towards over/under predicting interactions. The red line (based on the optimal coefficients) goes through the points, and indicates a rather good fit of the model.\n\n\n\n\n\nWe will have a far more nuanced discussion of “what is this model good for?” in Chapter 4, but for now, we can make a decision about this model: it provides a good approximation of the relationship between the species richness, and the number of interactions, in a food web.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Gradient descent</span>"
    ]
  },
  {
    "objectID": "chapters/gradientdescent.html#a-note-on-regularization",
    "href": "chapters/gradientdescent.html#a-note-on-regularization",
    "title": "3  Gradient descent",
    "section": "3.5 A note on regularization",
    "text": "3.5 A note on regularization\nOne delicate issue that we have avoided in this chapter is the absolute value of the parameters. In other words, we didn’t really care about how large the model parameters would be, only the quality of the fit. This is (generally) safe to do in a model with a single parameter. But what if we had many different terms? What if, for example, we had a linear model of the form \\(\\hat y \\approx \\beta_0 + \\beta_1 x + \\beta_2 x^2\\)? What if our model was of the form \\(\\hat y \\approx \\beta_0 + \\beta_1 x + \\dots + \\beta_n x^n\\)? What if \\(n\\) started to get very large compared to the number of data points?\nIn this situation, we would very likely see overfitting, wherein the model would use the polynomial terms we provided to capture more and more noise in the data. This would be a dangerous situation, as the model will lose its ability to work on unknown data!\nTo prevent this situation, we may need to use regularization. Thanfkully, regularization is a relatively simple process. In Equation 3.4, the function \\(f(p)\\) we used to measure the gradient was the loss function directly. In regularization, we use a slight variation on this, where\n\\[\nf(p) = \\text{loss} + \\lambda \\times g(\\beta) \\,,\n\\]\nwhere \\(\\lambda\\) is an hyper-parameter giving the strength of the regularization, and \\(g(\\beta)\\) is a function to calculate the total penalty of a set of parameters.\nWhen using \\(L1\\) regularization (LASSO regression), \\(g(\\beta) = \\sum |\\beta|\\), and when using \\(L2\\) regularization (ridge regression), \\(g(\\beta) = \\sum \\beta^2\\). When this gets larger, which happens when the absolute value of the parameters increases, the model is penalized. Note that if \\(\\lambda = 0\\), we are back to the initial formulation of the gradient, where the parameters have no direct effect on the cost.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Gradient descent</span>"
    ]
  },
  {
    "objectID": "chapters/gradientdescent.html#conclusion",
    "href": "chapters/gradientdescent.html#conclusion",
    "title": "3  Gradient descent",
    "section": "3.6 Conclusion",
    "text": "3.6 Conclusion\nIn this chapter, we have used a dataset of species richness and number of interactions to start exploring the practice of machine learning. We defined a model (a linear regression), and based about assumptions about how to get closer to ideal parameters, we used the technique of gradient descent to estimate the best possible relationship between \\(S\\) and \\(L\\). In order to provide a fair evaluation of the performance of this model, we kept a part of the dataset hidden from it while training. In Chapter 4, we will explore this last point in great depth, by introducing the concept of cross-validation, testing set, and performance evaluation.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Gradient descent</span>"
    ]
  },
  {
    "objectID": "chapters/crossvalidation.html",
    "href": "chapters/crossvalidation.html",
    "title": "4  Cross-validation",
    "section": "",
    "text": "4.1 How can we split a dataset?\nThere is a much more important question to ask first: why do we split a dataset? In a sense, answering this question echoes the discussion we started in Section 3.4.4, because the purpose of splitting a dataset is to ensure we can train and evaluate it properly, in order to deliver the best possible model.\nWhen a model is trained, it has learned from the data, we have tuned its hyper-parameters to ensure that it learned with the best possible conditions, and we have applied a measure of performance after the entire process is complete, to communicate how well we expect our model to work. These three tasks require three different datasets, and this is the purpose of splitting our data into groups.\nOne of the issues when reading about splitting data is that the terminology can be muddy. For example, what constitutes a testing and validation set can largely be a matter of perspective. In many instances, testing and validation are used interchangeably, especially when there is a single model involved. Nevertheless, it helps to settle on a few guidelines here, before going into the details of what each dataset constitutes and how to assemble it.\nThe training instances are examples that are given to the model during the training process. This dataset has the least ambiguous definition. The training data is defined by subtraction, in a sense, as whatever is left of the original data after we set aside testing and validation sets.\nThe testing instances are used at the end of the process, to measure the performance of a trained model with tuned hyper-parameters. If the training data are the lectures, testing data are the final exam: we can measure the performance of the model on this dataset and report it as the model performance we can expect when applying the model to new data. There is a very important, chapter-long, caveat about this last point, related to the potential of information leak between datasets, which is covered in Section 6.2.\nThe validation data are used in-between, as part of the training process. They are (possibly) a subset of the training data that we use internally to check the performance of the model, often in order to tune its hyper-parameters, or as a way to report on the over-fitting of the model during the training process.\nThe difference between testing and validation is largely a difference of intent. When we want to provide an a posteriori assessment of the model performance, the dataset we use to determine this performance is a testing dataset. When we want to optimize some aspect of the model, the data we use for this are the validation data. With this high-level perspective in mind, let’s look at each of these datasets in turn. The differences between these three datasets are summarized in Table 4.1.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Cross-validation</span>"
    ]
  },
  {
    "objectID": "chapters/crossvalidation.html#how-can-we-split-a-dataset",
    "href": "chapters/crossvalidation.html#how-can-we-split-a-dataset",
    "title": "4  Cross-validation",
    "section": "",
    "text": "Dataset\nTrains\nPurpose\nData used for training\n\n\n\n\nTraining\nyes\ntrain model\n\n\n\nValidation\n\nvalidate during training\ntraining data only\n\n\nTesting\n\nestimates of future performance\nall except testing\n\n\n\n\n\nTable 4.1: Overview of the three datasets used for training and cross-validation. Information in the “Data used for training” column refer to the data that have been used to train the model when calculating its performance.\n\n\n\n\n4.1.1 Training\nIn data science (in applied machine learning in particular), we do not fit models. We train them. This is an important difference: training is an iterative process, that we can repeat, optimize, and tweak. The outcome of training and the outcome of fitting are essentially the same (a model that is parameterized to work as well as possible on a given dataset), but it is good practice to adopt the language of a field, and the language of data science emphasizes the different practices in model training.\nTraining, to provide a general definition, is the action of modifying the parameters of a model, based on knowledge of the data, and the error that results from using the current parameter values. In Chapter 3, for example, we saw how to train a linear model using the technique of gradient descent, based on a specific dataset, with a learning rate and loss function we picked based on trial and error. Our focus in this chapter is not on the methods we use for training, but on the data that are required to train a model.\nTraining a model is a process akin to rote learning: we will present the same input, and the same expected responses, many times over, and we will find ways for the error on each response to decrease (this is usually achieved by minimizing the loss function).\nIn order to initiate this process, we need an untrained model. Untrained, in this context, refers to a model that has not been trained on the specific problem we are addressing; the model may have been trained on a different problem (for example, we want to predict the distribution of a species based on a GLM trained on a phylogenetically related species). It is important to note that by “training the model”, what we really mean is “change the structure of the parameters until the output looks right”. For example, assuming a simple linear model like \\(c(X) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2\\), training this model would lead to changes in the values of \\(\\beta\\), but not to the consideration of a new model \\(c(X) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\beta_3X_1X_2\\). Comparing models is (often) the point of validation, which we will address later on.\n\n\n4.1.2 Validating\nThe easiest way to think about the validation dataset is by thinking about what it is not used for: training the model (this is the training set), and giving a final overview of the model expected performance (this is the testing set). The validation set is used for everything else (model selection, cross-validation, hyper-parameters tuning), albeit in a specific way. With the training set, we communicate the predictors and the labels to the model, and update the weights of the model in response. With the validation set, we communicate the predictors and the labels to the model, but we do not update the weights in response. All we care about during validation is the performance of the model on a problem it has not yet encountered during this specific round of training. If the training set is like attending a lecture, the validation set is formative feedback.\nOf course, one issue with the creation of a validation set is that it needs to resemble the problem the model will have to solve in practice. We will discuss this more in depth in the following sections, but it is worth thinking about an example. Assume a model that classifies a picture as having either a black bear, or no black bear. Now, we can train this model using, for example, images from 10 camera traps that are situated in a forest. And we might want to validate with a camera trap that is in a zoo. In one of the enclosures. The one with a bear. A polar one.\nThe issue with this dataset as a validation dataset is that is does not matches the problem we try to solve in many different ways. First, we will have an excess of images with bears compared to our problem environment. Camera traps can have a large number of spurious activation, resulting in images without animals in them (Newey et al. 2015). Second, the data will come from very different environments (forest v. zoo). Finally, we are attempting to validate on something that is an entirely different species of bear. This sounds like an egregious case (it is), but it is easy to commit this type of mistake when our data get more complex than black bear, polar bear, no bear.\nValidation is, in particular, very difficult when the dataset we use for training has extreme events (Bellocchi et al. 2010). Similarly, the efficiency of validation datasets can be limited if it reflects the same biases as the training data (Martinez-Meyer 2005). Recall that this validation dataset is used to decide on the ideal conditions to train the final model before testing (and eventually, deployment); it is, therefore, extremely important to get it right. A large number of techniques to split data (Søgaard et al. 2021; Goot 2021) use heuristics to minimize the risk of picking the wrong validation data.\n\n\n4.1.3 Testing\nThe testing dataset is special. The model has never touched it. Not during training, and not for validation. For this reason, we can give it a very unique status: it is an analogue to data that are newly collected, and ready to be passed through the trained model in order to make a prediction.\nThe only difference between the testing set and actual new data is that, for the testing set, we know the labels. In other words, we can compare the model output to these labels, and this gives us an estimate of the model performance on future data. Assuming that this data selection was representative of the real data we will use for our model once it is trained, the performance on the validation set should be a good baseline for what to expect in production.\nBut this requires a trained model, and we sort of glossed over this step.\nIn order to come up with a trained model, it would be a strange idea not to use the validation data – they are, after all, holding information about the data we want to model! Once we have evaluated our model on the validation set, we can start the last round of training to produce the final model. We do this by training the model using everything except the testing data. This is an appropriate thing to do: because we have evaluated the model on the validation data, and assuming that it has a correct performance, we can expect that retraining the model on the validation data will not change the performance of the model.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Cross-validation</span>"
    ]
  },
  {
    "objectID": "chapters/crossvalidation.html#the-problem-cherry-blossom-phenology",
    "href": "chapters/crossvalidation.html#the-problem-cherry-blossom-phenology",
    "title": "4  Cross-validation",
    "section": "4.2 The problem: cherry blossom phenology",
    "text": "4.2 The problem: cherry blossom phenology\nThe cherry blossom tree (Prunus) is renowned for its impressive bloom, which happens from March to April. The blooming, and associated festivals, are of particular cultural significance (Moriuchi and Basil 2019), and is therefore a cultural ecosystem service (Kosanic and Petzold 2020). Climate change has a demonstrable effect on the date of first bloom on Prunus species in Japan (Primack, Higuchi, and Miller-Rushing 2009), which can affect the sustainability of cherry blossom festivals in the short term (Sakurai et al. 2011).\nLong-term time series of the date of first bloom in Japan reveal that in the last decades, cherry blossom blooms earlier, which has been linked to, possibly, climate change and urbanization. Prunus species respond to environmental cues at the local level for their flowering (Mimet et al. 2009; Ohashi et al. 2011). The suspected causal mechanism is as follows: both global warming and urbanization lead to higher temperatures, which means a faster accumulation of degree days over the growing season, leading to an earlier bloom (Shi et al. 2017). Indeed, the raw data presented in Figure 4.1 show that trees bloom early when the temperatures are higher; the data for phenology have been collected by Aono and Kazui (2008), and the temperature reconstructions are from Aono and Saito (2009).\n\n\n\n\n\n\n\n\nFigure 4.1: The raw data show a negative relationship between the temperature in March, and the bloom time. This suggests that when the trees have accumulated enough temperature, they can bloom early. In a context of warming, we should therefore see earlier blooms with rising temperatures.\n\n\n\n\n\nWith these data in hand (day of year with the first bloom, and smoothed reconstructed temperature in March), we can start thinking about this hypothesis. But by contrast with our simple strategy in Chapter 3, this time, we will split our dataset into training, validation, and testing sets, as we discussed in the previous section. Yet there are many ways to split a dataset, and therefore before starting the analysis, we will have a look at a few of them.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Cross-validation</span>"
    ]
  },
  {
    "objectID": "chapters/crossvalidation.html#strategies-to-split-data",
    "href": "chapters/crossvalidation.html#strategies-to-split-data",
    "title": "4  Cross-validation",
    "section": "4.3 Strategies to split data",
    "text": "4.3 Strategies to split data\nBefore seeing examples of strategies for cross-validation, it is important to consider the high-level perspective of the way we will perform the entire training sequence. First, we need to keep a testing dataset. Depending on the problem, it may be feasible or desirable to use an external testing dataset (Homeyer et al. 2022). In problems for which the volume of data is limited (the 99.99% of biodiversity applications that do not involve metagenomics of remote sensing), this is almost impossible, and therefore we need to resort to removing a proportion of the data. It means that collected data will never be used for training, which is not ideal, but what we gain in return is a fairer appraisal of the performance of the model, which is a really advantageous trade-off. When the testing data are removed, we can start splitting the rest of the data in testing and validation sets. This can involve two broad categories of families: exhaustive splits (all data are used for training and evaluation), and non-exhaustive splits (the opposite; for once, the terminology makes sense!).\n\n4.3.1 Holdout\nThe holdout method is what we used in Chapter 3, in which we randomly selected some observations to be part of the validation data (which was, in practice, a testing dataset in this example), and kept the rest to serve as the training data. Holdout cross-validation is possibly the simplest technique, but it suffers from a few drawbacks.\nThe model is only trained for one split of the data, and similarly only evaluated for one split of the data. There is, therefore, a chance to sample a particularly bad combination of the data that lead to erroneous results. Attempts to quantify the importance of the predictors are likely to give particularly unstable results, as the noise introduced by picking a single random subset will not be smoothed out by multiple attempts.\nIn addition, as Hawkins, Basak, and Mills (2003) point out, holdout validation is particularly wasteful in data-limited settings, where there are fewer than hundreds of observations. The reason is that the holdout dataset will never contribute to training, and assuming the data are split 80/20, one out of five observations will not contribute to the model. Other cross-validation schemes presented in this section will allow observations to be used both for training and validation.\n\n\n4.3.2 Leave-p-out\nIn leave-p-out cross-validation (LpOCV), starting from a dataset on \\(n\\) observations, we pick \\(p\\) at random to serve as validation data, and \\(n-p\\) to serve as the training dataset. This process is then repeated exhaustively, which is to say we split the dataset in every possible way that gives \\(p\\) and \\(n-p\\) observations, for a set value of \\(p\\). The model is then trained on the \\(n-p\\) observations, and validated on the \\(p\\) observations for validation, and the performance (or loss) is averaged to give the model performance before testing.\nCelisse (2014) points out that \\(p\\) has to be large enough (relative to the sample size \\(n\\)) to overcome the propensity of the model to overfit on a small training dataset. One issue with LpOCV is that the number of combinations is potentially very large. It is, in fact, given by the binomial coefficient \\(\\binom{n}{p}\\), which gets unreasonably large even for small datasets. For example, running LpOCV on \\(n=150\\) observations, leaving out \\(p=10\\) for validation every time, would require to train the model about \\(10^{15}\\) times. Assuming we can train the model in \\(10^{-3}\\) seconds, the entire process would require 370 centuries.\nOh well.\n\n\n4.3.3 Leave-one-out\nThe leave-one-out cross-validation (LOOCV) is a special case of LpOCV with \\(p=1\\). Note that it is a lot faster to run than LpOCV, because \\(\\binom{n}{1}=n\\), and so the validation step runs in \\(\\mathcal{O}(n)\\) (LpOCV runs in \\(\\mathcal{O}(n!)\\)). LOOCV is also an exhaustive cross-validation technique, as every possible way to split the dataset will be used for training and evaluation.\n\n\n4.3.4 k-fold\nOne of the most frequent cross-validation scheme is k-fold cross-validation. Under this approach, the dataset is split into \\(k\\) equal parts (and so when \\(k = n\\), this is also equivalent to LOOCV). Like with LOOCV, one desirable property of k-fold cross-validation is that each observation is used exactly one time to evaluate the model , and exactly \\(k-1\\) times to train it.\nBut by contrast with the holdout validation approach, all observations are used to train the model.\nWhen the data have some specific structure, it can be a good thing to manipulate the splits in order to maintain this structure. For example, Bergmeir and Benítez (2012) use temporal blocks for validation of time series, and retain the last part of the series for testing (we illustrate this in Figure 4.2). For spatial data, Hijmans (2012) suggests the use of a null model based on distance to training sites to decide on how to split the data; Valavi et al. (2018) have designed specific k-fold cross-validation schemes for species distribution models. These approaches all belong to the family of stratified k-fold cross-validation (Zeng and Martinez 2000).\n\n\n\n\n\n\n\n\nFigure 4.2: An illustration of a series of folds on a timeseries. The grey data are used for training, the green data for validation, and the purple data are kept for testing. Note that in this context, we sometimes use the future to validate on the past (look at the first fold!), but this is acceptable for reasons explained in the text.\n\n\n\n\n\nThe appropriate value of \\(k\\) is often an unknown. It is common to use \\(k = 10\\) as a starting point (tenfold cross-validation), but other values are justifiable based on data volume, or complexity of the model training, to name a few.\n\n\n4.3.5 Monte-Carlo\nOne limitation of k-fold cross-validation is that the number of splits is limited by the amount of observations, especially if we want to ensure that there are enough samples in the validation data. To compensate for this, Monte-Carlo cross-validation is essentially the application (and averaging) of holdout validation an arbitrary number of times. Furthermore, the training and validation datasets can be constructed in order to account for specific constraints in the dataset, giving more flexibility than k-fold cross-validation (Roberts et al. 2017). When the (computational) cost of training the model is high, and the dataset has specific structural constraints, Monte-Carlo cross-validation is a good way to generate data for hyperparameters tuning.\nOne issue with Monte-Carlo cross-validation is that we lose the guarantee that every observation will be used for training at least once (and similarly for validation). Trivially, this becomes less of an issue when we increase the number of replications, but then this suffers from the same issues as LpOCV, namely the unreasonable computational requirements.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Cross-validation</span>"
    ]
  },
  {
    "objectID": "chapters/crossvalidation.html#application-when-do-cherry-blossom-bloom",
    "href": "chapters/crossvalidation.html#application-when-do-cherry-blossom-bloom",
    "title": "4  Cross-validation",
    "section": "4.4 Application: when do cherry blossom bloom?",
    "text": "4.4 Application: when do cherry blossom bloom?\nThe model we will train for this section is really simple: \\(\\text{bloom day} = m \\times \\text{temperature} + b\\). This is a linear model, and one with a nice, direct biological interpretation: the average (baseline) day of bloom is \\(b\\), and each degree of temperature expected in March adds \\(m\\) days to the bloom date. At this point, we might start thinking about the distribution of the response, and what type of GLM we should used, but no. Not today. Today, we want to iterate quickly, and so we will start with a model that is exactly as simple as it needs to be: this is, in our case, linear regression.\nAt this point, we may be tempted to think a little more deeply about the variables and the structure of the model, to express the bloom day as a departure from the expected value, and similarly with the temperature, using for example the z-score. This is a transformation we will apply starting from Chapter 5, but in order to apply it properly, we need to consider some elements that will be introduced in Section 6.2. For this reason, we will not apply any transformation to the data yet; feel free to revisit this exercise after reading through Section 6.2.\nThis approach (start from a model that is suspiciously simple) is a good thing, for more than a few reasons. First, it gives us a baseline to compare more complicated models against. Second, it means that we do not need to focus on the complexity of the code (and the model) when building a pipeline for the analysis. Finally, and most importantly, it gives us a result very rapidly, which enables a loop of iterative model refinement on a very short timescale. Additionally, at least for this example, the simple models often work well enough to support a discussion of the model and training process.\n\n4.4.1 Performance evaluation\nWe can visualize the results of our model training and assessment process. These results are presented in Figure 4.3 (as well as in Table 4.2, if you want to see the standard deviation across all splits), and follow the same color-coding convention we have used so far. All three loss measures presented here express their loss in the units of the response variable, which in this case is the day of the year where the bloom was recorded. These results show that our trained model achieves a loss of the order of a day or two in the testing data, which sounds really good!\n\n\n\n\n\n\n\n\nFigure 4.3: Visualisation of the model performance for three loss functions (MA, RMSE, MBE, as defined in Table 3.1). The colors are the same as in Figure 4.2, i.e. grey for the training data, green for the validation data, and purple for the testing data.\n\n\n\n\n\nYet it is important to contextualize these results. What does it means for our prediction to be correct plus or minus two days? There are at least two important points to consider.\n\n\n\n\n\n\n\n\n\nDataset\nMeasure\nLoss (avg.)\nLoss (std. dev.)\n\n\n\n\nTesting\nMAE\n1.696\n\n\n\nTraining\nMAE\n2.2397\n0.0482364\n\n\nValidation\nMAE\n2.26331\n0.421513\n\n\nTesting\nMBE\n0.0971036\n\n\n\nTraining\nMBE\n9.8278e-15\n1.15597e-14\n\n\nValidation\nMBE\n0.000419595\n0.910229\n\n\nTesting\nMSE\n4.49123\n\n\n\nTraining\nMSE\n8.04855\n0.32487\n\n\nValidation\nMSE\n8.24897\n2.93094\n\n\nTesting\nRMSE\n2.11925\n\n\n\nTraining\nRMSE\n2.83648\n0.0570941\n\n\nValidation\nRMSE\n2.82514\n0.545232\n\n\n\n\n\n\n\nTable 4.2: TODO\n\n\n\n\nFirst, what are we predicting? Our response variable is not really the day of the bloom, but is rather a smoothed average looking back some years, and looking ahead some years too. For this reason, we are removing a lot of the variability in the underlying time series. This is not necessarily a bad thing, especially if we are looking for a trend at a large temporal scale, but it means that we should not interpret our results at a scale lower than the duration of the window we use for averaging.\nSecond, what difference does a day make? Figure 4.1 shows that most of the days of bloom happen between day-of-year 100 and day-of-year 110. Recall that the MAE is measured by taking the average absolute error – a mistake of 24 hours is 10% of this interval! This is an example of how thinking about the units of the loss function we use for model evaluation can help us contextualize the predictions, and in particular how actionable they can be.\n\n\n4.4.2 Model predictions\nThe predictions of our model are presented in Figure 4.4; these are the predictions of the final model, that is, the model that we trained on everything except the testing data, and for which we can get the performance by looking at Figure 4.3.\n\n\n\n\n\n\n\n\nFigure 4.4: Overview of the fit of the final model (trained on all the training examples), visualized as the time series. Note that the year was not used as a variable in the model. The purple part of the prediction corresponds to the prediction of the model for the testing data, which are zoomed-in on in Figure 4.5. Although the model captures the cycles reasonably well, it tends to smooth out a lot of extreme events.\n\n\n\n\n\nThe question we now need to answer is: is our model doing a good job? We can start thinking about this question in a very qualitative way: yes, it does a goob job at drawing a line that, through time, goes right through the original data more often that it doesn’t. As far as validation goes, it maybe underestimates the drop in the response variable (it predicts the bloom a little later), but maybe there are long-term effects, expressed over the lifetime of the tree (the first bloom usually takes places after 6 or 7 growth seasons), that we do not account for.\n\n\nThink about the structure of linear models. Can we use information about the previous years in our model? Would there be a risk associated to adding more parameters?\nOur model tends to smooth out some of the variation; it does not predict bloom dates before day of year 100, or after day of year 108, although they do happen. This may not be a trivial under-prediction: some of these cycles leading to very early/late bloom can take place over a century, meaning that our model could be consistently wrong (which is to say, wrong with the same bias) for dozens of years in a row.\n\n\n4.4.3 Is our model good, then?\nThe answer is, it depends. Models are neither good, nor bad. They are either fit, or unfit, for a specific purpose.\nIf the purpose is to decide when to schedule a one-day trip to see the cherry blossom bloom, our model is not really fit – looking at the predictions, it gets within a day of the date of bloom (but oh, by the way, this is an average over almost a decade!) about 15% of the time, which jumps up to almost 30% if you accept a two-days window of error.\nIf the purpose is to look at long-time trends in the date of bloom, then our model actually works rather well. It does under-estimate the amplitude of the cycles, but not by a large amount. In fact, we could probably stretch the predictions a little, applying a little correction factor, and have a far more interesting model.\nWe will often be confronted to this question when working with prediction. There is not really a criteria for “good”, only a series of compromises and judgment calls about “good enough”. This is important. It reinforces the imperative of keeping the practice of data science connected to the domain knowledge, as ultimately, a domain expert will have to settle on whether to use a model or not.\n\n\n\n\n\n\n\n\nFigure 4.5: Overview of the model predictions on the testing data. Note that the model still smoothes out some of the extreme values. More importantly, it seems that it is under-estimating the sharp decline in the day of first bloom that happens starting in 1950; this suggests that the model is not adequately capturing important processes shaping the data.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Cross-validation</span>"
    ]
  },
  {
    "objectID": "chapters/crossvalidation.html#conclusion",
    "href": "chapters/crossvalidation.html#conclusion",
    "title": "4  Cross-validation",
    "section": "4.5 Conclusion",
    "text": "4.5 Conclusion\nIn this chapter, we trained a linear regression model to predict the day of bloom of cherry blossom trees based on the predicted temperature in March. Although the model makes a reasonable error (of the order of a few days), a deeper investigation of the amplitude of this error compared to the amplitude of the response variable, and of the comparison of extreme values in the prediction and in the data, led us to a more cautious view about the usefulness of this model. In practice, if we really wanted to solve this problem, this is the point where we would either add variables, or try another regression algorithm, or both.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Cross-validation</span>"
    ]
  },
  {
    "objectID": "chapters/classification.html",
    "href": "chapters/classification.html",
    "title": "5  Supervised classification",
    "section": "",
    "text": "References",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Supervised classification</span>"
    ]
  },
  {
    "objectID": "chapters/classification.html#the-problem-distribution-of-an-endemic-species",
    "href": "chapters/classification.html#the-problem-distribution-of-an-endemic-species",
    "title": "5  Supervised classification",
    "section": "5.1 The problem: distribution of an endemic species",
    "text": "5.1 The problem: distribution of an endemic species\nThroughout these chapters, we will be working on a single problem, which is to predict the distribution of the Corsican nuthatch, Sitta whiteheadi. The Corsican nuthatch is endemic to Corsica, and its range has been steadily shrinking over time due to loss of habitat through human activity, including fire, leading to it being classified as “vulnerable to extinction” by the International Union for the Conservation of Nature. Barbet-Massin and Jiguet (2011) nevertheless show that the future of this species is not necessarily all gloom and doom, as climate change is not expected to massively affect its distribution.\nSpecies Distribution Modeling (SDM; Elith and Leathwick (2009)), also known as Ecological Niche Modeling (ENM), is an excellent instance of ecologists doing applied machine learning already, as Beery et al. (2021) rightfully pointed out. In fact, the question of fitness-for-purpose, which we discussed in previous chapters (for example in Section 4.4.3), has been covered in the SDM literature (Guillera-Arroita et al. 2015). In these chapters, we will fully embrace this idea, and look at the problem of predicting where species can be as a data science problem. In the next chapters, we will converge again on this problem as an ecological one. Being serious our data science practices when fitting a species distribution model is important: Chollet Ramampiandra et al. (2023) make the important point that it is easy to overfit more complex models, at which point they cease outperforming simple statistical models.\nBecause this chapter is the first of a series, we will start by building a bare-bones model on ecological first principles. This is an important step. The rough outline of a model is often indicative of how difficult the process of training a really good model will be. But building a good model is an iterative process, and so we will start with a very simple model and training strategy, and refine it over time. In this chapter, the purpose is less to have a very good training process; it is to familiarize ourselves with the task of classification.\nWe will therefore start with a blanket assumption: the distribution of species is something we can predict based on temperature and precipitation. We know this to be important for plants (Clapham et al. 1935) and animals (Whittaker 1962), to the point where the relationship between mean temperature and annual precipitation is how we find delimitations between biomes. If you need to train a lot of models on a lot of species, temperature and precipitation are not the worst place to start (Berteaux 2014).\nConsider our dataset for a minute. In order to predict the presence of a species, we need information about where the species has been observed; this we can get from the Global Biodiversity Information Facility. We need information about where the species has not been observed; this is usually not directly available, but there are ways to generate background points that are a good approximation of this (Hanberry, He, and Palik 2012; Barbet-Massin et al. 2012). All of these data points come in the form \\((\\text{lat.}, \\text{lon.}, y)\\), which give a position in space, as well as \\(y = \\{+,-\\}\\) (the species is present or absent!) at this position.\nTo build a model with temperature and precipitation as inputs, we need to extract the temperature and precipitation at all of these coordinates. We will use the CHELSA2 dataset (Karger et al. 2017), at a resolution of 30 seconds of arc. WorldClim2 (Fick and Hijmans 2017) is also appropriate, but is known to have some artifacts.\nThe predictive task we want to complete is to get a predicted presence or absence \\(\\hat y = \\{+,-\\}\\), from a vector \\(\\mathbf{x}^\\top = [\\text{temp.} \\quad \\text{precip.}]\\). This specific task is called classification, and we will now introduce some elements of theory.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Supervised classification</span>"
    ]
  },
  {
    "objectID": "chapters/classification.html#what-is-classification",
    "href": "chapters/classification.html#what-is-classification",
    "title": "5  Supervised classification",
    "section": "5.2 What is classification?",
    "text": "5.2 What is classification?\nClassification is the prediction of a qualitative response. In Chapter 2, for example, we predicted the class of a pixel, which is a qualitative variable with levels \\(\\{1, 2, \\dots, k\\}\\). This represented an instance of unsupervised learning, as we had no a priori notion of the correct class of the pixel. When building SDMs, by contrast, we often know where species are, and we can simulate “background points”, that represent assumptions about where the species are not. For this series of chapters, the background points have been generated by sampling preferentially the pixels that are farther away from known presences of the species.\n\n\nWhen working on \\(\\{+,-\\}\\) outcomes, we are specifically performing binary classification. Classification can be applied to more than two levels.\nIn short, our response variable has levels \\(\\{+, -\\}\\): the species is there, or it is not – we will challenge this assumption later in the series of chapters, but for now, this will do. The case where the species is present is called the positive class, and the case where it is absent is the negative class. We tend to have really strong assumptions about classification already. For example, monitoring techniques using environmental DNA (e.g. Perl et al. 2022) are a classification problem: the species can be present or not, \\(y = \\{+,-\\}\\), and the test can be positive of negative \\(\\hat y = \\{+,-\\}\\). We would be happy in this situation whenever \\(\\hat y = y\\), as it means that the test we use has diagnostic value. This is the essence of classification, and everything that follows is more precise ways to capture how close a test comes from this ideal scenario.\n\n5.2.1 Separability\nA very important feature of the relationship between the features and the classes is that, broadly speaking, classification is much easier when the classes are separable. Separability (often linear separability) is achieved when, if looking at some projection of the data on two dimensions, you can draw a line that separates the classes (a point in a single dimension, a plane in three dimension, and so on and so forth). For reasons that will become clear in ?sec-variableselection-curse, simply adding more predictors is not necessarily the right thing to do.\nIn Figure 5.1, we can see the temperature (in degrees) for locations with recorded presences of Corsican nuthatches, and for locations with assumed absences. These two classes are not quite linearly separable alongside this single dimension (maybe there is a different projection of the data that would change this; we will explore one in ?sec-variable-selection), but there are still some values at which our guess for a class changes. For example, at a location with a temperature colder than 1°C, presences are far more likely. For a location with a temperature warmer than 5°C, absences become overwhelmingly more likely. The locations with a temperature between 0°C and 5°C can go either way.\n\n\n\n\n\n\n\n\nFigure 5.1: This figures show the separability of the presences (orange) and pseudo-absences (grey) on the temperature and precipitation dimensions.\n\n\n\n\n\n\n\n5.2.2 The confusion table\nEvaluating the performance of a classifier (a classifier is a model that performs classification) is usually done by looking at its confusion table, which is a contingency table of the form\n\\[\n\\begin{pmatrix}\n\\text{TP} & \\text{FP}\\\\\n\\text{FN} & \\text{TN}\n\\end{pmatrix} \\,.\n\\tag{5.1}\\]\nThis can be stated as “counting the number of times each pair of (prediction, observation occurs)”, like so:\n\\[\n\\begin{pmatrix}\n|\\hat +, +| & |\\hat +, -|\\\\\n|\\hat -, +| & |\\hat -, -|\n\\end{pmatrix} \\,.\n\\tag{5.2}\\]\nThe four components of the confusion table are the true positives (TP; correct prediction of \\(+\\)), the true negatives (TN; correct prediction of \\(-\\)), the false positives (FP; incorrect prediction of \\(+\\)), and the false negatives (FN; incorrect prediction of \\(-\\)). Quite intuitively, we would like our classifier to return mostly elements in TP and TN: a good classifier has most elements on the diagonal, and off-diagonal elements as close to zero as possible (the proportion of predictions on the diagonal is called the accuracy, and we will spend Section 5.2.4 discussing why it is not such a great measure).\nAs there are many different possible measures on this matrix, we will introduce them as we go. In this section, it it more important to understand how the matrix responds to two important features of the data and the model: balance and bias.\nBalance refers to the proportion of the positive class. Whenever this balance is not equal to 1/2 (there are as many positives as negative cases), we are performing imbalanced classification, which comes with additional challenges; few ecological problems are balanced.\n\n\n5.2.3 The no-skill classifier\nThere is a specific hypothetical classifier, called the no-skill classifier, which guesses classes at random as a function of their proportion. It turns out to have an interesting confusion matrix! If we note \\(b\\) the proportion of positive classes, the no-skill classifier will guess \\(+\\) with probability \\(b\\), and \\(-\\) with probability \\(1-b\\). Because these are also the proportion in the data, we can write the adjacency matrix as\n\\[\n\\begin{pmatrix}\nb^2 & b(1-b)\\\\\n(1-b)b & (1-b)^2\n\\end{pmatrix} \\,.\n\\tag{5.3}\\]\nThe proportion of elements that are on the diagonal of this matrix is \\(b^2 + (1-b)^2\\). When \\(b\\) gets lower, this value actually increases: the more difficult a classification problem is, the more accurate random guesses look like. This has a simple explanation, which we expand Section 5.2.4 : when most of the cases are negative, if you predict a negative case often, you will by chance get a very high true negative score. For this reason, measures of model performance will combine the positions of the confusion table to avoid some of these artifacts.\n\n\nAn alternative to the no-skill classifier is the coin-flip classifier, in which classes have their correct prevalence \\(b\\), but the model picks at random (i.e. with probability \\(1/2\\)) within these classes.\nBias refers to the fact that a model can recommend more (or fewer) positive or negative classes than it should. An extreme example is the zero-rate classifier, which will always guess the most common class, and which is commonly used as a baseline for imbalanced classification. A good classifier has high skill (which we can measure by whether it beats the no-skill classifier for our specific problem) and low bias. In this chapter, we will explore different measures on the confusion table the inform us about these aspects of model performance, using the Naive Bayes Classifier.\n\n\n5.2.4 A note on accuracy\nIt is tempting to use accuracy to measure how good a classifier is, because it makes sense: it quantifies how many predictions are correct. But a good accuracy can hide a very poor performance. Let’s think about an extreme case, in which we want to detect an event that happens with prevalence \\(0.05\\). Out of 100 predictions, the confusion matrix of this model would be\n\\[\n\\begin{pmatrix}\n0 & 0 \\\\ 5 & 95\n\\end{pmatrix} \\,.\n\\]\nThe accuracy of this classifier would be \\(0.95\\), which seems extremely high! This is because prevalence is extremely low, and so most of the predictions are about the negative class: the model is on average really good, but is completely missing the point when it comes to making interesting predictions.\nIn fact, even a classifier that would not be that extreme would be mis-represented if all we cared about was the accuracy. If we take the case of the no-skill classifier, the accuracy is given by \\(b^2 + (1-b)^2\\), which is an inverted parabola that is maximized for \\(b \\approx 0\\) – a model guessing at random will appear better when the problem we want to solve gets more difficult.\nThis is an issue inherent to accuracy: it can tell you that a classifier is bad (when it is low), but it cannot really tell you when a classifier is good, as no-skill (or worse-than-no-skill) classifiers can have very high values. It remains informative as an a posteriori measure of performance, but only after using reliable measures to ensure that the model means something.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Supervised classification</span>"
    ]
  },
  {
    "objectID": "chapters/classification.html#the-naive-bayes-classifier",
    "href": "chapters/classification.html#the-naive-bayes-classifier",
    "title": "5  Supervised classification",
    "section": "5.3 The Naive Bayes Classifier",
    "text": "5.3 The Naive Bayes Classifier\nThe Naive Bayes Classifier (NBC) is my all-time favorite classifier. It is built on a very simple intuition, works with almost no data, and more importantly, often provides an annoyingly good baseline for other, more complex classifiers to meet. That NBC works at all is counter-intuitive (Hand and Yu 2001). It assumes that all variables are independent, it works when reducing the data to a simpler distribution, and although the numerical estimate of the class probability can be somewhat unstable, it generally gives good predictions. NBC is the data science equivalent of saying “eh, I reckon it’s probably this class” and somehow getting it right 95% of the case [there are, in fact, several papers questioning why NBC works so well; see e.g. Kupervasser (2014)].\n\n5.3.1 How the NBC works\nIn Figure 5.1, what is the most likely class if the temperature is 12°C? We can look at the density traces on top, and say that because the one for presences is higher, we would be justified in guessing that the species is present. Of course, this is equivalent to saying that \\(P(12^\\circ C | +) &gt; P(12^\\circ C | -)\\). It would appear that we are looking at the problem in the wrong way, because we are really interested in \\(P(+ | 12^\\circ C)\\), the probability that the species is present knowing that the temperature is 12°C.\nUsing Baye’s theorem, we can re-write our goal as\n\\[\nP(+|x) = \\frac{P(+)}{P(x)}P(x|+) \\,,\n\\tag{5.4}\\]\nwhere \\(x\\) is one value of one feature, \\(P(x)\\) is the probability of this observation (the evidence, in Bayesian parlance), and \\(P(+)\\) is the probability of the positive class (in other words, the prior). So, this is where the “Bayes” part comes from.\nBut why is NBC naïve?\nIn Equation 5.4, we have used a single feature \\(x\\), but the problem we want to solve uses a vector of features, \\(\\mathbf{x}\\). These features, statisticians will say, will have covariance, and a joint distribution, and many things that will challenge the simplicity of what we have written so far. These details, NBC says, are meaningless.\nNBC is naïve because it makes the assumptions that the features are all independent. This is very important, as it means that \\(P(+|\\mathbf{x}) \\propto P(+)\\prod_i P(\\mathbf{x}_i|+)\\) (by the chain rule). Note that this is not a strict equality, as we need to divide by the evidence. But the evidence is constant across all classes, and so we do not need to measure it to get an estimate of the score for a class.\nTo generalize our notation, the score for a class \\(\\mathbf{c}_j\\) is \\(P(\\mathbf{c}_j)\\prod_i P(\\mathbf{x}_i|\\mathbf{c}_j)\\). In order to decide on a class, we apply the following rule:\n\\[\n\\hat y = \\text{argmax}_j \\, P(\\mathbf{c}_j)\\prod_i P(\\mathbf{x}_i|\\mathbf{c}_j) \\,.\n\\tag{5.5}\\]\nIn other words, whichever class gives the higher score, is what the NBC will recommend for this instance \\(\\mathbf{x}\\). In Chapter 7, we will improve upon this model by thinking about the evidence \\(P(\\mathbf{x})\\), but as you will see, this simple formulation will already prove frightfully effective.\n\n\n5.3.2 How the NBC learns\nThere are two unknown quantities at this point. The first is the value of \\(P(+)\\) and \\(P(-)\\). These are priors, and are presumably important to pick correctly. In the spirit of iterating rapidly on a model, we can use two starting points: either we assume that the classes have the same probability, or we assume that the representation of the classes (the balance of the problem) is their prior. More broadly, we do not need to think about \\(P(-)\\) too much, as it is simply \\(1-P(+)\\), since the “state” of every single observation of environmental variables is either \\(+\\) or \\(-\\).\nThe most delicate problem is to figure out \\(P(x|c)\\), the probability of the observation of the variable when the class is known. There are variants here that will depend on the type of data that is in \\(x\\); as we work with continuous variables, we will rely on Gaussain NBC. In Gaussian NBC, we will consider that \\(x\\) comes from a normal distribution \\(\\mathcal{N}(\\mu_{x,c},\\sigma_{x,c})\\), and therefore we simply need to evaluate the probability density function of this distribution at the point \\(x\\). Other types of data are handled in the same way, with the difference that they use a different set of distributions.\nTherefore, the learning stage of NBC is extremely quick: we take the mean and standard deviation of the values, split by predictor and by class, and these are the parameters of our classifier. By contrast to the linear regression approach we worked with in Chapter 3, the learning phase only involves a single epoch: measuring the mean and standard deviation.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Supervised classification</span>"
    ]
  },
  {
    "objectID": "chapters/classification.html#application-a-baseline-model-of-the-corsican-nuthatch",
    "href": "chapters/classification.html#application-a-baseline-model-of-the-corsican-nuthatch",
    "title": "5  Supervised classification",
    "section": "5.4 Application: a baseline model of the Corsican nuthatch",
    "text": "5.4 Application: a baseline model of the Corsican nuthatch\nIn this section, we will have a look at the temperature and precipitation data from Figure 5.1, and come up with a first version of our classifier.\n\n5.4.1 Training and validation strategy\nTo evaluate our model, as we discussed in Chapter 4, we will keep a holdout testing set, that will be composed of 20% of the observations. In this chapter, we will not be using these data, because in order to use them as a stand-in for future predictions, it is important that the model only sees them once (this will happen at the end of Chapter 7). Therefore, for the next chapters, we will limit ourselves to an evaluation of the model performance based on the average values of the performance measure we picked as the most informative, calculated on the validation datasets. In this chapter, we will rely on Monte-Carlo cross validation (MCCV; see Section 4.3.5), using 50 replicates. In the following chapters, we will revert to using k-folds cross-validation, but using MCCV here is a good enough starting point.\nIn order to see how good our model really is, we will also compare its performances to the no-skill classifier. This is almost never a difficult classifier to outperform, but this nevertheless provides a good indication of whether our model works at all. In subsequent chapters, we will introduce a slightly more domain-specific model to provide a baseline that would look like an actual model we would like to out-perform.\n\n\n5.4.2 Performance evaluation of the model\nIn order to get a sense of the performance of our model, we will need to decide on a performance measure. This is an important step, as we will use the average value of this measure on the validation data to decide on the best model before reporting the expected performance. If we pick a measure that is biased, we will therefore use a model that is biased. Following Chicco and Jurman (2020) and Jurman, Riccadonna, and Furlanello (2012), we will use the Matthew’s Correlation Coefficient (MCC) as the “main” measure to evaluate the performance of a model (we will return to other alternative measures in Chapter 7).\nThe MCC is defined as\n\\[\n\\frac{\\text{TP}\\times \\text{TN} - \\text{FP}\\times \\text{FN}}{\\sqrt{(\\text{TP}+\\text{FP})\\times (\\text{TP}+\\text{FN})\\times (\\text{TN}+\\text{FP})\\times (\\text{TN}+\\text{FN})}} \\,.\n\\]\nThe MCC is a correlation coefficient (specifically, the Pearson product-moment correlation on a contingency table; Powers (2020)), meaning that it returns values in \\([-1, 1]\\). A negative value indicates perfectly wrong predictions, a value of 0 indicates no-skill, and a value of 1 indicates perfect predictions. Therefore, if we pick the model with the highest MCC, we are likely to pick the best possible model.\nIn addition to reporting the MCC, we will also look at values that inform us on the type of biases in the model, namely the positive and negative predictive values. These values, respectively \\(\\text{TP}/(\\text{TP}+\\text{FP})\\) and \\(\\text{TN}/(\\text{TN}+\\text{FN})\\), measure how likely a prediction of, respectively, presence and absence, are. These range in \\([0,1]\\), and values of one indicate a better performance of the model.\nWhy not pick one of these instead of the MCC? Well, all modeling is compromise; we don’t want a model to become too good at predicting absences, to the point where prediction about presences would become meaningless. Selecting models on the basis of a measure that only emphasizes one outcome is a risk that we shouldn’t be willing to take. For this reason, measures that are good at optimizing the value of a negative and a positive predictions are far better representations of the performance of a model. The MCC does just this.\n\n\n\n\n\n\n\n\nFigure 5.2: Overview of the scores for the Matthew’s correlation coefficient, as well as the positive and negative predictive values.\n\n\n\n\n\nThe output of cross-validation is given in Figure 5.2 (and compared to the no-skill classifier in Table 5.1). As we are satisfied with the model performance, we can re-train it using all the data (but not the part used for testing) in order to make our first series of predictions.\n\n\n\n\n\n\n\n\n\n\n\n\nMeasure\nTraining\nValidation\nNo-skill\n\n\n\n\nAccuracy\n0.879\n0.882\n0.505\n\n\nNPV\n0.857\n0.861\n0.449\n\n\nPPV\n0.899\n0.9\n0.551\n\n\nMCC\n0.757\n0.762\n0.0\n\n\n\n\n\nTable 5.1: Overview of the data presented in Figure 5.2, compared to the no-skill classifier.\n\n\n\n\n\n5.4.3 The decision boundary\nNow that the model is trained, we can take a break in our discussion of its performance, and think about why it makes a specific classification in the first place. Because we are using a model with only two input features, we can generate a grid of variables, and the ask, for every point on this grid, the classification made by our trained model. This will reveal the regions in the space of parameters where the model will conclude that the species is present.\n\n\n\n\n\n\n\n\nFigure 5.3: Overview of the decision boundary between the positive (blue) and negative (classes) using the NBC with two variables. Note that, as expected with a Gaussian distribution, the limit between the two classes looks circular. The assumption of statistical independance between the features means that we would not see, for example, an ellipse.\n\n\n\n\n\nThe output of this simulation is given in Figure 5.3. Of course, in a model with more features, we would need to adapt our visualisations, but because we only use two features here, this image actually gives us a complete understanding of the model decision process. Think of it this way: even if we lose the code of the model, we could use this figure to classify any input made of a temperature and a precipitation, and read what the model decision would have been.\nThe line that separates the two classes is usually refered to as the “decision boundary” of the classifier: crossing this line by moving in the space of features will lead the model to predict another class at the output. In this instance, as a consequence of the choice of models and of the distribution of presence and absences in the environmental space, the decision boundary is not linear.\n\n\nTake a minute to think about which places are more likely to have lower temperatures on an island. Is there an additional layer of geospatial information we could add that would be informative?\nIt is interesting to compare Figure 5.3 with, for example, the distribution of the raw data presented in Figure 5.1. Although we initially observed that temperature was giving us the best chance to separate the two classes, the shape of the decision boundary suggests that our classifier is considering that Corsican nuthatches enjoy colder climates with more rainfall.\n\n\n5.4.4 Visualizing the trained model\nWe can now go through all of the pixels in the island of Corsica, and apply the model to predict the presence of Sitta whiteheadi. This result is reported in Figure 5.4. In order to have a little more information about where the predictions can be trusted, we also perform a little bit of bootstrapping: in this approach, we re-train the model using samples with replacement of the training data (500 times), and apply this batch of models, and measure the proportion of times they give the same prediction as the model trained on all the data. When this is higher, this indicates that the prediction is robust with regard to the training dataset.\n\n\n\n\n\n\n\n\nFigure 5.4: Occurence data (left), prediction of presences in space (middle), with the uncertainty in the prediction derived from bootstrap replicates (right). As we could have anticipated from the high values of the MCC, even this simple model does an adequate job at predicting the presence of Sitta whiteheadi.\n\n\n\n\n\n\n\n5.4.5 What is an acceptable model?\nWhen comparing the prediction to the spatial distribution of occurrences (Figure 5.4), it appears that the model identifies an area in the northwest where the species is likely to be present, despite limited observations. This might result in more false positives, but this is the purpose of running this model – if the point data were to provide us with a full knowledge of the range, there would be no point in running the model. For this reason, it is very important to nuance our interpretation of what a false-positive prediction really is. We will get back to this discussion in the next chapters, when adding more complexity to the model.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Supervised classification</span>"
    ]
  },
  {
    "objectID": "chapters/classification.html#conclusion",
    "href": "chapters/classification.html#conclusion",
    "title": "5  Supervised classification",
    "section": "5.5 Conclusion",
    "text": "5.5 Conclusion\nIn this chapter, we introduced the Naive Bayes Classifier as a model for classification, and applied it to a data of species occurrence, in which we predicted the potential presence of the species using temperature and classification. Through cross-validation, we confirmed that this model gave a good enough performance (Figure 5.2), looked at the decisions that were being made by the trained model (Figure 5.3), and finally mapped the prediction and their uncertainty in space (Figure 5.4). In the next chapter, we will improve upon this model by looking at techniques to select and transform variables.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Supervised classification</span>"
    ]
  },
  {
    "objectID": "chapters/variableselection.html",
    "href": "chapters/variableselection.html",
    "title": "6  Variable preparation",
    "section": "",
    "text": "References",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Variable preparation</span>"
    ]
  },
  {
    "objectID": "chapters/variableselection.html#the-problem-optimal-set-of-bioclim-variables-for-the-corsican-nuthatch",
    "href": "chapters/variableselection.html#the-problem-optimal-set-of-bioclim-variables-for-the-corsican-nuthatch",
    "title": "6  Variable preparation",
    "section": "6.1 The problem: optimal set of BioClim variables for the Corsican nuthatch",
    "text": "6.1 The problem: optimal set of BioClim variables for the Corsican nuthatch\nThe BioClim suite of environmental variables are 19 measurements derived from monthly recordings of temperature and precipitation. They are widely used in species distribution modeling, despite some spatial discontinuities due to the methodology of their reconstruction (Booth 2022); this is particularly true when working from the WorldClim version (Fick and Hijmans 2017), and not as problematic when using other data products like CHELSA (Karger et al. 2017).\nThe definitions of the 19 BioClim variables are given in Table 6.1. As we can see from this table, a number of variables are either derived from the same months, or direct (even sometimes additive) combinations of one another. For this reason, and because there are 19 variables, this is a good dataset to evaluate the use of variable selection and transformation.\n\n\n\n\n\n\n\n\nLayer\nDescription\nDetails\n\n\n\n\nBIO1\nAnnual Mean Temperature\n\n\n\nBIO2\nMean Diurnal Range\nMean of monthly (max temp - min temp)\n\n\nBIO3\nIsothermality\n(BIO2/BIO7) (×100)\n\n\nBIO4\nTemperature Seasonality\nstandard deviation ×100\n\n\nBIO5\nMax Temperature of Warmest Month\n\n\n\nBIO6\nMin Temperature of Coldest Month\n\n\n\nBIO7\nTemperature Annual Range\n(BIO5-BIO6)\n\n\nBIO8\nMean Temperature of Wettest Quarter\n\n\n\nBIO9\nMean Temperature of Driest Quarter\n\n\n\nBIO10\nMean Temperature of Warmest Quarter\n\n\n\nBIO11\nMean Temperature of Coldest Quarter\n\n\n\nBIO12\nAnnual Precipitation\n\n\n\nBIO13\nPrecipitation of Wettest Month\n\n\n\nBIO14\nPrecipitation of Driest Month\n\n\n\nBIO15\nPrecipitation Seasonality\nCoefficient of Variation\n\n\nBIO16\nPrecipitation of Wettest Quarter\n\n\n\nBIO17\nPrecipitation of Driest Quarter\n\n\n\nBIO18\nPrecipitation of Warmest Quarter\n\n\n\nBIO19\nPrecipitation of Coldest Quarter\n\n\n\n\n\n\nTable 6.1: List of the 19 BioClim variables, including indications of their calculation. The model we used in Chapter 5 used BIO1 and BIO12.\n\n\n\n\n\nIn this chapter, we will try to improve the model introduced in Chapter 5, by evaluating different methods to prepare our predictor variables.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Variable preparation</span>"
    ]
  },
  {
    "objectID": "chapters/variableselection.html#sec-leakage",
    "href": "chapters/variableselection.html#sec-leakage",
    "title": "6  Variable preparation",
    "section": "6.2 What is data leakage?",
    "text": "6.2 What is data leakage?\nData leakage is a concept that is, if you can believe it, grosser than it sounds.\nThe purpose of this section is to put the fear of data leakage in you, because it can, and most assuredly will, lead to bad models, which is to say (as we discussed in Section 3.1), models that do not adequately represent the underlying data, in part because we have built-in some biases into them. In turn, this can eventually lead to decreased explainability of the models, which erodes trust in their predictions (Amarasinghe et al. 2023). As illustrated by Stock, Gregr, and Chan (2023), a large number of ecological applications of machine learning are particularly susceptible to data leakage, meaning that this should be a core point of concern for us.\n\n6.2.1 Consequences of data leakage\nWe take data leakage so seriously because it is one of the top ten mistakes in applied machine learning (Nisbet et al. 2018). Data leakage happens information “leaks” from the training conditions to the evaluation conditions; in other words, when the model is evaluated after mistakenly being fed information that would not be available in real-life situations. Note that this definition of leakage is different from another notion, namely the loss of data availability over time (Peterson et al. 2018).\nIt is worth stopping for a moment to consider what these “real-life situations” are, and how they differ from the training of the model. Most of this difference can be summarized by the fact that when we are applying a model, we can start from the model only. Which is to say, the data that have been used for the training and validation of the model may have been lost, without changing the applicability of the model: it works on entirely new data. We have discussed this situation in Section 4.1.3: the test of a model is conducted on data that have never been used for training, because we want to evaluate its performance in the conditions where it will be applied.\nBecause this is the behavior we want to simulate with a validation dataset, it is very important to fully disconnect the testing data from the rest of the data. We can illustrate this with an example. Let’s say we want to work on a time series of population size, such as provided by the BioTIME project (Dornelas et al. 2018). One naïve approach would be to split this the time series at random into three datasets. We can use one to train the models, one to validate these models, and a last one for testing.\nCongratulations! We have created data leakage! Because we are splitting our time series at random, the model will likely have been trained using data that date from after the start of the validation dataset. In other words: our model can peek into the future. This is highly unlikely to happen in practice, due to the laws of physics. A strategy that would prevent leakage would have been to pick a cut-off date to define the validation dataset, and then to decide how to deal with the training and testing sets.\n\n\n6.2.2 Avoiding data leakage\nThe most common advice given in order to prevent data leakage is the “learn/predict separation” (Kaufman, Rosset, and Perlich 2011). Simply put, this means that whatever happens to the data used for training cannot be simultaneously applied to the data used for testing (or validation).\n\nA counter-example where performing the transformation before the analysis is when the transformation is explicitly sought out as an embedding, where we want to predict the position of instances in the embedded space, as in .e.g. Runghen, Stouffer, and Dalla Riva (2022).\n\nAssume that we want to transform our data using a Principal Component Analysis (PCA; Pearson (1901)). Ecologists often think of PCA as a technique to explore data (Legendre and Legendre 2012), but it is so much more than that! PCA is a model, because we can derive, from the data, a series of weights (in the transformation matrix), which we can then apply to other datasets in order to project them in the space of the projection of the training data.\nIf we have a dataset \\(\\mathbf{X}\\), which we split into two components \\(\\mathbf{X}_0\\) for training ,and \\(\\mathbf{X}_1\\) for validation, there are two ways to use a PCA to transform these data. The first is \\(\\mathbf{T} = \\mathbf{X}\\mathbf{W}\\), which uses the full dataset. When we predict the position of the validation data, we could use the transformation \\(\\mathbf{T}_1 = \\mathbf{X}_1\\mathbf{W}\\), but this would introduce data leakage: we have trained the transformation we apply to \\(\\mathbf{X}_1\\) using data that are already in \\(\\mathbf{X}_1\\), and therefore we have not respected the learn/predict separation. This way to introduce data leakage is extremely common in the species distribution literature (see e.g. De Marco and Nóbrega 2018).\n\n\n\n\n\n\nFigure 6.1: Overview of a data transformation pipeline that introduces data leakage (left), or that does not introduce data leakage (right). In both cases, a transformation such as a PCA is applied to the data; in the example on the right, it is applied as part of the model, and can therefore be applied without breaking the train/predict separation. The pipeline on the left introduces data leakage, as the training data will be changed by information contained in the validation data.\n\n\n\nThe second (correct) way to handle this situation is to perform our PCA using \\(\\mathbf{T}_0 = \\mathbf{X}_0\\mathbf{W}_0\\), which is to say, the weights of our PCA are derived only from the training data. In this situation, whenever we project the data in the validation set using \\(\\mathbf{T}_1 = \\mathbf{X}_1\\mathbf{W}_0\\), we respect the learn/predict separation: the transformation of \\(\\mathbf{X}_1\\) is entirely independent from the data contained in \\(\\mathbf{X}_1\\). This is illustrated in Figure 6.1.\n\n6.2.2.1 How to work in practice?\nAlthough avoiding data leakage is a tricky problem, there is a very specific mindset we can adopt that goes a long way towards not introducing it in our analyses, and it is as follows: every data transformation step is a modeling step that is part of the learning process. We do not, for example, apply a PCA and train the model on the projected variables – we feed raw data into a model, the first step of which is to perform this PCA for us.\nThis approach works because everything that can be represented as numbers is a model that can be trained.\nIf you want to transform a variable using the z-score, this is a model! It has two parameters that you can learn from the data, \\(\\mu\\) (the average of the variable) and \\(\\sigma\\) (its standard deviation). You can apply it to a data point \\(y\\) with \\(\\hat y = (y - \\mu)\\sigma^{-1}\\). Because this is a model, we need a dataset to learn these parameters from, and because we want to maintain the learn/predict separation, we will use the train dataset to get the values of \\(\\mu_0\\) and \\(\\sigma_0\\). This way, when we want to get the z-score of a new observation, for example from the testing dataset, we can get it using \\(\\hat y_1 = (y_1 - \\mu_0)\\sigma_0^{-1}\\). The data transformation is entirely coming from information that was part of the training set.\nOne way to get the learn/predict transformation stupendously wrong is to transform our validation, testing, or prediction data using \\(\\hat y_1 = (y_1 - \\mu_1)\\sigma_1^{-1}\\). This can be easily understood with an example. Assume that the variable \\(y_0\\) is the temperature in our training dataset. We are interested in making a prediction in a world that is 2 degrees hotter, uniformly, which is to say that for whatever value of \\(y_0\\), the corresponding data point we use for prediction is \\(y_1 = y_0 + 2\\). If we take the z-score of this new value based on its own average and standard deviation, a temperature two degrees warmer in the prediction data will have the same z-score as its original value, or in other words, we have hidden the fact that there is a change in our predictors!\nTreating the data preparation step as a part of the learning process, which is to say that we learn every transformation on the training set, and retain this transformation as part of the prediction process, we are protecting ourselves against both data leakage and the hiding of relevant changes in our predictors.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Variable preparation</span>"
    ]
  },
  {
    "objectID": "chapters/variableselection.html#variable-selection",
    "href": "chapters/variableselection.html#variable-selection",
    "title": "6  Variable preparation",
    "section": "6.3 Variable selection",
    "text": "6.3 Variable selection\n\n6.3.1 The curse of dimensionality\nThe number of variables we use for prediction is the number of dimensions of a problem. It would be tempting to say that adding dimensions should improve our chances to find a feature alongside which the classes become linearly separable. If only!\nThe “curse of dimensionality” is the common term of everything breaking down when the dimensions of a problem increase. In our perspective, where we rely on the resemblance between features to make a prediction, increasing the dimensions of a problem means adding features, and it has important consequences on the distance between observations. Picture two points positioned at random on the unit interval: the average distance between them is 1/3. If we add one dimension, keeping two points but turning this line into a cube, the average distance would be about 1/2. For a cube, about 2/3. For \\(n\\) dimensions, we can figure out that the average distance grows like \\(\\sqrt{n/6 + c}\\), which is to say that when we add more dimensions, we make the average distance between two points go to infinity. This effect is also affecting ecological studies (e.g. Smith et al. 2017).\nTherefore, we need to approach the problem of “which variables to use” with a specific mindset: we want a lot of information for our model, but not so much that the space in which the predictors exist turns immense. There are techniques for this.\n\n\n6.3.2 Step-wise approaches to variable selection\nIn order to try and decrease the dimensionality of a problem, we can attempt to come up with a method to decide which variables to include, or to remove, from a model. This practice is usually called “stepwise” selection, and is the topic of intense debate in ecology, although several studies point to the fact that there is rarely a best technique to select variables (Murtaugh 2009), that the same data can usually be adequately described by competing models (WHITTINGHAM et al. 2006), and that classifiers can show high robustness to the inclusion of non-informative variables (Fox et al. 2017). Situations in which variable selection has been shown top be useful is the case of model transfer (Petitpierre et al. 2016), or (when informed by ecological knowledge), the demonstration that classes of variables had no measurable impact on model performance (Thuiller, Araújo, and Lavorel 2004).\nWhy, so, should we select the variables we put in our models?\nThe answer is simple: we seek to solve a specific problem in an optimal way, where “optimal” refers to the maximization of a performance measure we decided upon a priori. In our case, this is the MCC. Therefore, an ideal set of predictors is the one that, given our cross-validation strategy, maximizes our measure of performance.\n\n\n6.3.3 Forward selection\nIn forward selection, assuming that we have \\(f\\) features, we start by building \\(f\\) models, each using one feature. For example, using the BioClim variables, \\(m_1\\) would be attempting to predict presences and absences based only on temperature. Out of these models, we retain the variable given by \\(\\text{argmax}_f \\text{MCC}(m_f)\\), where \\(\\text{MCC}(m_f)\\) is the average value of MCC for the \\(f\\)-th model on the validation datasets. This is the first variable we add to our set of selected variables. We then train \\(f-1\\) models, and then again add the variable that leads to the best possible increase in the average value of the MCC. When we cannot find a remaining variable that would increase the performance of the model, we stop the process, and return the optimal set of variables. Forward selection can be constrained by, instead of starting from variables one by one, starting from a pre-selected set of variables that will always be included in the model.\nThere are two important things to consider here. First, the set of variables is only optimal under the assumptions of the stepwise selection process: the first variable is the one that boosts the predictive value of the model the most on its own, and the next variables in the context of already selected variables. Second, the variables are evaluated on the basis of their ability to improve the performance of the model; this does not imply that they are relevant to the ecological processes happening in the dataset. Infering mechanisms on the basis of variable selection is foolish (Tredennick et al. 2021).\n\n\n6.3.4 Backward selection\nThe opposite of forward selection is backward selection, in which we start from a complete set of variables, then remove the one with the worst impact on model performance, and keep proceeding until we cannot remove a variable without making the model worse. The set of variables that remain will be the optimal set of variables. In almost no cases will forward and backward selection agree on which set of variables is the best – we have to settle this debate by either picking the model with the least parameters (the most parsimonious), or the one with the best performance.\nWhy not evaluate all the combination of variables?\nKeep in mind that we do not know the number of variables we should use; therefore, for the 19 BioClim variables, we would have to evaluate \\(\\sum_f \\binom{19}{f}\\), which turns out to be an immense quantity (for example, \\(\\binom{19}{9}=92378\\)). For this reason, a complete enumeration of all variable combinations would be extremely wasteful.\n\n\n6.3.5 Removal of colinear variables\nCo-linearity of variables is challenging for all types of ecological models (Graham 2003). In the case of species distribution models (De Marco and Nóbrega 2018), the variables are expected to be strongly auto-correlated, both because they have innate spatial auto-correlation, and because they are derived from a smaller set of raw data (Dormann et al. 2012). For this reason, it is a good idea to limit the number of colinear variables. In the BioClim variables, there",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Variable preparation</span>"
    ]
  },
  {
    "objectID": "chapters/variableselection.html#multivariate-transformations",
    "href": "chapters/variableselection.html#multivariate-transformations",
    "title": "6  Variable preparation",
    "section": "6.4 Multivariate transformations",
    "text": "6.4 Multivariate transformations\n\n6.4.1 PCA-based transforms\nPrincipal Component Analysis (PCA) is one of the most widely used multi-variate techniques in ecology, and is a very common technique to prepare variables in applied machine learning. One advantage of PCA is that it serves both as a way to remove colinearity, in that the principal components are orthogonal, and as a way to reduce the dimensionality of the problem as long as we decide on a threshold on the proportion of variance explained, and only retain the number of principal components needed to reach this threshold. For applications where the features are high-dimensional, PCA is a well established method to reduce dimensionality and extract more information in the selected principal components (Howley et al. 2005). In PCA, the projection matrix \\(\\mathbf{P}\\) is applied to the data using \\(\\mathbf{P}^\\top(\\mathbf{x}-\\mathbf{\\mu})\\), where \\(\\mathbf{x}\\) is the feature matrix with means \\(\\mathbf{\\mu}\\). Typically, the dimensions of \\(\\mathbf{P}\\) are lower than the dimensions of \\(\\mathbf{x}\\), resulting in fewer dimensions to the problem. Cutoffs on the dimensions of \\(\\mathbf{P}\\) are typically expressed as a proportion of the overall variance maintained after the projection. Variants of PCA include kernel PCA (Schölkopf, Smola, and Müller 1998), using a higher-dimensional space to improve the separability of classes, and probabilistic PCA (Tipping and Bishop 1999), which relies on modeling the data within a latent space with lower dimensionality.\n\n\n6.4.2 Whitening transforms\nAnother class of potentially very useful data transformations is whitening transforms, which belongs to the larger category of decorrelation methods. These methods do not perform any dimensionality reduction, but instead remove the covariance in the datasets. Whitening has proven to be particularly effective at improving the predictive ability of models applied to data with strong covariance structure (Koivunen and Kostinski 1999). In essence, given a matrix of features \\(\\mathbf{x}\\), with averages \\(\\mathbf{\\mu}\\) and covariance \\(\\mathbf{C}\\), a whitening transform \\(\\mathbf{W}\\) is the one of the matrices that satisfies \\(\\mathbf{W}^\\top\\mathbf{C}\\mathbf{W} = \\mathbf{I}\\). In other words, the whitening transform results in a new set of features with unit variance and no covariance: the dimensionality of the problem remains the same but the new random variables are independent. Given any dataset with covariance matrix \\(\\mathbf{C}\\), if any \\(\\mathbf{W}\\) is a whitening transform, then so to are any matrices \\(\\mathbf{W}\\mathbf{R}\\) where \\(\\mathbf{R}\\) performs a rotation with \\(\\mathbf{R}^\\top\\mathbf{R} = \\mathbf{I}\\). The optimal whitening transform can be derived through a variety of ways (see e.g. Kessy, Lewin, and Strimmer 2018). The whitening transform is applied to the input vector using \\(\\mathbf{W}^\\top (\\mathbf{x}-\\mathbf{\\mu})\\): this results in new random variables that have a mean of 0, and unit variance. The new input vector after the transformation is therefore an instance of “white noise” (Vasseur and Yodzis 2004).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Variable preparation</span>"
    ]
  },
  {
    "objectID": "chapters/variableselection.html#application-optimal-variables-for-corsican-nuthatch",
    "href": "chapters/variableselection.html#application-optimal-variables-for-corsican-nuthatch",
    "title": "6  Variable preparation",
    "section": "6.5 Application: optimal variables for Corsican nuthatch",
    "text": "6.5 Application: optimal variables for Corsican nuthatch\nBefore we start, we can re-establish the baseline performance of the model from Chapter 5. In this (and the next) chapters, we will perform k-folds cross-validation (see Section 4.3.4 for a refresher), using \\(k=15\\). This strategy gives an average MCC of 0.76, which represents our “target”: any model with a higher MCC will be “better” according to our criteria.\nIn a sense, this initial model was already coming from a variable selection process, only we did not use a quantitative criteria to include variables. And so, it is a good idea to evaluate how our model performed, relative to a model including all the variables. Running the NBC again using all 19 BioClim variables from Table 6.1, we get an average MCC on the validation data of 0.794. This is a small increase, but an increase nevertheless – our dataset had information that was not captured by temperature and precipitation. But this model with all the variables most likely includes extraneous information that does not help, or even hinders, the predictive ability of our model. Therefore, there is probably a better version of the model somewhere, that uses the optimal set of variables, potentially with the best possible transformation applied to them.\nIn this section, we will start by evaluating the efficiency of different approaches to variable selection, then merge selection and transformation together to provide a model that is optimal with regards to the training data we have (the workflow is outlined in Figure 6.2). In order to evaluate the model, we will maintain the use of the MCC; in addition, we will report the PPV and NPV (like in Chapter 5), as well as the accuracy and True-Skill Statistic (TSS). The TSS is defined as the sum of true positive and true negative rates, minus one, and is an alternative measure to the MCC (although it is more sensitive to some biases). Although several authors have advocated for the use of TSS (ALLOUCHE, TSOAR, and KADMON 2006), Leroy et al. (2018) have an interesting discussion of how the TSS is particularly sensitive to issues in the quality of (pseudo) absence data. For this reason, and based on the literature we covered in Chapter 5, there is no strong argument against using MCC as our selection measure.\n\n\nIn Chapter 7, we will revisit the question of how the MCC is “better”, and spend more time evaluating alternatives. For now, we can safely assume that MCC is the best.\nTo prevent the risk of interpreting the list of variables that have been retained by the model, we will not make a list of which they are (yet). This is because, in order to discuss the relative importance of variables, we need to introduce a few more concepts and techniques, which will not happen until Chapter 8; at this point, we will revisit the list of variables identified during this chapter, and compare their impact on model performance to their actual importance in explaining predictions.\n\n\n\n\n\n\nFigure 6.2: Overview of the variable selection workflow; starting from a list of variables and a routine to select them, we will perform cross-validation and measure whether the model performance increases.\n\n\n\n\n6.5.1 Variable selection\nWe will perform four different versions of stepwise variable selection. Forward, forward from a pre-selected set of two variables (temperature and precipitation), backward, and based on the Variance Inflation Factor (with a cutoff of 10). The results are presented in Table 6.2.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel\nVariables\nMCC\nNPV\nPPV\nAcc.\nTSS\n\n\n\n\nChapter 5 baseline\n2\n0.76\n0.859\n0.901\n0.881\n0.76\n\n\nAll var.\n19\n0.794\n0.871\n0.922\n0.897\n0.795\n\n\nFwd.\n6\n0.848\n0.901\n0.946\n0.924\n0.849\n\n\nFwd. (constr.)\n8\n0.83\n0.888\n0.94\n0.915\n0.832\n\n\nBackw.\n9\n0.846\n0.901\n0.944\n0.923\n0.847\n\n\nVIF\n2\n0.466\n0.793\n0.708\n0.736\n0.434\n\n\n\n\n\nTable 6.2: Consequences of different variable selection approaches on the performance of the model, as evaluated by the MCC, NPV, PPV, accuracy, and True-Skill Statistic (TSS).\n\n\n\n\n\nThe best model is given by forward selection, although backwards selection also gives a very close performance. At this point, we may decide to keep these two strategies, and evaluate the effect of different transformations of the data.\n\n\n6.5.2 Variable transformation\nBased on the results from Table 6.2, we retain forward and backwards selection as our two stepwise selection methods, and now apply an additional transformation (as in Figure 6.2) to the subset of the variables. The results are presented in Table 6.3. Based on these results, and using the MCC as the criteria for the “best” model, we see that combining forward selection with a whitening transform gives the best predictive performance. Note that the application of a transformation does change the result of variable selection, as evidences by the fact that the number of retained variables changes when we apply a transformation.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSelection\nTransformation\nVariables\nMCC\nNPV\nPPV\nAcc.\nTSS\n\n\n\n\nFwd.\nPCA\n4\n0.853\n0.898\n0.952\n0.926\n0.855\n\n\nFwd.\nWhitening\n10\n0.877\n0.918\n0.956\n0.938\n0.878\n\n\nFwd.\nRaw data\n6\n0.848\n0.901\n0.946\n0.924\n0.849\n\n\nBackw.\nPCA\n13\n0.835\n0.888\n0.943\n0.918\n0.838\n\n\nBackw.\nWhitening\n15\n0.869\n0.913\n0.953\n0.934\n0.871\n\n\nBackw.\nRaw data\n9\n0.846\n0.901\n0.944\n0.923\n0.847\n\n\n\n\n\nTable 6.3: Model performance when coupling variable selection with variable transformation. The measures of performance are given as in Table 6.2, and as we use the same folds for validation, can be directly compared.\n\n\n\n\n\n\n\n6.5.3 Model selection\nIn Table 6.2 and ?tbl-predictions-transformation, we have evaluated a series of several modeling strategies, defined by a variable selection and transformation technique. Using the MCC as our reference for what constitutes the best model, we can now apply the model to the relevant set of predictors, in order to see how these refinements result in a new predicted range for the species.\nThese results are presented in Figure 6.3.\n\n\n\n\n\n\n\n\nFigure 6.3: Consequences of different variable transformations on the predicted range of Sitta whiteheadi, as introduced in Figure 5.4. Note that the small area of predicted presence in the Cap Corse (the Northern tip) has disappeared with the new set of variables and their optimal transformation.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Variable preparation</span>"
    ]
  },
  {
    "objectID": "chapters/variableselection.html#conclusion",
    "href": "chapters/variableselection.html#conclusion",
    "title": "6  Variable preparation",
    "section": "6.6 Conclusion",
    "text": "6.6 Conclusion\nIn this chapter, we have discussed the issues with dimensionality and data leakage, and established a methodology to reduce the number of dimensions (and possible re-project the variables) while maintaining the train/predict separation. This resulted in a model whose performance (as evaluated using the MCC) increased quite significantly, which resulted in the predicted range of Sitta whiteheadi changing in space.\nIn Chapter 7, we will finish to refine this model, by considering that the NBC is a probabilistic classifier, and tuning various hyper-parameters of the model using learning curves and thresholding. This will result in the final trained model, the behavior of which we will explore in Chapter 8, to understand how the model makes predictions.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Variable preparation</span>"
    ]
  },
  {
    "objectID": "chapters/learningcurves.html",
    "href": "chapters/learningcurves.html",
    "title": "7  Hyper-parameters tuning",
    "section": "",
    "text": "References",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Hyper-parameters tuning</span>"
    ]
  },
  {
    "objectID": "chapters/learningcurves.html#sec-learningcurves-threshold",
    "href": "chapters/learningcurves.html#sec-learningcurves-threshold",
    "title": "7  Hyper-parameters tuning",
    "section": "7.1 Classification based on probabilities",
    "text": "7.1 Classification based on probabilities\nWhen first introducing classification in Chapter 5 and Chapter 6, we used a model that returned a deterministic answer, which is to say, the name of a class (in our case, this class was either “present” or “absent”). But a lot of classifiers return quantitative values, that correspond to (proxies for) the probability of the different classes. Nevertheless, because we are interested in solving a classification problem, we need to end up with a confusion table, and so we need to turn a number into a class. In the context of binary classification (we model a yes/no variable), this can be done using a threshold for the probability.\n\n\nNote that the quantitative value returned by the classifier does not need to be a probability; it simply needs to be on an interval or ratio scale.\nThe idea behind the use of thresholds is simple: if the classifier output \\(\\hat y\\) is larger than (or equal to) the threshold value \\(\\tau\\), we consider that this prediction corresponds to the positive class (the event we want to detect, for example the presence of a species). In the other case, this prediction corresponds to the negative class. Note that we do not, strictly, speaking, require that the value \\(\\hat y\\) returned by the classifier be a probability. We can simply decide to pick \\(\\tau\\) somewhere in the support of the distribution of \\(\\hat y\\).\nThe threshold to decide on a positive event is an hyper-parameter of the model. In the NBC we built in Chapter 5, our decision rule was that \\(p(+) &gt; p(-)\\), which when all is said and done (but we will convince ourselves of this in Section 7.3.1), means that we used \\(\\tau = 0.5\\). But there is no reason to assume that the threshold needs to be one half. Maybe the model is overly sensitive to negatives. Maybe there is a slight issue with our training data that bias the model predictions. And for this reason, we have to look for the optimal value of \\(\\tau\\).\nThere are two important values for the threshold, at which we know the behavior of our model. The first is \\(\\tau = \\text{min}(\\hat y)\\), for which the model always returns a negative answer; the second is, unsurprisingly, \\(\\tau = \\text{max}(\\hat y)\\), where the model always returns a positive answer. Thinking of this behavior in terms of the measures on the confusion matrix, as we have introduced them in Chapter 5, the smallest possible threshold gives only negatives, and the largest possible one gives only positives: they respectively maximize the false negatives and false positives rates.\n\n7.1.1 The ROC curve\nThis is a behavior we can exploit, as increasing the threshold away from the minimum will lower the false negatives rate and increase the true positive rate, while decreasing the threshold away from the maximum will lower the false positives rate and increase the true negative rate. If we cross our fingers and knock on wood, there will be a point where the false events rates have decreased as much as possible, and the true events rates have increased as much as possible, and this corresponds to the optimal value of \\(\\tau\\) for our problem.\nWe have just described the Receiver Operating Characteristic (ROC; Fawcett (2006)) curve! The ROC curve visualizes the false positive rate on the \\(x\\) axis, and the true positive rate on the \\(y\\) axis. The area under the curve (the ROC-AUC) is a measure of the overall performance of the classifier (Hanley and McNeil 1982); a model with ROC-AUC of 0.5 performs at random, and values moving away from 0.5 indicate better (close to 1) or worse (close to 0) performance.The ROC curve is a description of the model performance across all of the possible threshold values we investigated!\n\n\n7.1.2 The PR curve\nOne very common issue with ROC curves, is that they are overly optimistic about the performance of the model, especially when the problem we work on suffers from class imbalance, which happens when observations of the positive class are much rarer than observations of the negative class. In ecology, this is a common feature of data on species interactions (Poisot et al. 2023). In addition, although a good model will have a high ROC-AUC, a bad model can get a high ROC-AUC too (Halligan, Altman, and Mallett 2015); this means that ROC-AUC alone is not enough to select a model.\nAn alternative to ROC is the PR (for precision-recall) curve, in which the positive predictive value is plotted against the true-positive rate; in other words, the PR curve (and therefore the PR-AUC) quantify whether a classifier makes reliable positive predictions, both in terms of these predictions being associated to actual positive outcomes (true-positive rate) and not associated to actual negative outcomes (positive predictive value). Because the PR curve uses the positive predictive values, it captures information that is similar to the ROC curve, but is in general more informative (Saito and Rehmsmeier 2015).\n\n\n7.1.3 A note on cross-entropy loss\nIn Chapter 3, we used loss functions to measure the progress of our learning algorithm. Unsurprisingly, loss functions exist for classification tasks too. One of the most common is the cross-entropy (or log-loss), which is defined as\n\\[\n−\\left[y \\times \\text{log}\\ p+(1−y)\\times \\text{log}\\ (1−p)\\right] \\,,\n\\]\nwhere \\(y\\) is the actual class, and \\(p\\) is the probability associated to the positive class. Note that the log-loss is very similar to Shannon’s measure of entropy, and in fact can be expressed based on the Kullback-Leibler divergence of the distributions of \\(y\\) and \\(p\\). Which is to say that log-loss measures how much information about \\(y\\) is conveyed by \\(p\\). In this chapter, we use measures like the MCC that describe the performance of a classifier when the predictions are done, but log-loss is useful when there are multiple epochs of training. Neural networks used for classification commonly use log-loss as a loss function; note that the gradient of the log-loss function is very easy to calculate, and that gives it its usefulness as a measure of learning.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Hyper-parameters tuning</span>"
    ]
  },
  {
    "objectID": "chapters/learningcurves.html#how-to-optimize-the-threshold",
    "href": "chapters/learningcurves.html#how-to-optimize-the-threshold",
    "title": "7  Hyper-parameters tuning",
    "section": "7.2 How to optimize the threshold?",
    "text": "7.2 How to optimize the threshold?\nIn order to understand the optimization of the threshold, we first need to understand how a model with thresholding works. When we run such a model on multiple input features, it will return a list of probabilities, for example \\([0.2, 0.8, 0.1, 0.5, 1.0]\\). We then compare all of these values to an initial threshold, for example \\(\\tau = 0.05\\), giving us a vector of Boolean values, in this case \\([+, +, +, +, +]\\). We can then compare this classified output to a series of validation labels, e.g. \\([-, +, -, -, +]\\), and report the performance of our model. In this case, the very low thresholds means that we accept any probability as a positive case, and so our model is very strongly biased. We then increase the threshold, and start again.\nAs we have discussed in Section 7.1, moving the threshold is essentially a way to move in the space of true/false rates. As the measures of classification performance capture information that is relevant in this space, there should be a value of the threshold that maximizes one of these measures. Alas, no one agrees on which measure this should be (Perkins and Schisterman 2006; Unal 2017). The usual recommendation is to use the True Skill Statistic, also known as Youden’s \\(J\\) (Youden 1950). The biomedical literature, which is quite naturally interested in getting the interpretation of tests right, has established that maximizing this value brings us very close to the optimal threshold for a binary classifier (Perkins and Schisterman 2005). In a simulation study, using the True Skill Statistic gave good predictive performance for models of species interactions (Poisot 2023).\nSome authors have used the MCC as a measure of optimality (Zhou and Jakobsson 2013), as it is maximized only when a classifier gets a good score for the basic rates of the confusion matrix. Based on this information, Chicco and Jurman (2023) recommend that MCC should be used to pick the optimal threshold regardless of the question, and I agree with their assessment. A high MCC is always associated to a high ROC-AUC, TSS, etc., but the opposite is not necessarily true. This is because the MCC can only reach high values when the model is good at everything, and therefore it is not possible to trick it. In fact, previous comparisons show that MCC even outperform measures of classification loss (Jurman, Riccadonna, and Furlanello 2012).\nFor once, and after over 15 years of methodological discussion, it appears that we have a conclusive answer! In order to pick the optimal threshold, we find the value that maximizes the MCC. Note that in previous chapters, we already used the MCC as a our criteria for the best model, and now you know why.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Hyper-parameters tuning</span>"
    ]
  },
  {
    "objectID": "chapters/learningcurves.html#application-improved-corsican-nuthatch-model",
    "href": "chapters/learningcurves.html#application-improved-corsican-nuthatch-model",
    "title": "7  Hyper-parameters tuning",
    "section": "7.3 Application: improved Corsican nuthatch model",
    "text": "7.3 Application: improved Corsican nuthatch model\nIn this section, we will finish the training of the model for the distribution of Sitta whiteheadi, by picking optimal hyper-parameters, and finally reporting its performance on the testing dataset. At the end of this chapter, we will therefore have established a trained model, that we will use in Chapter 8 to see how each prediction emerges.\n\n7.3.1 Making the NBC explicitly probabilistic\nIn Chapter 5, we have expressed the probability that the NBC recommends a positive outcome as\n\\[\n    P(+|x) = \\frac{P(+)}{P(x)}P(x|+)\\,,\n\\]\nand noted that because \\(P(x)\\) is constant across all classes, we could simplify this model as \\(P(+|x) \\propto P(+)P(x|+)\\). But because we know the only two possible classes are \\(+\\) and \\(-\\), we can figure out the expression for \\(P(x)\\). Because we are dealing with probabilities, we know that \\(P(+|x)+P(-|x) = 1\\). We can therefore re-write this as\n\\[\n\\frac{P(+)}{P(x)}P(x|+)+\\frac{P(-)}{P(x)}P(x|-) = 1\\,\n\\]\nwhich after some reorganization (and note that \\(P(-) = 1-P(+)\\)), results in\n\\[\nP(x) = P(+) P(x|+)+P(-) P(x|-) \\,.\n\\]\nThis value \\(P(x)\\) is the “evidence” in Bayesian parlance, and we can use this value explicitly to get the prediction for the probability associated to the class \\(+\\) using the NBC.\nNote that we can see that using the approximate version we used so far (the prediction is positive if \\(P(+) P(x|+) &gt; P(-) P(x|-)\\)) is equivalent to saying that the prediction is positive whenever \\(P(+|x) &gt; \\tau\\) with \\(\\tau = 0.5\\). In the next sections, we will challenge the assumption that \\(0.5\\) is the optimal value of \\(\\tau\\).\nIn Figure 7.1, we show the effect of moving the threshold from 0 to 1 on the value of the MCC. This figure reveals that the value of the threshold that maximizes the average MCC across folds is \\(\\tau \\approx 0.43\\). But more importantly, it seems that the “landscape” of the MCC around this value is relatively flat – in other words, as long as we do not pick a threshold that is too outlandishly low (or high!), the model would have a good performance. It is worth pausing for a minute and questioning why that is.\n\n\n\n\n\n\n\n\nFigure 7.1: Learning curve for the threshold of the NBC model. Note that the profile of the MCC with regards to the threshold is relatively flat. In other words, even picking a non-optimal value of the threshold would not necessarilly lead to a very bad model. Each grey line corresponds to a fold, and the blue line is the average.\n\n\n\n\n\nTo do so, we can look at the distribution of probabilities returned by the NBC, which are presented in Figure 7.2. It appears that the NBC is often confident in its recommendations, with a bimodal distribution of probabilities. For this reason, small changes in the position of the threshold would only affect a very small number of instances, and consequently only have a small effect on the MCC and other statistics. If the distribution of probabilities returned by the NBC had been different, the shape of the learning curve may have been a lot more skewed.\n\n\n\n\n\n\n\n\nFigure 7.2: Probabilities assigned to each pixel (bottom), color-coded by their value in the validation set (top scatterplots). The NBC is making a lot of recommendations very close to 0 or very close to 1, and for this reason, positioning the threshold anywhere in the middle of the range would give almost similar results in terms of the MCC.\n\n\n\n\n\n\n\n7.3.2 How good is the model?\nAfter picking a threshold and seeing how it relates to the distribution of probabilities in the model output, we can have a look at the ROC and PR curves. They are presented in Figure 7.3. In both cases, we see that the model is behaving correctly (it is nearing the point in the graph corresponding to perfect classifications), and importantly, we can check that the variability between the different folds is low. The model also outperforms the no-skill classifier. Taken together, these results give us a strong confidence in the fact that our model with the threshold applied represents an improvement over the version without the threshold.\n\n\n\n\n\n\n\n\nFigure 7.3: ROC and PR curve for each fold, calculated on the validation datasets. The area highlighted in green corresponds to perfect classifiers, and the dashed line is the no-skill classifier. The solid arrow shows direction alongside which model performance increases in both cases.\n\n\n\n\n\n\n\n7.3.3 Fine-tuning the NBC prior\nIn the previous section, we have assumed that the prior on occurrences \\(P(+)\\) was one half, which is a decision we can revisit. But changing this value would probably require that we also change the threshold, and for this reason we need to optimize both hyperparameters at the same time. We present the results of a simple grid search in Figure 7.4.\nBased on these results, it appears that changing the value of the prior has very little impact on the best MCC we can achieve: the threshold is simply adjusted to reflect the fact that we assume occurrences to be increasingly likely. In this example, there is very little incentive for us to change the value of the prior, as it would have a very small effect on the overall performance of the model. For this reason, we will keep the previous model (\\(P(+) = 0.5\\) and \\(\\tau \\approx 0.43\\)) as the best one.\n\n\nA grid search in an exhaustive sweep of all possible combinations of parameter values. In order to make the process more efficient, refined approaches like successive halvings (Jamieson and Talwalkar 2016) can be used.\nJust because we decided to use a learning curve does not mean we have to change the hyper-parameters. Sometimes, this approach reveals that the value of an hyper-parameter is not really important to model performance, and we need to make a decision on what to do next. Here, although there are marginal changes in the value of the MCC, they do not feel significant enough to change the value of the prior.\n\n\n\n\n\n\n\n\nFigure 7.4: learning curve for the threshold and prior\n\n\n\n\n\n\n\n7.3.4 Testing and visualizing the final model\nAs we are now considering that our model is adequately trained, we can apply it to the testing data we had set aside early in Chapter 5. Applying the trained model to this data provides a fair estimate of the expected model performance, and relaying this information to people who are going to use the model is important.\nWe are not applying the older versions of the model to the testing data, as we had decided against this. We had established the rule of “we pick the best model as the one with the highest validation MCC”, and this is what we will stick to. To do otherwise would be the applied machine learning equivalent of \\(p\\)-hacking, as the question of “what to do in case a model with lower validation MCC had a better performance on the testing data?” would arise, and we do not want to start questioning our central decision this late in the process.\nWe can start by taking a look at the confusion matrix on the testing data:\n\\[\n\\begin{pmatrix}\n153 & 12 \\\\\n4 & 105\n\\end{pmatrix}\n\\]\nThis is very promising! There are far more predictions on the diagonal (258) than outside of it (16), which suggests an accurate classifier. The MCC of this model is 0.881, its true-skill statistic is 0.872, and its positive and negative predictive values are respectively 0.927 and 0.963. In other words: this model is extremely good. The values of PPV and NPV in particular are important to report: they tell us that when the model predicts a positive or negative outcome, it is expected to be correct more than 9 out of 10 times.\nThe final predictions are shown in Figure 7.5. Although the range map is very similar to the one we produced by the end of Chapter 6, the small addition of an optimized threshold leads to a model that is overall a little more accurate. Note that the uncertainty has a much nicer spatial structure when compared to our initial attempt (in Figure 5.4): there are combinations of environmental variables that make prediction more difficult, but they tend to be very spatially clustered.\n\n\n\n\n\n\n\n\nFigure 7.5: Predicted range of Sitta whiteheadi (left) and associated bootstrap uncertainty (right; see Chapter 5). This prediction was made using the final trained model, including variable selection, transformations, and thresholding of the probability. Darker pixels in the uncertainty map indicate a higher variance between bootstrap runs.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Hyper-parameters tuning</span>"
    ]
  },
  {
    "objectID": "chapters/learningcurves.html#conclusion",
    "href": "chapters/learningcurves.html#conclusion",
    "title": "7  Hyper-parameters tuning",
    "section": "7.4 Conclusion",
    "text": "7.4 Conclusion\nIn this chapter, we have refined a model by adopting a principled approach to establishing hyper-parameters. This resulted in a final trained model, which we applied to produce the final prediction of the distribution of Sitta whiteheadi. In Chapter 8, we will start asking “why”? Specifically, we will see a series of tools to evaluate why the model was making a specific prediction at a specific place, and look at the relationship between the importance of variables for model performance and for actual predictions.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Hyper-parameters tuning</span>"
    ]
  },
  {
    "objectID": "chapters/explanations.html",
    "href": "chapters/explanations.html",
    "title": "8  Explaining predictions",
    "section": "",
    "text": "In this chapter, we will\nnavigate the accuracy-explainability for public policy Bell et al. (2022)\nwhat is explainable differs between stakeholders Amarasinghe et al. (2023)\nbiodiversity need sustained model uptake Weiskopf et al. (2022)\nŠtrumbelj and Kononenko (2013) monte carlo approximation of shapley values\nWadoux, Saby, and Martin (2023) mapping of shapley values\nMesgaran, Cousens, and Webber (2014) mapping of most important covariates\nLundberg and Lee (2017) SHAP\ntransfo in model = we can still apply these techniques instead of asking “what does PC1 = 0.4 mean”\n\n\n\nReferences\n\n\nAmarasinghe, Kasun, Kit T. Rodolfa, Hemank Lamba, and Rayid Ghani. 2023. “Explainable Machine Learning for Public Policy: Use Cases, Gaps, and Research Directions.” Data & Policy 5. https://doi.org/10.1017/dap.2023.2.\n\n\nBell, Andrew, Ian Solano-Kamaiko, Oded Nov, and Julia Stoyanovich. 2022. “It’s Just Not That Simple: An Empirical Study of the Accuracy-Explainability Trade-Off in Machine Learning for Public Policy.” 2022 ACM Conference on Fairness, Accountability, and Transparency, June. https://doi.org/10.1145/3531146.3533090.\n\n\nLundberg, Scott M, and Su-In Lee. 2017. “A Unified Approach to Interpreting Model Predictions.” In Advances in Neural Information Processing Systems, edited by I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett. Vol. 30. Curran Associates, Inc. https://proceedings.neurips.cc/paper_files/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Paper.pdf.\n\n\nMesgaran, Mohsen B., Roger D. Cousens, and Bruce L. Webber. 2014. “Here Be Dragons: A Tool for Quantifying Novelty Due to Covariate Range and Correlation Change When Projecting Species Distribution Models.” Edited by Janet Franklin. Diversity and Distributions 20 (10): 1147–59. https://doi.org/10.1111/ddi.12209.\n\n\nŠtrumbelj, Erik, and Igor Kononenko. 2013. “Explaining Prediction Models and Individual Predictions with Feature Contributions.” Knowledge and Information Systems 41 (3): 647–65. https://doi.org/10.1007/s10115-013-0679-x.\n\n\nWadoux, Alexandre M. J.-C., Nicolas P. A. Saby, and Manuel P. Martin. 2023. “Shapley Values Reveal the Drivers of Soil Organic Carbon Stock Prediction.” SOIL 9 (1): 21–38. https://doi.org/10.5194/soil-9-21-2023.\n\n\nWeiskopf, Sarah R., Zuzana V. Harmáčková, Ciara G. Johnson, María Cecilia Londoño-Murcia, Brian W. Miller, Bonnie J. E. Myers, Laura Pereira, et al. 2022. “Increasing the Uptake of Ecological Model Results in Policy Decisions to Improve Biodiversity Outcomes.” Environmental Modelling & Software 149 (March): 105318. https://doi.org/10.1016/j.envsoft.2022.105318.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Explaining predictions</span>"
    ]
  }
]