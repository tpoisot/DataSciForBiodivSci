<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.386">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Machine Learning for Biodiversity Scientists - 3&nbsp; Gradient descent</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": true,
  "collapse-after": 1,
  "panel-placement": "start",
  "type": "overlay",
  "limit": 10,
  "keyboard-shortcut": [
    null
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar docked slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../chapters/gradientdescent.html"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Gradient descent</span></a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Machine Learning for Biodiversity Scientists</a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://github.com/tpoisot/MLBS" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="../Machine-Learning-for-Biodiversity-Scientists.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
    <div id="quarto-search" class="quarto-navigation-tool px-1" title="Search"></div>
</div>
    </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/clustering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Clustering</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/gradientdescent.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Gradient descent</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/crossvalidation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Cross-validation</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Supervised classification</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/variableselection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Selecting variables</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/learningcurves.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Tuning hyper-parameters</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/explanations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Explaining predictions</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendices/instructornotes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Instructor notes</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="-1">
    <h2 id="toc-title">Chapter overview</h2>
   
  <ul>
  <li><a href="#sec-gradientdescent-trainedmodel" id="toc-sec-gradientdescent-trainedmodel" class="nav-link active" data-scroll-target="#sec-gradientdescent-trainedmodel"><span class="header-section-number">3.1</span> A digression: what is a trained model?</a></li>
  <li><a href="#sec-gradientdescent-problem" id="toc-sec-gradientdescent-problem" class="nav-link" data-scroll-target="#sec-gradientdescent-problem"><span class="header-section-number">3.2</span> The problem: how many interactions in a food web?</a></li>
  <li><a href="#sec-gradientdescent-explanation" id="toc-sec-gradientdescent-explanation" class="nav-link" data-scroll-target="#sec-gradientdescent-explanation"><span class="header-section-number">3.3</span> Gradient descent</a>
  <ul class="collapse">
  <li><a href="#sec-gradientdescent-lossfunctions" id="toc-sec-gradientdescent-lossfunctions" class="nav-link" data-scroll-target="#sec-gradientdescent-lossfunctions"><span class="header-section-number">3.3.1</span> Defining the loss function</a></li>
  <li><a href="#sec-gradientdescent-gradient" id="toc-sec-gradientdescent-gradient" class="nav-link" data-scroll-target="#sec-gradientdescent-gradient"><span class="header-section-number">3.3.2</span> Calculating the gradient</a></li>
  <li><a href="#descending-the-gradient" id="toc-descending-the-gradient" class="nav-link" data-scroll-target="#descending-the-gradient"><span class="header-section-number">3.3.3</span> Descending the gradient</a></li>
  <li><a href="#sec-gradientdescent-learningrate" id="toc-sec-gradientdescent-learningrate" class="nav-link" data-scroll-target="#sec-gradientdescent-learningrate"><span class="header-section-number">3.3.4</span> A note on the learning rate</a></li>
  </ul></li>
  <li><a href="#sec-gradientdescent-application" id="toc-sec-gradientdescent-application" class="nav-link" data-scroll-target="#sec-gradientdescent-application"><span class="header-section-number">3.4</span> Application: how many links are in a food web?</a>
  <ul class="collapse">
  <li><a href="#the-things-we-wont-do" id="toc-the-things-we-wont-do" class="nav-link" data-scroll-target="#the-things-we-wont-do"><span class="header-section-number">3.4.1</span> The things we won’t do</a></li>
  <li><a href="#starting-the-learning-process" id="toc-starting-the-learning-process" class="nav-link" data-scroll-target="#starting-the-learning-process"><span class="header-section-number">3.4.2</span> Starting the learning process</a></li>
  <li><a href="#stopping-the-learning-process" id="toc-stopping-the-learning-process" class="nav-link" data-scroll-target="#stopping-the-learning-process"><span class="header-section-number">3.4.3</span> Stopping the learning process</a></li>
  <li><a href="#sec-gradientdescent-overfitting" id="toc-sec-gradientdescent-overfitting" class="nav-link" data-scroll-target="#sec-gradientdescent-overfitting"><span class="header-section-number">3.4.4</span> Detecting over-fitting</a></li>
  <li><a href="#visualizing-the-learning-process" id="toc-visualizing-the-learning-process" class="nav-link" data-scroll-target="#visualizing-the-learning-process"><span class="header-section-number">3.4.5</span> Visualizing the learning process</a></li>
  <li><a href="#outcome-of-the-model" id="toc-outcome-of-the-model" class="nav-link" data-scroll-target="#outcome-of-the-model"><span class="header-section-number">3.4.6</span> Outcome of the model</a></li>
  </ul></li>
  <li><a href="#a-note-on-regularization" id="toc-a-note-on-regularization" class="nav-link" data-scroll-target="#a-note-on-regularization"><span class="header-section-number">3.5</span> A note on regularization</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span class="header-section-number">3.6</span> Conclusion</a></li>
  <li><a href="#bibliography" id="toc-bibliography" class="nav-link" data-scroll-target="#bibliography">References</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/tpoisot/MLBS/blob/main/chapters/gradientdescent.qmd" class="toc-action"><i class="bi bi-github"></i>View source</a></li><li><a href="https://github.com/tpoisot/MLBS/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="sec-gradientdescent" class="quarto-section-identifier"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Gradient descent</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>As we progress into this book, the process of delivering a trained model is going to become more and more complex. In <a href="clustering.html" class="quarto-xref"><span>Chapter&nbsp;2</span></a>, we worked with a model that did not really require training (but did require to pick the best hyper-parameter). In this chapter, we will only increase complexity very slightly, by considering how we can train a model when we have a reference dataset to compare to.</p>
<p>Doing do will require to introduce several new concepts, and so the “correct” way to read this chapter is to focus on the high-level process. The problem we will try to solve (which is introduced in <a href="#sec-gradientdescent-problem" class="quarto-xref"><span>Section&nbsp;3.2</span></a>) is very simple; in fact, the empirical data looks more fake than many simulated datasets!</p>
<section id="sec-gradientdescent-trainedmodel" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="sec-gradientdescent-trainedmodel"><span class="header-section-number">3.1</span> A digression: what is a trained model?</h2>
<p>Models are data. When a model is trained, it represents a series of measurements (its parameters), taken on a representation of the natural world (the training data), through a specific instrument <span class="citation" data-cites="morrison1999">(the model itself, see <em>e.g.</em> <a href="#ref-morrison1999" role="doc-biblioref">Morrison and Morgan 1999</a>)</span>. A trained model is, therefore, capturing our understanding of a specific situation we encountered. We need to be very precise when defining what, exactly, a model describes. In fact, we need to take a step back and try to figure out where the model stops.</p>
<p>As we will see in this chapter, then in <a href="crossvalidation.html" class="quarto-xref"><span>Chapter&nbsp;4</span></a>, and finally in <a href="learningcurves.html" class="quarto-xref"><span>Chapter&nbsp;7</span></a>, the fact of training a model means that there is a back and forth between the algorithm we train, the data we use for training, and the criteria we set to define the performance of the trained model. The algorithm bound to its dataset is the <em>machine</em> we train in machine learning.</p>
<p>Therefore, a trained model is never independent from its training data: they describe the scope of the problem we want to address with this model. In <a href="clustering.html" class="quarto-xref"><span>Chapter&nbsp;2</span></a>, we ended up with a machine (the trained <em>k</em>-means algorithm) whose parameters (the centroids of the classes) made sense in the specific context of the training data we used; applied to a different dataset, there are no guarantees that our model would deliver useful information.</p>
<p>For the purpose of this book, we will consider that a model is trained when we have defined the algorithm, the data, the measure through which we will evaluate the model performance, and then measured the performance on a dataset built specifically for this task. All of these elements are important, as they give us the possibility to <em>explain</em> how we came up with the model, and therefore, how we made the predictions. This is different from reasoning about why the model is making a specific prediction (we will discuss this in <a href="explanations.html" class="quarto-xref"><span>Chapter&nbsp;8</span></a>), and is more related to explaining the process, the “outer core” of the model. As you read this chapter, pay attention to these elements: what algorithm are we using, on what data, how do we measure its performance, and how well does it perform?</p>
</section>
<section id="sec-gradientdescent-problem" class="level2 page-columns page-full" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="sec-gradientdescent-problem"><span class="header-section-number">3.2</span> The problem: how many interactions in a food web?</h2>
<p>One of the earliest observation that ecologists made about food webs is that when there are more species, there are more interactions. A remarkably insightful crowd, food web ecologists. Nevertheless, it turns out that this apparently simple question had received a few different answers over the years.</p>
<p>The initial model was proposed by <span class="citation" data-cites="cohen1984">Cohen and Briand (<a href="#ref-cohen1984" role="doc-biblioref">1984</a>)</span>: the number of interactions <span class="math inline">\(L\)</span> scales linearly with the number of species <span class="math inline">\(S\)</span>. After all, we can assume that when averaging over many consumers, there will be an average diversity of resources they consume, and so the number of interactions could be expressed as <span class="math inline">\(L \approx b\times S\)</span>.</p>
<p>Not so fast, said <span class="citation" data-cites="martinez1992">Martinez (<a href="#ref-martinez1992" role="doc-biblioref">1992</a>)</span>. When we start looking a food webs with more species, the increase of <span class="math inline">\(L\)</span> with regards to <span class="math inline">\(S\)</span> is superlinear. Thinking in ecological terms, maybe we can argue that consumers are flexible, and that instead of sampling a set number of resources, they will sample a set proportion of the number of consumer-resource combinations (of which there are <span class="math inline">\(S^2\)</span>). In this interpretation, <span class="math inline">\(L \approx b\times S^2\)</span>.</p>
<p>But the square term can be relaxed; and there is no reason not to assume a power law, with <span class="math inline">\(L\approx b\times S^a\)</span>. This last formulation has long been accepted as the most workable one, because it is possible to approximate values of its parameters using other ecological processes <span class="citation" data-cites="brose2004">(<a href="#ref-brose2004" role="doc-biblioref">Brose et al. 2004</a>)</span>.</p>
<p>The “reality” (<em>i.e.</em> the relationship between <span class="math inline">\(S\)</span> and <span class="math inline">\(L\)</span> that correctly accounts for ecological constraints, and fit the data as closely as possible) is a little bit different than this formula <span class="citation" data-cites="macdonald2020">(<a href="#ref-macdonald2020" role="doc-biblioref">MacDonald, Banville, and Poisot 2020</a>)</span>. But for the purpose of this chapter, figuring out the values of <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> from empirical data is a very instructive exercise.</p>
<p>In <a href="#fig-gradient-data" class="quarto-xref">Figure&nbsp;<span>3.1</span></a>, we can check that there is a linear relationship between the natural log of the number of species and the natural log of the number of links. This is not surprising! If we assume that <span class="math inline">\(L \approx b\times S^a\)</span>, then we can take the log of both sides, and we get <span class="math inline">\(\text{log}\, L \approx a \times \text{log}\, S + \text{log}\,b\)</span>. This is linear model, and so we can estimate its parameters using linear regression!</p>
<div id="cell-fig-gradient-data" class="cell page-columns page-full" data-execution_count="3">
<div class="cell-output cell-output-display page-columns page-full" data-execution_count="4">
<div id="fig-gradient-data" class="quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-gradient-data-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="gradientdescent_files/figure-html/fig-gradient-data-output-1.png" class="img-fluid figure-img">
</div>
<figcaption class="figure quarto-float-caption quarto-float-fig margin-caption" id="fig-gradient-data-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.1: We have assumed that the relationship between <span class="math inline">\(L\)</span> and <span class="math inline">\(S\)</span> could be represented by <span class="math inline">\(L \approx b\times S^a\)</span>, which gave us a reason to take the natural log of both variables. On this figure, we see that the relationship between the logs look linear, which means that linear regression has a good chance of estimating the values of the parameters.
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="sec-gradientdescent-explanation" class="level2 page-columns page-full" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="sec-gradientdescent-explanation"><span class="header-section-number">3.3</span> Gradient descent</h2>
<p>Gradient descent is built around a remarkably simple intuition: knowing the formula that gives rise to our prediction, and the value of the error we made for each point, we can take the derivative of the error with regards to each parameter, and this tells us how much this parameter contributed to the error. Because we are taking the derivative, we can futher know whether to increase, or decrease, the value of the parameter in order to make a smaller error next time.</p>
<p>In this section, we will use linear regression as an example, because it is the model we have decided to use when exploring our ecological problem in <a href="#sec-gradientdescent-problem" class="quarto-xref"><span>Section&nbsp;3.2</span></a>, and because it is suitably simple to keep track of everything when writing down the gradient by hand.</p>
<p>Before we start assembling the different pieces, we need to decide what our model is. We have settled on a linear model, which will have the form <span class="math inline">\(\hat y = m\times x + b\)</span>. The little hat on <span class="math inline">\(\hat y\)</span> indicates that this is a prediction. The input of this model is <span class="math inline">\(x\)</span>, and its parameters are <span class="math inline">\(m\)</span> (the slope) and <span class="math inline">\(b\)</span> (the intercept). Using the notation we adopted in <a href="#sec-gradientdescent-problem" class="quarto-xref"><span>Section&nbsp;3.2</span></a>, this would be <span class="math inline">\(\hat l = a \times s + b\)</span>, with <span class="math inline">\(l = \text{log} L\)</span> and <span class="math inline">\(s = \text{log} S\)</span>.</p>
<section id="sec-gradientdescent-lossfunctions" class="level3 page-columns page-full" data-number="3.3.1">
<h3 data-number="3.3.1" class="anchored" data-anchor-id="sec-gradientdescent-lossfunctions"><span class="header-section-number">3.3.1</span> Defining the loss function</h3>
<p>The loss function is an important concept for anyone attempting to compare predictions to outcomes: it quantifies how far away an ensemble of predictions is from a benchmark of known cases. There are many loss functions we can use, and we will indeed use a few different ones in this book. But for now, we will start with a very general understanding of what these functions <em>do</em>.</p>
<p>Think of prediction as throwing a series of ten darts on ten different boards. In this case, we know what the correct outcome is (the center of the board, I assume, although I can be mistaken since I have only played darts once, and lost). A cost function would be any mathematical function that compares the position of each dart on each board, the position of the correct event, and returns a score that informs us about how poorly our prediction lines up with the reality.</p>
<p>In the above example, you may be tempted to say that we can take the Euclidean distance of each dart to the center of each board, in order to know, for each point, how far away we landed. Because there are several boards, and because we may want to vary the number of boards while still retaining the ability to compare our performances, we would then take the average of these measures.</p>
<p>We will note the position of our dart as being <span class="math inline">\(\hat y\)</span>, the position of the center as being <span class="math inline">\(y\)</span> (we will call this the <em>ground truth</em>), and the number of attempts <span class="math inline">\(n\)</span>, and so we can write our loss function as</p>
<p><span id="eq-loss-mse"><span class="math display">\[
\frac{1}{n}\sum_{i=1}^{n}(y_i - \hat y_i)^2
\tag{3.1}\]</span></span></p>

<div class="no-row-height column-margin column-container"><div class="">
<p>In data science, things often have multiple names. This is true of loss functions, and this will be even more true on other things later.</p>
</div></div><p>This loss function is usually called the MSE (Mean Standard Error), or L2 loss, or the quadratic loss, because the paths to machine learning terminology are many. This is a good example of a loss function for regression (and we will discuss loss functions for classification later in this book). There are alternative loss functions to use for regression problems in <a href="#tbl-gradientdescent-regressionloss" class="quarto-xref">Table&nbsp;<span>3.1</span></a>.</p>
<div id="tbl-gradientdescent-regressionloss" class="anchored" data-tbl-colwidths="[25,25,50]">
<figure class="quarto-float quarto-float-tbl figure">
<div aria-describedby="tbl-gradientdescent-regressionloss-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table">
<colgroup>
<col style="width: 47%">
<col style="width: 26%">
<col style="width: 26%">
</colgroup>
<thead>
<tr class="header">
<th>Measure</th>
<th>Expression</th>
<th>Remarks</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Mean Squared Error (MSE, L2)</td>
<td><span class="math inline">\(\frac{1}{n}\sum_{i=1}^{n}\left(y_i - \hat y_i\right)^2\)</span></td>
<td>Large errors are (proportionally) more penalized because of the squaring</td>
</tr>
<tr class="even">
<td>Mean Absolute Error (MAE, L1)</td>
<td><span class="math inline">\(\frac{1}{n}\sum_{i=1}^{n}\|y_i - \hat y_i\|\)</span></td>
<td>Error measured in the units of the response variable</td>
</tr>
<tr class="odd">
<td>Root Mean Square Error (RMSE)</td>
<td><span class="math inline">\(\sqrt{\text{MSE}}\)</span></td>
<td>Error measured in the units of the response variable</td>
</tr>
<tr class="even">
<td>Mean Bias Error</td>
<td><span class="math inline">\(\frac{1}{n}\sum_{i=1}^{n}\left(y_i - \hat y_i\right)\)</span></td>
<td>Errors <em>can</em> cancel out, but this can be used as a measure of positive/negative bias</td>
</tr>
</tbody>
</table>
</div>
<figcaption class="table quarto-float-caption quarto-float-tbl" id="tbl-gradientdescent-regressionloss-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;3.1: List of common loss functions for regression problems {tbl-colwidths='[25,25,50]'}
</figcaption>
</figure>
</div>
<p>Throughout this chapter, we will use the L2 loss (<a href="#eq-loss-mse" class="quarto-xref">Equation&nbsp;<span>3.1</span></a>), because it has <em>really</em> nice properties when it comes to taking derivatives, which we will do a lot of. In the case of a linear model, we can rewrite <a href="#eq-loss-mse" class="quarto-xref">Equation&nbsp;<span>3.1</span></a> as</p>
<p><span id="eq-loss-withmodel"><span class="math display">\[
f = \frac{1}{n}\sum\left(y_i - m\times x_i - b\right)^2
\tag{3.2}\]</span></span></p>
<p>There is an important change in <a href="#eq-loss-withmodel" class="quarto-xref">Equation&nbsp;<span>3.2</span></a>: we have replaced the prediction <span class="math inline">\(\hat y_i\)</span> with a term that is a function of the predictor <span class="math inline">\(x_i\)</span> and the model parameters: this means that we can calculate the value of the loss as a function of a pair of values <span class="math inline">\((x_i, y_i)\)</span>, and the model parameters.</p>
</section>
<section id="sec-gradientdescent-gradient" class="level3" data-number="3.3.2">
<h3 data-number="3.3.2" class="anchored" data-anchor-id="sec-gradientdescent-gradient"><span class="header-section-number">3.3.2</span> Calculating the gradient</h3>
<p>With the loss function corresponding to our problem in hands (<a href="#eq-loss-withmodel" class="quarto-xref">Equation&nbsp;<span>3.2</span></a>), we can calculate the gradient. Given a function that is scalar-valued (it returns a single value), taking several variables, that is differentiable, the gradient of this function is a vector-valued (it returns a vector) function; when evaluated at a specific point, this vectors indicates both the direction and the rate of fastest increase, which is to say the direction in which the function increases away from the point, and how fast it moves.</p>
<p>We can re-state this definition using the terms of the problem we want to solve. At a point <span class="math inline">\(p = [m\quad b]^\top\)</span>, the gradient <span class="math inline">\(\nabla f\)</span> of <span class="math inline">\(f\)</span> is given by:</p>
<p><span id="eq-gradientdescent-gradientfull"><span class="math display">\[
\nabla f\left(
p
\right) =
\begin{bmatrix}
\frac{\partial f}{\partial m}(p) \\
\frac{\partial f}{\partial b}(p)
\end{bmatrix}\,.
\tag{3.3}\]</span></span></p>
<p>This indicates how changes in <span class="math inline">\(m\)</span> and <span class="math inline">\(b\)</span> will <em>increase</em> the error. In order to have a more explicit formulation, all we have to do is figure out an expression for both of the partial derivatives. In practice, we can let auto-differentiation software calculate the gradient for us <span class="citation" data-cites="innes2018">(<a href="#ref-innes2018" role="doc-biblioref">Innes 2018</a>)</span>; these packages are now advanced enough that they can take the gradient of code directly.</p>
<p>Solving <span class="math inline">\((\partial f / \partial m)(p)\)</span> and <span class="math inline">\((\partial f / \partial c)(p)\)</span> is easy enough:</p>
<p><span id="eq-gradientdescent-gradientexplicit"><span class="math display">\[
\nabla f\left(
p
\right) =
\begin{bmatrix}
-\frac{2}{n}\sum \left[x_i \times (y_i - m\times x_i - b)\right] \\
-\frac{2}{n}\sum \left(y_i - m\times x_i - b\right)
\end{bmatrix}\,.
\tag{3.4}\]</span></span></p>
<p>Note that both of these partial derivatives have a term in <span class="math inline">\(2n^{-1}\)</span>. Getting rid of the <span class="math inline">\(2\)</span> in front is very straightforward! We can modify <a href="#eq-loss-withmodel" class="quarto-xref">Equation&nbsp;<span>3.2</span></a> to divide by <span class="math inline">\(2n\)</span> instead of <span class="math inline">\(n\)</span>. This modified loss function retains the important characteristics: it increases when the prediction gets worse, and it allows comparing the loss with different numbers of points. As with many steps in the model training process, it is important to think about <em>why</em> we are doing certain things, as this can enable us to make some slight changes to facilitate the analysis.</p>
<p>With the gradient written down in <a href="#eq-gradientdescent-gradientexplicit" class="quarto-xref">Equation&nbsp;<span>3.4</span></a>, we can now think about what it means to <em>descend</em> the gradient.</p>
</section>
<section id="descending-the-gradient" class="level3" data-number="3.3.3">
<h3 data-number="3.3.3" class="anchored" data-anchor-id="descending-the-gradient"><span class="header-section-number">3.3.3</span> Descending the gradient</h3>
<p>Recall from <a href="#sec-gradientdescent-gradient" class="quarto-xref"><span>Section&nbsp;3.3.2</span></a> that the gradient measures how far we <em>increase</em> the function of which we are taking the gradient. Therefore, it measures how much each parameter contributes to the loss value. Our working definition for a trained model is “one that has little loss”, and so in an ideal world, we could find a point <span class="math inline">\(p\)</span> for which the gradient is as small as feasible.</p>
<p>Because the gradient measures how far away we increase error, and intuitive way to use it is to take steps in the <em>opposite</em> direction. In other words, we can update the value of our parameters using <span class="math inline">\(p := p - \nabla f(p)\)</span>, meaning that we subtract from the parameter values their contribution to the overall error in the predictions.</p>
<p>But, as we will discuss further in <a href="#sec-gradientdescent-learningrate" class="quarto-xref"><span>Section&nbsp;3.3.4</span></a>, there is such a thing as “too much learning”. For this reason, we will usually not move the entire way, and introduce a term to regulate how much of the way we actually want to descend the gradient. Our actual scheme to update the parameters is</p>
<p><span id="eq-gradientdescent-loop"><span class="math display">\[
p := p - \eta\times \nabla f(p) \,.
\tag{3.5}\]</span></span></p>
<p>This formula can be <em>iterated</em>: with each successive iteration, it will get us closer to the optimal value of <span class="math inline">\(p\)</span>, which is to say the combination of <span class="math inline">\(m\)</span> and <span class="math inline">\(b\)</span> that minimizes the loss.</p>
</section>
<section id="sec-gradientdescent-learningrate" class="level3" data-number="3.3.4">
<h3 data-number="3.3.4" class="anchored" data-anchor-id="sec-gradientdescent-learningrate"><span class="header-section-number">3.3.4</span> A note on the learning rate</h3>
<p>The error we can make on the first iteration will depend on the value of our initial pick of parameters. If we are <em>way off</em>, especially if we did not re-scale our predictors and responses, this error can get very large. And if we make a very large error, we will have a very large gradient, and we will end up making very big steps when we update the parameter values. There is a real risk to end up over-compensating, and correcting the parameters too much.</p>
<p>In order to protect against this, in reality, we update the gradient only a little, where the value of “a little” is determined by an hyper-parameter called the <em>learning rate</em>, which we noted <span class="math inline">\(\eta\)</span>. This value will be very small (much less than one). Picking the correct learning rate is not simply a way to ensure that we get correct results (though that is always a nice bonus), but can be a way to ensure that we get results <em>at all</em>. The representation of numbers in a computer’s memory is tricky, and it is possible to create an overflow: a number so large it does not fit within 64 (or 32, or 16, or however many we are using) bits of memory.</p>
<p>The conservative solution of using the smallest possible learning rate is not really effective, either. If we almost do not update our parameters at every epoch, then we will take almost forever to converge on the correct parameters. Figuring out the learning rate is an example of hyper-parameter tuning, which we will get back to later in this book.</p>
</section>
</section>
<section id="sec-gradientdescent-application" class="level2 page-columns page-full" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="sec-gradientdescent-application"><span class="header-section-number">3.4</span> Application: how many links are in a food web?</h2>
<p>We will not get back to the problem exposed in <a href="#fig-gradient-data" class="quarto-xref">Figure&nbsp;<span>3.1</span></a>, and use gradient descent to fit the parameters of the model defined as <span class="math inline">\(\hat y \approx \beta_0 + \beta_1 \times x\)</span>, where, using the notation introduced in <a href="#sec-gradientdescent-problem" class="quarto-xref"><span>Section&nbsp;3.2</span></a>, <span class="math inline">\(\hat y\)</span> is the natural log of the number of interactions (what we want to predict), <span class="math inline">\(x\)</span> is the natural log of the species richness (our predictor), and <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are the parameters of the model.</p>
<section id="the-things-we-wont-do" class="level3" data-number="3.4.1">
<h3 data-number="3.4.1" class="anchored" data-anchor-id="the-things-we-wont-do"><span class="header-section-number">3.4.1</span> The things we won’t do</h3>
<p>At this point, we could decide that it is a good idea to transform our predictor and our response, for example using the z-score. But this is not really required here; we know that our model will give results that make sense in the units of species and interactions (after dealing with the natural log, of course). In addition, as we will see in <a href="variableselection.html#sec-leakage" class="quarto-xref"><span>Section&nbsp;6.2</span></a>, applying a transformation to the data too soon can be a dangerous thing. We will have to live with raw features for a few more chapters.</p>
<p>In order to get a sense of the performance of our model, we will remove some of the data, meaning that the model will not learn on these data points. We will get back to this practice (cross-validation) in a lot more details in <a href="crossvalidation.html" class="quarto-xref"><span>Chapter&nbsp;4</span></a>, but for now it is enough to say that we hide 20% of the dataset, and we will use them to evaluate how good the model is as it trains. The point of this chapter is not to think too deeply about cross-validation, but simply to develop intuitions about the way a machine learns.</p>
</section>
<section id="starting-the-learning-process" class="level3" data-number="3.4.2">
<h3 data-number="3.4.2" class="anchored" data-anchor-id="starting-the-learning-process"><span class="header-section-number">3.4.2</span> Starting the learning process</h3>
<p>In order to start the gradient descent process, we need to decide on an initial value of the parameters. There are many ways to do it. We could work our way from our knowledge of the system; for example <span class="math inline">\(b &lt; 1\)</span> and <span class="math inline">\(a = 2\)</span> would fit relatively well with early results in the food web literature. Or we could draw a pair of values <span class="math inline">\((a, b)\)</span> at random. Looking at <a href="#fig-gradient-data" class="quarto-xref">Figure&nbsp;<span>3.1</span></a>, it is clear that our problem is remarkably simple, and so presumably either solution would work.</p>
</section>
<section id="stopping-the-learning-process" class="level3 page-columns page-full" data-number="3.4.3">
<h3 data-number="3.4.3" class="anchored" data-anchor-id="stopping-the-learning-process"><span class="header-section-number">3.4.3</span> Stopping the learning process</h3>
<p>The gradient descent algorithm is entirely contained in <a href="#eq-gradientdescent-loop" class="quarto-xref">Equation&nbsp;<span>3.5</span></a> , and so we only need to iterate several times to optimize the parameters. How long we need to run the algorithm for depends on a variety of factors, including our learning rate (slow learning requires more time!), our constraints in terms of computing time, but also how good we need to model to be.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>The number of iterations over which we train the model is usually called the number of epochs, and is an hyper-parameter of the model.</p>
</div></div><p>One usual approach is to decide on a number of iterations (we need to start somewhere), and to check how rapidly the model seems to settle on a series of parameters. But more than this, we also need to ensure that our model is not learning <em>too much</em> from the data. This would result in over-fitting, in which the models gets better on the data we used to train it, and worse on the data we kept hidden from the training! In <a href="#tbl-gradient-attempt-one" class="quarto-xref">Table&nbsp;<span>3.2</span></a>, we present the RMSE loss for the training and testing datasets, as well as the current estimates of the values of the parameters of the linear model.</p>
<div class="cell" data-execution_count="7">
<div id="tbl-gradient-attempt-one" class="cell anchored" data-execution_count="7">
<figure class="quarto-float quarto-float-tbl figure">
<div aria-describedby="tbl-gradient-attempt-one-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output cell-output-display">
<table data-quarto-postprocess="true" class="table table-sm table-striped small">
<thead>
<tr class="header headerLastRow">
<th style="text-align: right;" data-quarto-table-cell-role="th">Step</th>
<th style="text-align: right;" data-quarto-table-cell-role="th">Loss (training)</th>
<th style="text-align: right;" data-quarto-table-cell-role="th">Loss (testing)</th>
<th style="text-align: right;" data-quarto-table-cell-role="th">β₀</th>
<th style="text-align: right;" data-quarto-table-cell-role="th">β₁</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">1</td>
<td style="text-align: right;">3.92114</td>
<td style="text-align: right;">3.18785</td>
<td style="text-align: right;">0.4</td>
<td style="text-align: right;">0.2</td>
</tr>
<tr class="even">
<td style="text-align: right;">10</td>
<td style="text-align: right;">2.99934</td>
<td style="text-align: right;">2.3914</td>
<td style="text-align: right;">0.487395</td>
<td style="text-align: right;">0.226696</td>
</tr>
<tr class="odd">
<td style="text-align: right;">30</td>
<td style="text-align: right;">1.72775</td>
<td style="text-align: right;">1.31211</td>
<td style="text-align: right;">0.640263</td>
<td style="text-align: right;">0.271814</td>
</tr>
<tr class="even">
<td style="text-align: right;">100</td>
<td style="text-align: right;">0.536207</td>
<td style="text-align: right;">0.373075</td>
<td style="text-align: right;">0.907004</td>
<td style="text-align: right;">0.337644</td>
</tr>
<tr class="odd">
<td style="text-align: right;">300</td>
<td style="text-align: right;">0.392477</td>
<td style="text-align: right;">0.308264</td>
<td style="text-align: right;">1.03855</td>
<td style="text-align: right;">0.311011</td>
</tr>
<tr class="even">
<td style="text-align: right;">1000</td>
<td style="text-align: right;">0.326848</td>
<td style="text-align: right;">0.253939</td>
<td style="text-align: right;">1.11195</td>
<td style="text-align: right;">0.110083</td>
</tr>
<tr class="odd">
<td style="text-align: right;">3000</td>
<td style="text-align: right;">0.225623</td>
<td style="text-align: right;">0.167897</td>
<td style="text-align: right;">1.25704</td>
<td style="text-align: right;">-0.311373</td>
</tr>
<tr class="even">
<td style="text-align: right;">10000</td>
<td style="text-align: right;">0.164974</td>
<td style="text-align: right;">0.119597</td>
<td style="text-align: right;">1.4487</td>
<td style="text-align: right;">-0.868105</td>
</tr>
<tr class="odd">
<td style="text-align: right;">20000</td>
<td style="text-align: right;">0.162808</td>
<td style="text-align: right;">0.118899</td>
<td style="text-align: right;">1.48864</td>
<td style="text-align: right;">-0.984121</td>
</tr>
</tbody>
</table>
</div>
</div>
<figcaption class="table quarto-float-caption quarto-float-tbl" id="tbl-gradient-attempt-one-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;3.2: This table shows the change in the model, as measured by the loss and by the estimates of the parameters, after an increasing amount of training epochs. The loss drops sharply in the first 500 iterations, but even after 20000 iterations, there are still some changes in the values of the parameters.
</figcaption>
</figure>
</div>
</div>
<p>In order to protect against over-fitting, it is common to add a check to the training loop, to say that after a minimum number of iterations has been done, we stop the training when the loss on the testing data starts increasing. In order to protect against very long training steps, it is also common to set a tolerance (absolute or relative) under which we decide that improvements to the loss are not meaningful, and which serves as a stopping criterion for the training.</p>
</section>
<section id="sec-gradientdescent-overfitting" class="level3 page-columns page-full" data-number="3.4.4">
<h3 data-number="3.4.4" class="anchored" data-anchor-id="sec-gradientdescent-overfitting"><span class="header-section-number">3.4.4</span> Detecting over-fitting</h3>
<p>As we mentioned in the previous section, one risk with training that runs for too long is to start seeing over-fitting. The usual diagnosis for over-fitting is an increase in the testing loss, which is to say, in the loss measured on the data that were not used for training. In <a href="#fig-gradient-loss-comparison" class="quarto-xref">Figure&nbsp;<span>3.2</span></a>, we can see that the RMSE loss decreases at the same rate on both datasets, which indicates that the model is learning from the data, but not to a point where its ability to generalize suffers.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>Underfitting is also a possible scenario, where the model is <em>not</em> learning from the data, and can be detected by seeing the loss measures remain high or even increase.</p>
</div></div><div id="cell-fig-gradient-loss-comparison" class="cell page-columns page-full" data-execution_count="8">
<div class="cell-output cell-output-display page-columns page-full" data-execution_count="9">
<div id="fig-gradient-loss-comparison" class="quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-gradient-loss-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="gradientdescent_files/figure-html/fig-gradient-loss-comparison-output-1.png" class="img-fluid figure-img">
</div>
<figcaption class="figure quarto-float-caption quarto-float-fig margin-caption" id="fig-gradient-loss-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.2: This figures shows the change in the loss for the training and testing dataset. As the two curves converge on low values at the same rate, this suggests that the model is not over-fitting, and is therefore suitable for use.
</figcaption>
</figure>
</div>
</div>
</div>
<p>We are producing the loss over time figure after the training, as it is good practice – but as we mentioned in the previous section, it is very common to have the training code look at the dynamics of these two values in order to decide whether to stop the training early.</p>
<p>Before moving forward, let’s look at <a href="#fig-gradient-loss-comparison" class="quarto-xref">Figure&nbsp;<span>3.2</span></a> a little more closely. In the first steps, the loss decreases very rapidly – this is because we started from a value of <span class="math inline">\(\mathbf{\beta}\)</span> that is, presumably, far away from the optimum, and therefore the gradient is really strong. Despite the low learning rate, we are making long steps in the space of parameters. After this initial rapid increase, the loss decreases much more slowly. This, counter-intuitively, indicates that we are getting closer to the optimum! At the exact point where <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> optimally describe our dataset, the gradient vanishes, and our system would stop moving. And as we get closer and closer to this point, we are slowing down. In the next section, we will see how the change in loss over times ties into the changes with the optimal parameter values.</p>
</section>
<section id="visualizing-the-learning-process" class="level3 page-columns page-full" data-number="3.4.5">
<h3 data-number="3.4.5" class="anchored" data-anchor-id="visualizing-the-learning-process"><span class="header-section-number">3.4.5</span> Visualizing the learning process</h3>
<p>From <a href="#fig-gradient-param-change" class="quarto-xref">Figure&nbsp;<span>3.3</span></a>, we can see the change in <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>, as well as the movement of the current best estimate of the parameters (right panel). The sharp decrease in loss early in the training is specifically associated to a rapid change in the value of <span class="math inline">\(\beta_0\)</span>. Further note that the change in parameters values is <em>not</em> monotonous! The value of <span class="math inline">\(\beta_1\)</span> initially increases, but when <span class="math inline">\(\beta_0\)</span> gets closer to the optimum, the gradient indicates that we have been moving <span class="math inline">\(\beta_1\)</span> in the “wrong” direction.</p>
<div id="cell-fig-gradient-param-change" class="cell page-columns page-full" data-execution_count="10">
<div class="cell-output cell-output-display page-columns page-full" data-execution_count="11">
<div id="fig-gradient-param-change" class="quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-gradient-param-change-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="gradientdescent_files/figure-html/fig-gradient-param-change-output-1.png" class="img-fluid figure-img">
</div>
<figcaption class="figure quarto-float-caption quarto-float-fig margin-caption" id="fig-gradient-param-change-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.3: This figure shows the change in the parameters values over time. Note that the change is very large initially, because we make large steps when the gradient is strong. The rate of change gets much lower as we get nearer to the “correct” value.
</figcaption>
</figure>
</div>
</div>
</div>
<p>This is what gives rise to the “elbow” shape in the right panel of <a href="#fig-gradient-param-change" class="quarto-xref">Figure&nbsp;<span>3.3</span></a>. Remember that the gradient descent algorithm, in its simple formulation, assumes that we can <em>never</em> climb back up, <em>i.e.</em> we never accept a costly move. The trajectory of the parameters therefore represents the path that brings them to the lowest point they can reach <em>without</em> having to temporarily recommend a worse solution.</p>
<p>But how good is the solution we have reached?</p>
</section>
<section id="outcome-of-the-model" class="level3 page-columns page-full" data-number="3.4.6">
<h3 data-number="3.4.6" class="anchored" data-anchor-id="outcome-of-the-model"><span class="header-section-number">3.4.6</span> Outcome of the model</h3>
<p>We could read the performance of the model using the data in <a href="#fig-gradient-loss-comparison" class="quarto-xref">Figure&nbsp;<span>3.2</span></a>, but what we <em>really</em> care about is the model’s ability to tell us something about the data we initially gave it. This is presented in <a href="#fig-gradient-fitted" class="quarto-xref">Figure&nbsp;<span>3.4</span></a>. As we can see, the model is doing a rather good job at capturing the relationship between the number of species and the number of interactions.</p>
<div id="cell-fig-gradient-fitted" class="cell page-columns page-full" data-execution_count="11">
<div class="cell-output cell-output-display page-columns page-full" data-execution_count="12">
<div id="fig-gradient-fitted" class="quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-gradient-fitted-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="gradientdescent_files/figure-html/fig-gradient-fitted-output-1.png" class="img-fluid figure-img">
</div>
<figcaption class="figure quarto-float-caption quarto-float-fig margin-caption" id="fig-gradient-fitted-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.4: Overview of the fitted model. The residuals (top panel) are mostly centered around 0, which suggests little bias towards over/under predicting interactions. The red line (based on the optimal coefficients) goes through the points, and indicates a rather good fit of the model.
</figcaption>
</figure>
</div>
</div>
</div>
<p>We will have a far more nuanced discussion of “what is this model good for?” in <a href="crossvalidation.html" class="quarto-xref"><span>Chapter&nbsp;4</span></a>, but for now, we can make a decision about this model: it provides a good approximation of the relationship between the species richness, and the number of interactions, in a food web.</p>
</section>
</section>
<section id="a-note-on-regularization" class="level2" data-number="3.5">
<h2 data-number="3.5" class="anchored" data-anchor-id="a-note-on-regularization"><span class="header-section-number">3.5</span> A note on regularization</h2>
<p>One delicate issue that we have avoided in this chapter is the absolute value of the parameters. In other words, we didn’t really care about how large the model parameters would be, only the quality of the fit. This is (generally) safe to do in a model with a single parameter. But what if we had many different terms? What if, for example, we had a linear model of the form <span class="math inline">\(\hat y \approx \beta_0 + \beta_1 x + \beta_2 x^2\)</span>? What if our model was of the form <span class="math inline">\(\hat y \approx \beta_0 + \beta_1 x + \dots + \beta_n x^n\)</span>? What if <span class="math inline">\(n\)</span> started to get very large compared to the number of data points?</p>
<p>In this situation, we would very likely see overfitting, wherein the model would use the polynomial terms we provided to capture more and more noise in the data. This would be a dangerous situation, as the model will lose its ability to work on unknown data!</p>
<p>To prevent this situation, we may need to use regularization. Thanfkully, regularization is a relatively simple process. In <a href="#eq-gradientdescent-gradientexplicit" class="quarto-xref">Equation&nbsp;<span>3.4</span></a>, the function <span class="math inline">\(f(p)\)</span> we used to measure the gradient was the loss function directly. In regularization, we use a slight variation on this, where</p>
<p><span class="math display">\[
f(p) = \text{loss} + \lambda \times g(\beta) \,,
\]</span></p>
<p>where <span class="math inline">\(\lambda\)</span> is an hyper-parameter giving the strength of the regularization, and <span class="math inline">\(g(\beta)\)</span> is a function to calculate the total penalty of a set of parameters.</p>
<p>When using <span class="math inline">\(L1\)</span> regularization (LASSO regression), <span class="math inline">\(g(\beta) = \sum |\beta|\)</span>, and when using <span class="math inline">\(L2\)</span> regularization (ridge regression), <span class="math inline">\(g(\beta) = \sum \beta^2\)</span>. When this gets larger, which happens when the absolute value of the parameters increases, the model is penalized. Note that if <span class="math inline">\(\lambda = 0\)</span>, we are back to the initial formulation of the gradient, where the parameters have no direct effect on the cost.</p>
</section>
<section id="conclusion" class="level2" data-number="3.6">
<h2 data-number="3.6" class="anchored" data-anchor-id="conclusion"><span class="header-section-number">3.6</span> Conclusion</h2>
<p>In this chapter, we have used a dataset of species richness and number of interactions to start exploring the practice of machine learning. We defined a model (a linear regression), and based about assumptions about how to get closer to ideal parameters, we used the technique of gradient descent to estimate the best possible relationship between <span class="math inline">\(S\)</span> and <span class="math inline">\(L\)</span>. In order to provide a fair evaluation of the performance of this model, we kept a part of the dataset hidden from it while training. In <a href="crossvalidation.html" class="quarto-xref"><span>Chapter&nbsp;4</span></a>, we will explore this last point in great depth, by introducing the concept of cross-validation, testing set, and performance evaluation.</p>


</section>
<section id="bibliography" class="level1 unnumbered">
<h1 class="unnumbered">References</h1>
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-brose2004" class="csl-entry" role="listitem">
Brose, Ulrich, Annette Ostling, Kateri Harrison, and Neo D. Martinez. 2004. <span>“Unified Spatial Scaling of Species and Their Trophic Interactions.”</span> <em>Nature</em> 428 (6979): 167–71. <a href="https://doi.org/10.1038/nature02297">https://doi.org/10.1038/nature02297</a>.
</div>
<div id="ref-cohen1984" class="csl-entry" role="listitem">
Cohen, J. E., and F. Briand. 1984. <span>“Trophic Links of Community Food Webs.”</span> <em>Proc Natl Acad Sci U S A</em> 81 (13): 4105–9.
</div>
<div id="ref-innes2018" class="csl-entry" role="listitem">
Innes, Michael. 2018. <span>“Don’t Unroll Adjoint: Differentiating SSA-Form Programs.”</span> <a href="https://doi.org/10.48550/ARXIV.1810.07951">https://doi.org/10.48550/ARXIV.1810.07951</a>.
</div>
<div id="ref-macdonald2020" class="csl-entry" role="listitem">
MacDonald, Arthur Andrew Meahan, Francis Banville, and Timothée Poisot. 2020. <span>“Revisiting the Links-Species Scaling Relationship in Food Webs.”</span> <em>Patterns</em> 1 (0). <a href="https://doi.org/10.1016/j.patter.2020.100079">https://doi.org/10.1016/j.patter.2020.100079</a>.
</div>
<div id="ref-martinez1992" class="csl-entry" role="listitem">
Martinez, Néo D. 1992. <span>“Constant Connectance in Community Food Webs.”</span> <em>The American Naturalist</em> 139 (6): 1208–18. <a href="http://www.jstor.org/stable/2462337">http://www.jstor.org/stable/2462337</a>.
</div>
<div id="ref-morrison1999" class="csl-entry" role="listitem">
Morrison, Margaret, and Mary S. Morgan. 1999. <span>“Models as Mediating Instruments.”</span> In, 10–37. Cambridge University Press. <a href="https://doi.org/10.1017/cbo9780511660108.003">https://doi.org/10.1017/cbo9780511660108.003</a>.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    const typesetMath = (el) => {
      if (window.MathJax) {
        // MathJax Typeset
        window.MathJax.typeset([el]);
      } else if (window.katex) {
        // KaTeX Render
        var mathElements = el.getElementsByClassName("math");
        var macros = [];
        for (var i = 0; i < mathElements.length; i++) {
          var texText = mathElements[i].firstChild;
          if (mathElements[i].tagName == "SPAN") {
            window.katex.render(texText.data, mathElements[i], {
              displayMode: mathElements[i].classList.contains('display'),
              throwOnError: false,
              macros: macros,
              fleqn: false
            });
          }
        }
      }
    }
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        for (let i = 0; i < 2; i++) {
          container.appendChild(note.children[i].cloneNode(true));
        }
        typesetMath(container);
        return container.innerHTML
      } else {
        typesetMath(note);
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      typesetMath(note);
      return note.innerHTML;
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p>This work is licensed under the <a href="https://creativecommons.org/licenses/by-nc/4.0/">CC BY-NC 4.0 International License</a></p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>