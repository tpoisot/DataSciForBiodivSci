# Supp. Mat. - variable selection

Again, this is largely the same code as the classification notebook

```{julia}
_code_path = joinpath(@__DIR__, "..", "lib")
include(joinpath(_code_path, "pkg.jl"))
include(joinpath(_code_path, "confusiontable.jl"))
include(joinpath(_code_path, "nbc.jl"))
```

We load the data and the model

```{julia}
_ptm_path = joinpath(@__DIR__, "..", "ptm")
modelpath = joinpath(_ptm_path, "reindeer-initial.jld")
ptm = JLD.load(modelpath)
```

```{julia}
trainlabels, trainfeatures = ptm["training"]
testlabels, testfeatures = ptm["testing"]
```

we can prepare our folds now

```{julia}
folds = kfold(trainlabels, trainfeatures; k=15, permute=true)
```

function to validate with a subset of variables

```{julia}
function confusion_for_fold(y, X, fold; vars=1:19)
    t_idx, v_idx = fold
    ty = y[t_idx]
    vy = y[v_idx]
    tX = X[t_idx,vars]
    vX = X[v_idx,vars]
    
    predictor = naivebayes(ty, tX)
    
    prediction = vec(mapslices(predictor, tX, dims=2))
    validation = vec(mapslices(predictor, vX, dims=2))

    tc = ConfusionMatrix(prediction, ty)
    vc = ConfusionMatrix(validation, vy)
    return tc, vc
end
```

we prepare a dataframe to put the values in

```{julia}
varsel = DataFrame()
```

we run it bois

```{julia}
possiblevariables = collect(axes(trainfeatures, 2))
variableset = eltype(possiblevariables)[]

current_best = -Inf

while (!isempty(possiblevariables))
    scores = zeros(Float64, length(possiblevariables))
    for (i,pv) in enumerate(possiblevariables)
        tv = push!(copy(variableset), pv)
        C = [last(confusion_for_fold(trainlabels, trainfeatures, fold; vars=tv)) for fold in folds]
        scores[i] = mean(mcc.(C))
    end
    bs, bi = findmax(scores)
    if bs > current_best
        current_best = bs
        push!(variableset, possiblevariables[bi])
        deleteat!(possiblevariables, bi)
    else
        break
    end
    @info "Current set: $(variableset) - MCC: $(current_best)"
end

fwvar = copy(variableset)

# Retrain
C = [last(confusion_for_fold(trainlabels, trainfeatures, fold; vars=variableset)) for fold in folds]
_tss = mean(trueskill.(C))
_acc = mean(accuracy.(C))
_ppv = mean(ppv.(C))
_npv = mean(npv.(C))
_mcc = mean(mcc.(C))
push!(varsel, 
    (Method = :Forwards, N = length(variableset), Accuracy = _acc, TSS = _tss, NPV = _npv, PPV = _ppv, MCC = _mcc)
)

@info variableset
```

same but now we we run the thing on a set that forces temperature and precipitation

```{julia}

variableset = [1, 12]
possiblevariables = filter(i -> !(i in variableset), collect(axes(trainfeatures, 2)))

current_best = -Inf

while (!isempty(possiblevariables))
    scores = zeros(Float64, length(possiblevariables))
    for (i,pv) in enumerate(possiblevariables)
        tv = push!(copy(variableset), pv)
        C = [last(confusion_for_fold(trainlabels, trainfeatures, fold; vars=tv)) for fold in folds]
        scores[i] = mean(mcc.(C))
    end
    bs, bi = findmax(scores)
    if bs > current_best
        current_best = bs
        push!(variableset, possiblevariables[bi])
        deleteat!(possiblevariables, bi)
    else
        break
    end
    @info "Current set: $(variableset) - MCC: $(current_best)"
end

crvar = copy(variableset)

# Retrain
C = [last(confusion_for_fold(trainlabels, trainfeatures, fold; vars=variableset)) for fold in folds]
_tss = mean(trueskill.(C))
_acc = mean(accuracy.(C))
_ppv = mean(ppv.(C))
_npv = mean(npv.(C))
_mcc = mean(mcc.(C))
push!(varsel, 
    (Method = :Constrained, N = length(variableset), Accuracy = _acc, TSS = _tss, NPV = _npv, PPV = _ppv, MCC = _mcc)
)

@info variableset
```

now we run variable elimination

```{julia}
variableset = collect(axes(trainfeatures, 2))

current_best = -Inf

while (!isempty(variableset))
    scores = zeros(Float64, length(variableset))
    for (i,pv) in enumerate(variableset)
        tv = deleteat!(copy(variableset), i)
        C = [last(confusion_for_fold(trainlabels, trainfeatures, fold; vars=tv)) for fold in folds]
        scores[i] = mean(mcc.(C))
    end
    bs, bi = findmax(scores)
    if bs > current_best
        current_best = bs
        deleteat!(variableset, bi)
    else
        break
    end
    @info "Current set: $(variableset) - MCC: $(current_best)"
end

bwvar = copy(variableset)

# Retrain
C = [last(confusion_for_fold(trainlabels, trainfeatures, fold; vars=variableset)) for fold in folds]
_tss = mean(trueskill.(C))
_acc = mean(accuracy.(C))
_ppv = mean(ppv.(C))
_npv = mean(npv.(C))
_mcc = mean(mcc.(C))
push!(varsel, 
    (Method = :Backwards, N = length(variableset), Accuracy = _acc, TSS = _tss, NPV = _npv, PPV = _ppv, MCC = _mcc)
)

@info variableset
```

look at method

```{julia}
for col in [:Accuracy, :TSS, :NPV, :PPV, :MCC]
    varsel[!,col] = round.(varsel[!,col]; digits=3)
end
sort!(varsel, :MCC)
```

retrain the model with the best possible set

```{julia}
predictor = naivebayes(trainlabels, trainfeatures[:,crvar])
prediction = vec(mapslices(predictor, testfeatures[:,crvar], dims=2))
conf = ConfusionMatrix(prediction, testlabels)
@info tpr(conf)
@info tnr(conf)
@info fpr(conf)
@info fnr(conf)
@info trueskill(conf)
@info mcc(conf)
```

we save the data for the next run

```{julia}
_ptm_path = joinpath(@__DIR__, "..", "ptm")
modelpath = joinpath(_ptm_path, "reindeer-varselected.jld")
JLD.save(
    modelpath,
    "testing", (testlabels, testfeatures[:,crvar]),
    "training", (trainlabels, trainfeatures[:,crvar]),
    "varidx", crvar,
    "C", conf
)
```