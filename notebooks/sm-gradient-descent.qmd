# Gradient descent and linear regression

This notebook contains the code used in the chapter on gradient descent and linear regression. Note that this code does *not* rely on a package for gradient descent, instead showcasing the use of `Zygote` to calculate the gradients.

To run everything, we need to get a few data input packages, as well as the Makie plotting suite.

```{julia}
using DataFrames
import CSV
using CairoMakie
using Statistics
CairoMakie.activate!(; px_per_unit = 2)
```

We can now load the data -- because they come with a little more information than what we use, we will drop the columns we don't need (`:P` are the predation interactions, and `:H` the number of herbivory interactions) , and remove the rows with obvious issues (such as no documented interaction). The data are described in the main chapter.

```{julia}
ls = DataFrame(CSV.File(joinpath(pwd(), "data/gradientdescent/ls.csv")))
select!(ls, Not(:P))
select!(ls, Not(:H))
ls = ls[ls.L .> 0, :]
ls = ls[ls.L .>= (ls.S .- 1), :]
describe(ls)
```

We will make an obvious first plot: the relationship between $L$ and $S$ on a log-log scale:

```{julia}
#| label: fig-gradient-data
#| fig-cap: >
#|  Overview of data from ...
fig = Figure(; resolution=(500, 500))
axs = Axis(fig[1,1]; xlabel="Number of species", ylabel="Number of interactions", xscale=log, yscale=log)
scatter!(axs, ls.S, ls.L, color=:black, markersize=4)
current_figure()
```

The next step is to split the data into a training and testing set:

```{julia}
training_inclusion = rand(size(ls, 1)) .< 0.8
training = ls[findall(training_inclusion),:]
testing = ls[findall(.!training_inclusion),:]
```

We are interested in this relationship as a linear model, so we can write this code:

```{julia}
lm(x, m, b) = m .* x .+ b
```

We will use the $L_2$ loss for this dataset, so let's go ahead and write a function for this:

```{julia}
L2(ŷ, x, f, p...) = sum((ŷ .- f(x, p...)) .^ 2.0) / length(x)
```

And we need to get the model and the gradients -- this is done using `Zygote`; this code is hardcoding the loss function, which isn't ideal, but is fine. We don't really mind.

```{julia}
using Zygote
∇L(ŷ, x, f, p...) = gradient((p...) -> L2(ŷ, x, f, p...), p...)
```

Finally, we need a learning rate $\eta$:

```{julia}
η = 1e-3
```

We will then create a random set of parameters for the model; we could make a guess based on our knowledge of the system, but this isn't necessary here.

```{julia}
p = rand(2)
```

And now we get the values of $L$ and $S$ in the training/testing dataset. Because we want to model the log-log relationship, we will transform these into their natural logs here:

```{julia}
y = log.(training.L)
x = log.(training.S)

ty = log.(testing.L)
tx = log.(testing.S)
```

then

```{julia}
loss_train = zeros(Float64, 20_000+1)
loss_test = copy(loss_train)
loss_train[1] = L2(y, x, lm, p...)
loss_test[1] = L2(ty, tx, lm, p...)
epochs = length(loss_train)-1
```

then

```{julia}
for i in 1:epochs
    p .-= (η .* ∇L(y, x, lm, p...))
    loss_train[i+1] = L2(y, x, lm, p...)
    loss_test[i+1] = L2(ty, tx, lm, p...)
end
@info "Initial training loss: $(loss_train[begin])"
@info "Initial testing loss: $(loss_test[begin])"

@info "Final training loss: $(loss_train[end])"
@info "Final testing loss: $(loss_test[end])"
```

Now we can plot the training:

```{julia}
#| label: fig-gradient-loss-comparison
#| fig-cap: >
#|  Overview of data from ...
fig = Figure(; resolution=(500, 300))

axs = Axis(fig[1,1]; xlabel="Training epoch", ylabel="L₂ loss", yscale=log10)
lines!(axs, 0:epochs, loss_train, color=:black, label="Training")
lines!(axs, 0:epochs, loss_test, color=:red, label="Testing")
axislegend(axs; position=:rt)

current_figure()
```

and the residuals

```{julia}
#| label: fig-gradient-residuals
#| fig-cap: >
#|  Overview of the distributions of residuals (note that the residuals are calculated on the $log_2(L)$ and $log_2(S)$) after a large enough
#|  number of epochs for gradient descent.
#|  The grey histogram is the residuals on the training data, and the red density line is the residuals on the testing data.
fig = Figure(; resolution=(500, 300))
axs = Axis(fig[1,1])
hist!(axs, lm(x, p...) .- y; normalization=:pdf, color=:lightgrey, bins=20)
density!(axs, lm(tx, p...) .- ty, color=:transparent, strokewidth=1, strokecolor=:red, npoints=500)
tightlimits!(axs)
hidespines!(axs, :l, :r, :t)
hideydecorations!(axs; grid=true)
hidexdecorations!(axs; grid=true, label=false, ticks=false, ticklabels=false)
current_figure()
```

and the predictions:

```{julia}
#| label: fig-gradient-fitted
#| fig-cap: >
#|  Overview of data from ...
fig = Figure(; resolution=(500, 500))
axs = Axis(fig[1,1]; xlabel="Number of species", ylabel="Number of interactions", xscale=log, yscale=log)
scatter!(axs, training.S, training.L, color=:black, markersize=4)
pseudox = exp.(LinRange(1, log.(maximum(training.S)), 20))
lines!(axs, pseudox, exp.(p[1] .* log.(pseudox) .+ p[2]), color=:red)
lines!(axs, pseudox, pseudox.^2.0, color=:grey, linestyle=:dash)
lines!(axs, pseudox, pseudox.-1.0, color=:grey, linestyle=:dash)
current_figure()
```