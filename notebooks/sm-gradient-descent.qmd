# Suppl Mat. - Gradient descent and linear regression

This notebook contains the code used in the chapter on gradient descent and linear regression. Note that this code does *not* rely on a package for gradient descent, instead showcasing the use of `Zygote` to calculate the gradients.

To run everything, we need to get a few data input packages, as well as the Makie plotting suite.

```{julia}
_code_path = joinpath(@__DIR__, "..", "lib")
include(joinpath(_code_path, "pkg.jl"))
include(joinpath(_code_path, "splitters.jl"))
```

We can now load the data -- because they come with a little more information than what we use, we will drop the columns we don't need (`:P` are the predation interactions, and `:H` the number of herbivory interactions) , and remove the rows with obvious issues (such as no documented interaction). The data are described in the main chapter.

```{julia}
ls = DataFrame(CSV.File(joinpath(@__DIR__, "../data/gradientdescent/ls.csv")))
select!(ls, Not(:P))
select!(ls, Not(:H))
ls = ls[ls.L .> 0, :]
ls = ls[ls.L .>= (ls.S .- 1), :]
describe(ls)
```

We will make an obvious first plot: the relationship between $L$ and $S$ on a log-log scale:

```{julia}
#| label: fig-gradient-data
#| fig-cap: We have assumed that the relationship between $L$ and $S$ could be represented by $L \approx b\times S^a$, which gave us a reason to take the natural log of both variables. On this figure, we see that the relationship between the logs look linear, which means that linear regression has a good chance of estimating the values of the parameters.
fig = Figure(; resolution=(500, 500))
axs = Axis(fig[1,1]; xlabel="Number of species", ylabel="Number of interactions", xscale=log, yscale=log)
scatter!(axs, ls.S, ls.L, color=:black, markersize=4)
current_figure()
```

The next step is to split the data into a training and testing set:

```{julia}
train_idx, test_idx = holdout(ls.S, ls.L)
training = ls[train_idx,:]
testing = ls[test_idx,:]
```

We are interested in this relationship as a linear model, so we can write this code:

```{julia}
lm(x, m, b) = m .* x .+ b
```

We will use the $L_2$ loss for this dataset, so let's go ahead and write a function for this:

```{julia}
L2(ŷ, x, f, p...) = sum((ŷ .- f(x, p...)) .^ 2.0) / length(x)
```

And we need to get the model and the gradients -- this is done using `Zygote`; this code is hardcoding the loss function, which isn't ideal, but is fine. We don't really mind.

```{julia}
using Zygote
∇L(ŷ, x, f, p...) = gradient((p...) -> L2(ŷ, x, f, p...), p...)
```

Finally, we need a learning rate $\eta$:

```{julia}
η = 1e-3
```

We will then create a random set of parameters for the model; we could make a guess based on our knowledge of the system, but this isn't necessary here.

```{julia}
p = [0.4, 0.2]
```

And now we get the values of $L$ and $S$ in the training/testing dataset. Because we want to model the log-log relationship, we will transform these into their natural logs here:

```{julia}
y = log.(training.L)
x = log.(training.S)

ty = log.(testing.L)
tx = log.(testing.S)
```

then

```{julia}
loss_train = zeros(Float64, 20_000+1)
loss_test = copy(loss_train)
loss_train[1] = L2(y, x, lm, p...)
loss_test[1] = L2(ty, tx, lm, p...)
epochs = length(loss_train)-1
track_p1 = copy(loss_train)
track_p1[1] = p[1]
track_p2 = copy(loss_train)
track_p2[1] = p[2]
```

then

```{julia}
for i in 1:epochs
    p .-= (η .* ∇L(y, x, lm, p...))
    loss_train[i+1] = L2(y, x, lm, p...)
    loss_test[i+1] = L2(ty, tx, lm, p...)
    track_p1[i+1] = p[1]
    track_p2[i+1] = p[2]
end
@info "Initial training loss: $(loss_train[begin])"
@info "Initial testing loss: $(loss_test[begin])"

@info "Final training loss: $(loss_train[end])"
@info "Final testing loss: $(loss_test[end])"
```

```{julia}
#| label: tbl-gradient-attempt-one
#| tbl-cap: This table shows the change in the model, as measured by the loss and by the estimates of the parameters, after an increasing amount of training epochs. The loss drops sharply in the first 500 iterations, but even after 20000 iterations, there are still some changes in the values of the parameters.
df = DataFrame()
for i in [1, 10, 30, 100, 300, 1000, 3000, 10000, 20000]
    push!(df, (Step = i, LossTrain = loss_train[i] , LossTest = loss_test[i], b0 = track_p1[i], b1 = track_p2[i]))
end
rename!(df, :LossTrain => "Loss (training)")
rename!(df, :LossTest => "Loss (testing)")
rename!(df, :b0 => "β₀")
rename!(df, :b1 => "β₁")
out = pretty_table(String, df, tf = tf_html_simple, formatters = ft_nomissing, show_subheader = false, show_header = true)
display("text/html", out)
```

Now we can plot the training:

```{julia}
#| label: fig-gradient-loss-comparison
#| fig-cap: This figures shows the change in the loss for the training and testing dataset. As the two curves converge on low values at the same rate, this suggests that the model is not over-fitting, and is therefore suitable for use.
fig = Figure(; resolution=(500, 300))

axs = Axis(fig[1,1]; xlabel="Training epoch", ylabel="L₂ loss", yscale=log10)
lines!(axs, 0:epochs, loss_train, color=:black, label="Training")
lines!(axs, 0:epochs, loss_test, color=:red, label="Testing")
axislegend(axs; position=:rt)

current_figure()
```

see parameter values


```{julia}
#| label: fig-gradient-param-change
#| fig-cap: This figure shows the change in the parameters values over time. Note that the change is very large initially, because we make large steps when the gradient is strong. The rate of change gets much lower as we get nearer to the "correct" value.
fig = Figure(; resolution=(500, 300))

axs = Axis(fig[1,1]; xlabel="Training epoch", ylabel="β₀")
lines!(axs, 0:epochs, track_p1, color=:black, label="Training")

axs = Axis(fig[2,1]; xlabel="Training epoch", ylabel="β₁")
lines!(axs, 0:epochs, track_p2, color=:black, label="Testing")

current_figure()
```


and the residuals

```{julia}
#| label: fig-gradient-residuals
#| fig-cap: Overview of the distributions of residuals (note that the residuals are calculated on the $log_2(L)$ and $log_2(S)$) after a large enough number of epochs for gradient descent. The grey histogram is the residuals on the training data, and the red density line is the residuals on the testing data.
fig = Figure(; resolution=(500, 300))
axs = Axis(fig[1,1])
hist!(axs, lm(x, p...) .- y; normalization=:pdf, color=:lightgrey, bins=20)
density!(axs, lm(tx, p...) .- ty, color=:transparent, strokewidth=1, strokecolor=:red, npoints=500)
tightlimits!(axs)
hidespines!(axs, :l, :r, :t)
hideydecorations!(axs; grid=true)
hidexdecorations!(axs; grid=true, label=false, ticks=false, ticklabels=false)
current_figure()
```

and the predictions:

```{julia}
#| label: fig-gradient-fitted
#| fig-cap: Overview of data from ...
fig = Figure(; resolution=(500, 500))
axs = Axis(fig[1,1]; xlabel="Number of species", ylabel="Number of interactions", xscale=log, yscale=log)
scatter!(axs, training.S, training.L, color=:black, markersize=4)
pseudox = exp.(LinRange(1, log.(maximum(training.S)), 20))
lines!(axs, pseudox, exp.(p[1] .* log.(pseudox) .+ p[2]), color=:red)
lines!(axs, pseudox, pseudox.^2.0, color=:grey, linestyle=:dash)
lines!(axs, pseudox, pseudox.-1.0, color=:grey, linestyle=:dash)
current_figure()
```

model for the parameters

```{julia}
#| label: fig-gradient-landscape
#| fig-cap: Alternative representation of the data from @fig-gradient-param-change, where the change in parameter values (solid line) is plotted on top of the surface giving the log-loss for all pairs of parameters. We can intuit this figure as showing the movement of a ball (the parameters) rolling down a bowl, at the bottom of which is the correct model parameterization.
f = Figure()

p0 = LinRange(0.0, 2.0, 50)
p1 = LinRange(-2.0, 1.0, 50)
S = zeros(Float64, (length(p0), length(p1)))

for i in axes(S, 1)
    for j in axes(S, 2)
        S[i,j] = L2(y, x, lm, p0[i], p1[j])
    end
end

ax = Axis(f[1,1,], xlabel="β₀", ylabel="β₁")

contourf!(ax, p0, p1, log.(S), colormap=[:white, :darkblue], levels=12)
lines!(ax, track_p1, track_p2, color=:black)

current_figure()
```