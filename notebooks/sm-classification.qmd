# Supp. Mat. - classification

```{julia}
using DataFrames
import CSV
using CairoMakie
using Distributions
using StatsBase
using Statistics
using LinearAlgebra
CairoMakie.activate!(; px_per_unit = 2)
```

We load the data

```{julia}
_path = joinpath(@__DIR__, "..", "data", "general", "rangifer-observations.csv")
rangifer = DataFrame(CSV.File(_path))
select!(rangifer, Not(:latitude))
select!(rangifer, Not(:longitude))
select!(rangifer, [:presence, :BIO1, :BIO12])
```

we take aside a testing set

```{julia}
n_testing = floor(Int, 0.2 * size(rangifer, 1))

testind_idx = sample(1:size(rangifer, 1), n_testing, replace=false)
training_idx = filter(i -> !(i in testind_idx), 1:size(rangifer, 1))

train, test = rangifer[training_idx, :], rangifer[testind_idx, :]
```

we will split this a little more to make it easier

```{julia}
trainlabels = Array(select(train, :presence))[:,1]
testlabels = Array(select(test, :presence))[:,1]
trainfeatures = Array(select(train, Not(:presence)))
testfeatures = Array(select(test, Not(:presence)))
```

we will use the z-score to standardize, but not now! we want to do the training set FIRST and then use this for the prediction steps, we treat z-score as a trainable parameter (link with data leakage)

```{julia}
function vnorm(x::Vector{T}, μ::T1, σ::T2) where {T <: Number, T1 <: Number, T2 <: Number}
    return (x .- μ) ./ σ
end
function vnorm(x::Vector{T}) where {T <: Number}
    return vnorm(x, mean(x), std(x))
end
```

we do a whole function here

```{julia}
function NBC(y, X)
    μ = mapslices(mean, X, dims=1)
    σ = mapslices(std, X, dims=1)
    Xpos = (X[findall(y),:] .- μ) ./ σ
    Xneg = (X[findall(.!y),:] .- μ) ./ σ
    pred_pos = mapslices(x -> Normal(mean(x), std(x)), Xpos, dims=1)
    pred_neg = mapslices(x -> Normal(mean(x), std(x)), Xneg, dims=1)
    function inner_predictor(v)
        nv = (v' .- μ) ./ σ
        is_pos = prod(pdf.(pred_pos, nv))
        is_neg = prod(pdf.(pred_neg, nv))
        return is_pos > is_neg
    end
    return inner_predictor
end
```

confusion matrix

```{julia}
function confmat(pred, truth)
    tp = sum(pred .& truth)
    tn = sum(.!pred .& .!truth)
    fp = sum(pred .& .!truth)
    fn = sum(.!pred .& truth)
    return [tp fn; fp tn]
end
```

We'll do LOOCV here, so it's easier **TODO** monte carlo CV instead

```{julia}
conftrain = zeros(Float64, (2, 2, 2000))
confvalid = similar(conftrain)
n_valid = floor.(Int, 0.2*length(trainlabels))

for mcfold in axes(conftrain, 3)
    validx = sample(axes(trainlabels, 1), n_valid, replace=false)
    trnidx = filter!(i -> !(i in validx), collect(axes(trainlabels, 1)))
    predictor = NBC(trainlabels[trnidx], trainfeatures[trnidx,:])
    # Training conf mat
    trnpred = vec(mapslices(predictor, trainfeatures[trnidx,:], dims=2))
    conftrain[:,:,mcfold] = confmat(trnpred, trainlabels[trnidx,:])
    # Validation conf mat
    valpred = vec(mapslices(predictor, trainfeatures[validx,:], dims=2))
    confvalid[:,:,mcfold] = confmat(valpred, trainlabels[validx,:])
end
```

some measures on the table

```{julia}
acc(m) = tr(m)/sum(m)
tpr(m) = m[1,1]/(m[1,1]+m[1,2])
tnr(m) = m[2,2]/(m[2,2]+m[2,1])
fpr(m) = m[2,1]/(m[2,1]+m[2,2])
fnr(m) = m[1,2]/(m[1,2]+m[1,1])
ppv(m) = m[1,1]/(m[1,1]+m[2,1])
npv(m) = m[2,2]/(m[2,2]+m[1,2])
fone(m) = 2.0*(ppv(m)*tpr(m))/(ppv(m)+tpr(m))
inform(m) = tpr(m) + tnr(m) - 1.0
```

how good are our folds

```{julia}
#| label: fig-classification-crossvalidation
#| fig-cap: WAH NEVER CHANGES
fmat = [(ppv, "PPV") (npv, "NPV"); (ppv, "PPV") (npv, "NPV")]
f = Figure()

for ci in CartesianIndices(fmat)
    ax = Axis(f[ci.I...])
    fm, lab = fmat[ci]
    trv = dropdims(mapslices(fm, conftrain, dims=(1,2)), dims=(1,2))
    vav = dropdims(mapslices(fm, confvalid, dims=(1,2)), dims=(1,2))
    density!(ax, trv, color=(:lightgrey, 0.1), strokecolor=:lightgrey, strokewidth=2)
    c = Makie.wong_colors()[3]
    density!(ax, vav, color=(c, 0.3), strokecolor=c, strokewidth=2)
    tightlimits!(ax)
end
current_figure()
```

on the testing data now

```{julia}
predictor = NBC(trainlabels, trainfeatures)
prediction = vec(mapslices(predictor, testfeatures, dims=2))
conf = confmat(prediction, testlabels)
@info ppv(conf)
@info npv(conf)
@info fone(conf)
@info inform(conf)
```

figure

```{julia}
#| label: fig-classification-separability
#| fig-cap: show the separability of classes
f = Figure(; resolution=(500, 500))

axtemp = Axis(f[1,1])
c = Makie.wong_colors()[1]
density!(axtemp, 0.1filter(r -> r.presence, train).BIO1, color=(c, 0.3), strokecolor=c, strokewidth=1, normalization=:pdf)
c = Makie.wong_colors()[2]
density!(axtemp, 0.1filter(r -> !r.presence, train).BIO1, color=(c, 0.3), strokecolor=c, strokewidth=1, normalization=:pdf)

axprec = Axis(f[2,2])
c = Makie.wong_colors()[1]
density!(axprec, 0.1filter(r -> r.presence, train).BIO12, color=(c, 0.3), strokecolor=c, strokewidth=1, normalization=:pdf, direction=:y)
c = Makie.wong_colors()[2]
density!(axprec, 0.1filter(r -> !r.presence, train).BIO12, color=(c, 0.3), strokecolor=c, strokewidth=1, normalization=:pdf, direction=:y)


for ax in [axprec, axtemp]
    hidespines!(ax, :l, :r, :t)
end

axboth = Axis(f[2,1])
xp = 0.1filter(r -> r.presence, train).BIO1
xn = 0.1filter(r -> !r.presence, train).BIO1
yp = filter(r -> r.presence, train).BIO12
yn = filter(r -> !r.presence, train).BIO12
c = Makie.wong_colors()[1]
scatter!(axboth, xp, yp, color=c, markersize=4, label="Presence")
c = Makie.wong_colors()[2]
scatter!(axboth, xn, yn, color=c, markersize=4, label="Pseudo-absence")

current_figure()
```
