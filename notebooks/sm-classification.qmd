# Supp. Mat. - classification

```{julia}
_code_path = joinpath(@__DIR__, "..", "lib")
include(joinpath(_code_path, "pkg.jl"))
include(joinpath(_code_path, "confusiontable.jl"))
include(joinpath(_code_path, "nbc.jl"))
include(joinpath(_code_path, "splitters.jl"))
```

We load the data

```{julia}
_path = joinpath(@__DIR__, "..", "data", "general", "rangifer-observations.csv")
rangifer = DataFrame(CSV.File(_path))
select!(rangifer, Not(:latitude))
select!(rangifer, Not(:longitude))
select!(rangifer, [:presence, :BIO1, :BIO12])
```

we take aside a testing set

```{julia}
labels = rangifer.presence
features = Matrix(select(rangifer, Not(:presence)))
training_idx, testing_idx = holdout(labels, features)
```

we will split this a little more to make it easier

```{julia}
trainlabels = labels[training_idx]
testlabels = labels[testing_idx]
trainfeatures = features[training_idx,:]
testfeatures = features[testing_idx,:]
```

we will use the z-score to standardize, but not now! we want to do the training set FIRST and then use this for the prediction steps, we treat z-score as a trainable parameter (link with data leakage)

We will use Monte-Carlo cross-validations, which is simply the repetition of holdout validation a large enough number of times.

```{julia}
conftrain = zeros(ConfusionMatrix, 100)
confvalid = zeros(ConfusionMatrix, length(conftrain))

for fold in axes(conftrain, 1)
    t_idx, v_idx = holdout(trainlabels, trainfeatures)
    predictor = naivebayes(trainlabels[t_idx], trainfeatures[t_idx,:])
    # Training conf mat
    prediction = vec(mapslices(predictor, trainfeatures[t_idx,:], dims=2))
    conftrain[fold] = ConfusionMatrix(prediction, vec(trainlabels[t_idx,:]))
    # Validation conf mat
    validation = vec(mapslices(predictor, trainfeatures[v_idx,:], dims=2))
    confvalid[fold] = ConfusionMatrix(validation, vec(trainlabels[v_idx,:]))
end
```

how good are our folds

```{julia}
#| label: fig-classification-crossvalidation
#| fig-cap: WAH NEVER CHANGES
fmat = [(tpr, "Sensitivity") (ppv, "PPV") (trueskill, "TSS"); (tnr, "Specificity") (npv, "NPV") (mcc, "MCC")]
f = Figure()

for ci in CartesianIndices(fmat)
    cfunc, lab = fmat[ci]
    ax = Axis(f[ci.I...]; xlabel=lab)
    trv = cfunc.(conftrain)
    vav = cfunc.(confvalid)
    x = vcat(fill(0, length(trv)), fill(1, length(vav)))
    y = vcat(trv, vav)
    trc = Makie.wong_colors()[3]
    vac = :lightgrey
    boxplot!(ax, x, y; orientation=:vertical, color = map(d->d==1 ? vac : trc, x))
    hidexdecorations!(ax, label=false)
end

current_figure()
```

on the testing data now

```{julia}
predictor = naivebayes(trainlabels, trainfeatures)
prediction = vec(mapslices(predictor, testfeatures, dims=2))
conf = ConfusionMatrix(prediction, testlabels)
@info tpr(conf)
@info tnr(conf)
@info ppv(conf)
@info npv(conf)
@info f1(conf)
@info trueskill(conf)
@info mcc(conf)
```

figure

```{julia}
#| label: fig-classification-separability
#| fig-cap: This figures show the separability of the presences (blue) and pseudo-absences (orange) on the temperature, but not on the precipitation, dimensions.
f = Figure(; resolution=(500, 500))

gl = f[1,1] = GridLayout()

axtemp = Axis(gl[1,1])
c = Makie.wong_colors()[1]
density!(axtemp, 0.1filter(r -> r.presence, rangifer).BIO1, color=(c, 0.3), strokecolor=c, strokewidth=1, normalization=:pdf)
c = Makie.wong_colors()[2]
density!(axtemp, 0.1filter(r -> !r.presence, rangifer).BIO1, color=(c, 0.3), strokecolor=c, strokewidth=1, normalization=:pdf)

axprec = Axis(gl[2,2])
c = Makie.wong_colors()[1]
density!(axprec, 0.1filter(r -> r.presence, rangifer).BIO12, color=(c, 0.3), strokecolor=c, strokewidth=1, normalization=:pdf, direction=:y)
c = Makie.wong_colors()[2]
density!(axprec, 0.1filter(r -> !r.presence, rangifer).BIO12, color=(c, 0.3), strokecolor=c, strokewidth=1, normalization=:pdf, direction=:y)

axboth = Axis(gl[2,1], xlabel="Mean air temperature (°C)", ylabel = "Annual precipitation (kg m⁻²)")
xp = 0.1filter(r -> r.presence, rangifer).BIO1
xn = 0.1filter(r -> !r.presence, rangifer).BIO1
yp = filter(r -> r.presence, rangifer).BIO12
yn = filter(r -> !r.presence, rangifer).BIO12
c = Makie.wong_colors()[1]
scatter!(axboth, xp, yp, color=c, markersize=4, label="Presence")
c = Makie.wong_colors()[2]
scatter!(axboth, xn, yn, color=c, markersize=4, label="Pseudo-absence")

hidespines!(axtemp, :l, :r, :t)
hidespines!(axprec, :b, :r, :t)
hidedecorations!(axtemp, grid = true)
hidedecorations!(axprec, grid = true)
ylims!(axtemp, low = 0)
xlims!(axprec, low = 0)
colgap!(gl, 0)
rowgap!(gl, 0)

colsize!(gl, 1, Relative(4/5))
rowsize!(gl, 2, Relative(4/5))

current_figure()
```

prediction diagram

```{julia}
air = LinRange(extrema(rangifer.BIO1)..., 150)
prc = LinRange(extrema(rangifer.BIO12)..., 150)
X = zeros(Float64, (length(air), length(prc)))
for i in axes(air, 1)
    for j in axes(prc, 1)
        X[i,j] = predictor([air[i], prc[j]]) > 0.5
    end
end
```

and now we plot

```{julia}
#| label: fig-classification-decision
#| fig-cap: Overview of the decision boundary between the positive (blue) and negative (classes) using the NBC with two variables. Note that, as expected with a Gaussian distribution, the limit between the two classes looks circular. The assumption of statistical independance between the features means that we would not see, for example, an ellipse.
f = Figure(; resolution=(500, 500))
ax = Axis(f[1,1], xlabel="Mean air temperature (°C)", ylabel = "Annual precipitation (kg m⁻²)")
pal = Makie.wong_colors()[[2,1]]
heatmap!(ax, 0.1air, 0.1prc, X, colormap=pal, alpha=0.3)
current_figure()
```