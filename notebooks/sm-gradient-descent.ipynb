{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Gradient descent and linear regression\n",
        "\n",
        "```{julia}\n",
        "using DataFrames\n",
        "import CSV\n",
        "using CairoMakie\n",
        "using Statistics\n",
        "CairoMakie.activate!(; px_per_unit = 2)\n",
        "```\n",
        "\n",
        "We can now load the data -- because they come with a little more information, we will drop the columns we don't need, and remove the rows with obvious issues:\n",
        "\n",
        "```{julia}\n",
        "ls = DataFrame(CSV.File(joinpath(pwd(), \"data/gradientdescent/ls.csv\")))\n",
        "select!(ls, Not(:P))\n",
        "select!(ls, Not(:H))\n",
        "ls = ls[ls.L .> 0, :]\n",
        "describe(ls)\n",
        "```\n",
        "\n",
        "Now we can make a first dataviz:\n",
        "\n",
        "```{julia}\n",
        "#| label: fig-gradient-data\n",
        "#| fig-cap: >\n",
        "#|  Overview of data from ...\n",
        "fig = Figure(; resolution=(500, 500))\n",
        "axs = Axis(fig[1,1]; xlabel=\"Nodes\", ylabel=\"Edges\", xscale=log2, yscale=log2)\n",
        "scatter!(axs, ls.S, ls.L, color=:black, markersize=4)\n",
        "current_figure()\n",
        "```\n",
        "\n",
        "The next step is to split the data into a training and testing set:\n",
        "\n",
        "```{julia}\n",
        "training_inclusion = rand(size(ls, 1)) .< 0.8\n",
        "training = ls[findall(training_inclusion),:]\n",
        "testing = ls[findall(.!training_inclusion),:]\n",
        "```\n",
        "\n",
        "And we need to get the model and the gradients -- this is done using `Zygote`:\n",
        "\n",
        "```{julia}\n",
        "using Zygote\n",
        "lm(x, m, b) = m .* x .+ b\n",
        "L2(ŷ, x, f, p...) = sum((ŷ .- f(x, p...)) .^ 2.0) / length(x)\n",
        "∇L(ŷ, x, f, p...) = gradient((p...) -> L2(ŷ, x, f, p...), p...)\n",
        "η = 1e-3\n",
        "```\n",
        "\n",
        "Initial set of values and initial loss:\n",
        "\n",
        "```{julia}\n",
        "p = rand(2)\n",
        "\n",
        "y = log2.(training.L)\n",
        "x = log2.(training.S)\n",
        "\n",
        "ty = log2.(testing.L)\n",
        "tx = log2.(testing.S)\n",
        "```\n",
        "\n",
        "```{julia}\n",
        "p = rand(2)\n",
        "loss_train = zeros(Float64, 20_000+1)\n",
        "loss_test = copy(loss_train)\n",
        "loss_train[1] = L2(y, x, lm, p...)\n",
        "loss_test[1] = L2(ty, tx, lm, p...)\n",
        "epochs = length(loss_train)-1\n",
        "for i in 1:epochs\n",
        "    p .-= (η .* ∇L(y, x, lm, p...))\n",
        "    loss_train[i+1] = L2(y, x, lm, p...)\n",
        "    loss_test[i+1] = L2(ty, tx, lm, p...)\n",
        "end\n",
        "@info \"Initial training loss: $(loss_train[begin])\"\n",
        "@info \"Initial testing loss: $(loss_test[begin])\"\n",
        "\n",
        "@info \"Final training loss: $(loss_train[end])\"\n",
        "@info \"Final testing loss: $(loss_test[end])\"\n",
        "```\n",
        "\n",
        "Now we can plot the training:\n",
        "\n",
        "```{julia}\n",
        "#| label: fig-gradient-loss-comparison\n",
        "#| fig-cap: >\n",
        "#|  Overview of data from ...\n",
        "fig = Figure(; resolution=(500, 500))\n",
        "\n",
        "axs = Axis(fig[1,1]; xlabel=\"Training epoch\", ylabel=\"L₂ loss\")\n",
        "lines!(axs, 0:epochs, loss_train, color=:black)\n",
        "lines!(axs, 0:epochs, loss_test, color=:red)\n",
        "\n",
        "axs2 = Axis(fig[2,1]; xlabel=\"Training epoch\", ylabel=\"Excess loss on training data\")\n",
        "fitdiag = loss_test ./ (loss_test .+ loss_train)\n",
        "lines!(axs2, 0:epochs, log.(fitdiag ./ (1.0 .- fitdiag)), color=:black)\n",
        "ylims!(axs2, (-0.5, 0.5))\n",
        "\n",
        "current_figure()\n",
        "```\n",
        "\n",
        "and the residuals\n",
        "\n",
        "```{julia}\n",
        "#| label: fig-gradient-residuals\n",
        "#| fig-cap: >\n",
        "#|  Overview of the distributions of residuals (note that the residuals are calculated on the $log_2(L)$ and $log_2(S)$) after a large enough\n",
        "#|  number of epochs for gradient descent.\n",
        "#|  The grey histogram is the residuals on the training data, and the red density line is the residuals on the testing data.\n",
        "fig = Figure(; resolution=(500, 300))\n",
        "axs = Axis(fig[1,1])\n",
        "hist!(axs, lm(x, p...) .- y; normalization=:pdf, color=:lightgrey, bins=20)\n",
        "density!(axs, lm(tx, p...) .- ty, color=:transparent, strokewidth=1, strokecolor=:red, npoints=500)\n",
        "tightlimits!(axs)\n",
        "hidespines!(axs, :l, :r, :t)\n",
        "hideydecorations!(axs; grid=true)\n",
        "hidexdecorations!(axs; grid=true, label=false, ticks=false, ticklabels=false)\n",
        "current_figure()\n",
        "```\n",
        "\n",
        "and the predictions:\n",
        "\n",
        "```{julia}\n",
        "#| label: fig-gradient-fitted\n",
        "#| fig-cap: >\n",
        "#|  Overview of data from ...\n",
        "fig = Figure(; resolution=(500, 500))\n",
        "axs = Axis(fig[1,1]; xlabel=\"Nodes\", ylabel=\"Edges\", xscale=log2, yscale=log2)\n",
        "scatter!(axs, training.S, training.L, color=:black, markersize=4)\n",
        "pseudox = 2.0.^LinRange(1, log2.(maximum(training.S)), 20)\n",
        "lines!(axs, pseudox, 2.0.^(p[1] .* log2.(pseudox) .+ p[2]), color=:red)\n",
        "current_figure()\n",
        "```"
      ],
      "id": "f437922e"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}