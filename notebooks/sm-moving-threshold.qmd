# Supp. Mat. - moving threshold

This follows the Naive Bayes module, and expands it to have a little more depth to it with priors and a probabilistic output; most of the setup code is shared with the NBC notebook, but duplication is fine as it makes the two notebooks independent.

```{julia}
using DataFrames
import CSV
using CairoMakie
using Distributions
using StatsBase
using Statistics
using LinearAlgebra
using PrettyTables, Latexify
CairoMakie.activate!(; px_per_unit = 2)
```

We load the data

```{julia}
_path = joinpath(@__DIR__, "..", "data", "general", "rangifer-observations.csv")
rangifer = DataFrame(CSV.File(_path))
presabs = select(rangifer, [:longitude, :latitude, :presence])
select!(rangifer, Not(:latitude))
select!(rangifer, Not(:longitude))
select!(rangifer, [:presence, :BIO1, :BIO2, :BIO15])
```

we take aside a testing set

```{julia}
n_testing = floor(Int, 0.2 * size(rangifer, 1))

testind_idx = sample(1:size(rangifer, 1), n_testing, replace=false)
training_idx = filter(i -> !(i in testind_idx), 1:size(rangifer, 1))

train, test = rangifer[training_idx, :], rangifer[testind_idx, :]
```

we will split this a little more to make it easier

```{julia}
trainlabels = Array(select(train, :presence))[:,1]
testlabels = Array(select(test, :presence))[:,1]
trainfeatures = Array(select(train, Not(:presence)))
testfeatures = Array(select(test, Not(:presence)))
```

we will use the z-score to standardize, but not now! we want to do the training set FIRST and then use this for the prediction steps, we treat z-score as a trainable parameter (link with data leakage)

```{julia}
function vnorm(x::Vector{T}, μ::T1, σ::T2) where {T <: Number, T1 <: Number, T2 <: Number}
    return (x .- μ) ./ σ
end
function vnorm(x::Vector{T}) where {T <: Number}
    return vnorm(x, mean(x), std(x))
end
```

we do a whole function here

```{julia}
function pNBC(y, X)
    μ = mapslices(mean, X, dims=1)
    σ = mapslices(std, X, dims=1)
    Xpos = (X[findall(y),:] .- μ) ./ σ
    Xneg = (X[findall(.!y),:] .- μ) ./ σ
    pred_pos = mapslices(x -> Normal(mean(x), std(x)), Xpos, dims=1)
    pred_neg = mapslices(x -> Normal(mean(x), std(x)), Xneg, dims=1)
    function inner_predictor(v; p=0.5)
        nv = (v' .- μ) ./ σ
        is_pos = prod(pdf.(pred_pos, nv))
        is_neg = prod(pdf.(pred_neg, nv))
        evid = p * is_pos + (1 - p) * is_neg
        return (p * is_pos)/evid
    end
    return inner_predictor
end
```

confusion matrix

```{julia}
function confmat(pred, truth)
    tp = sum(pred .& truth)
    tn = sum(.!pred .& .!truth)
    fp = sum(pred .& .!truth)
    fn = sum(.!pred .& truth)
    return [tp fn; fp tn]
end
acc(m) = tr(m)/sum(m)
tpr(m) = m[1,1]/(m[1,1]+m[1,2])
tnr(m) = m[2,2]/(m[2,2]+m[2,1])
fpr(m) = m[2,1]/(m[2,1]+m[2,2])
fnr(m) = m[1,2]/(m[1,2]+m[1,1])
ppv(m) = m[1,1]/(m[1,1]+m[2,1])
npv(m) = m[2,2]/(m[2,2]+m[1,2])
fone(m) = 2.0*(ppv(m)*tpr(m))/(ppv(m)+tpr(m))
inform(m) = tpr(m) + tnr(m) - 1.0
mcc(m) = ((m[1,1]*m[2,2])-(m[1,2]*m[2,1]))/sqrt((m[1,1]+m[2,1])*(m[1,1]+m[1,2])*(m[2,2]+m[2,1])*(m[2,2]+m[1,2]))
```

We'll do LOOCV here, so it's easier **TODO** monte carlo CV instead

```{julia}
thresholds = collect(LinRange(0.0, 1.0, 300))
conftrain = zeros(Float64, (2, 2, 50, length(thresholds)))
confvalid = similar(conftrain)
n_valid = floor.(Int, 0.1*length(trainlabels))

for mcfold in axes(conftrain, 3)
    validx = sample(axes(trainlabels, 1), n_valid, replace=false)
    trnidx = filter!(i -> !(i in validx), collect(axes(trainlabels, 1)))
    predictor = pNBC(trainlabels[trnidx], trainfeatures[trnidx,:])
    # Probability predictions
    trnpred = vec(mapslices(predictor, trainfeatures[trnidx,:], dims=2))
    valpred = vec(mapslices(predictor, trainfeatures[validx,:], dims=2))
    # Thresholds
    for (i,t) in enumerate(thresholds)
        conftrain[:,:,mcfold,i] = confmat(trnpred .>= t, trainlabels[trnidx,:])
        confvalid[:,:,mcfold,i] = confmat(valpred .>= t, trainlabels[validx,:])
    end
end
```

how good are our folds

```{julia}
#| label: fig-moving-roc-pr
#| fig-cap: roc and pr curves
raw_tpr = dropdims(mapslices(tpr, confvalid; dims=(1,2)), dims=(1,2))
raw_fpr = dropdims(mapslices(fpr, confvalid; dims=(1,2)), dims=(1,2))
raw_ppv = dropdims(mapslices(ppv, confvalid; dims=(1,2)), dims=(1,2))
f = Figure()
roc = Axis(f[1,1])
pr = Axis(f[1,2])
for i in axes(confvalid, 3)
    lines!(roc, raw_fpr[i,:], raw_tpr[i,:], color=(:black, 0.1))
    lines!(pr, raw_tpr[i,:], raw_ppv[i,:], color=(:black, 0.1))
end
for ax in [roc, pr]
    xlims!(ax, (0., 1.0))
    ylims!(ax, (0., 1.0))
    tightlimits!(ax)
end
hlines!(pr, [0.5], color=:black, linestyle=:dash)
ablines!(roc, [0.0], [1.0], color=:black, linestyle=:dash)
current_figure()
```

pretty good -- now we need to pick an optimal value for the threshold

```{julia}
#| label: fig-moving-thresholding
#| fig-cap: performance tuning curve
raw_perf = dropdims(mapslices(mcc, confvalid; dims=(1,2)), dims=(1,2))
raw_perf[findall(isnan, raw_perf)] .= 0.0
f = Figure()
ax = Axis(f[1,1]; xlabel="Threshold", ylabel="MCC")
ylims!(ax, low=0.0, high=1.0)
xlims!(ax, low=0.0, high=1.0)
for i in axes(confvalid, 3)
    lines!(ax, thresholds, raw_perf[i,:], color=:lightgrey)
end
mean_perf = vec(mapslices(mean, raw_perf, dims=1))
lines!(ax, thresholds, mean_perf, color=:black, linewidth=4, linestyle=:dash)
current_figure()
```

train the final model

```{julia}
τ = thresholds[last(findmax(mean_perf))]
@info τ
predictor = pNBC(trainlabels, trainfeatures)
```

and test on the testing data

```{julia}
#| label: tbl-moving-confusion
#| tbl-cap: confusion table after tuning the value of $\tau$ etc etc 
prediction = vec(mapslices(predictor, testfeatures, dims=2))
conf = confmat(prediction .>= τ, testlabels)
out = pretty_table(String, conf, tf = tf_html_simple, formatters = ft_nomissing, show_subheader = false, show_header = false)
display("text/html", out)
```

but we can add an extra step! optimize the prior and the threshold at the same time using grid search using two loops, first the prior and then the thresholding  -- this is rather long to run, so we actually do the search only for a relevant range of parameters

```{julia}
thresholds = collect(LinRange(0.0, 1.0, 20))
priors = collect(LinRange(0.0, 1.0, 10))
scores = zeros(Float64, (length(thresholds), length(priors)))

for (pri, pr) in enumerate(priors)
    @info pri
    conftrain = zeros(Float64, (2, 2, 10, length(thresholds)))
    confvalid = similar(conftrain)
    n_valid = floor.(Int, 0.1*length(trainlabels))
    for mcfold in axes(conftrain, 3)
        validx = sample(axes(trainlabels, 1), n_valid, replace=false)
        trnidx = filter!(i -> !(i in validx), collect(axes(trainlabels, 1)))
        predictor = (v) -> pNBC(trainlabels[trnidx], trainfeatures[trnidx,:])(v; p=pr)
        trnpred = vec(mapslices(predictor, trainfeatures[trnidx,:], dims=2))
        valpred = vec(mapslices(predictor, trainfeatures[validx,:], dims=2))
        # Thresholds
        for (i,t) in enumerate(thresholds)
            conftrain[:,:,mcfold,i] = confmat(trnpred .>= t, trainlabels[trnidx,:])
            confvalid[:,:,mcfold,i] = confmat(valpred .>= t, trainlabels[validx,:])
        end
    end
    raw_perf = dropdims(mapslices(mcc, confvalid; dims=(1,2)), dims=(1,2))
    raw_perf[findall(isnan, raw_perf)] .= 0.0
    mean_perf = vec(mapslices(mean, raw_perf, dims=1))
    scores[:,pri] .= mean_perf
    @info findmax(scores)
end
```

look at the grid at the end of the tuning process

```{julia}
heatmap(scores)
```

we get the best parameters

```{julia}
best_pos = last(findmax(scores))
prior = priors[best_pos[2]]
τ = thresholds[best_pos[1]]
@info prior, τ
```

now we run the prediction on the model with both parameters tuned

```{julia}
predictor = (v) -> pNBC(trainlabels, trainfeatures)(v; p=prior)
```

output the table

```{julia}
#| label: tbl-moving-confusion-grid
#| tbl-cap: confusion table after tuning the value of $\tau$ and the prior etc etc 
prediction = vec(mapslices(predictor, testfeatures, dims=2))
conf = confmat(prediction .>= τ, testlabels)
out = pretty_table(String, conf, tf = tf_html_simple, formatters = ft_nomissing, show_subheader = false, show_header = false)
display("text/html", out)
```

look at probas - this explains why the thresholding curve was essentially flat, the model is *really* good at picking 0/1 probabilities

```{julia}
#| label: fig-moving-explanation
#| fig-cap: probas etc etc
f = Figure()
gl = f[1,1] = GridLayout()

axp = Axis(gl[1,1])
hist!(axp, prediction, normalization=:probability, bins=10, color=:lightgrey)
xlims!(axp, low=0.0, high=1.0)
ylims!(axp, low=0.0)
hidespines!(axp)
hidedecorations!(axp)

axr = Axis(gl[2,1]; xlabel="Predicted probability for the presence class")
colpos = Makie.wong_colors()[6]
cl = [(p ? colpos : :grey) for p in ((prediction .>= τ).==testlabels)]
rainclouds!(axr, testlabels, prediction, plot_boxplots=false, clouds=nothing, side_nudge=0.0, orientation=:horizontal, jitter_width=0.5, markersize=8, color=cl)
xlims!(axr, low=0.0, high=1.0)
ylims!(axr, low=-0.5, high=1.5)

vlines!(axr, τ, color=:black, linestyle=:dash)

rowgap!(gl, 0)
rowsize!(gl, 1, Relative(1/7))

current_figure()
```

```{julia}
@info tpr(conf)
@info tnr(conf)
@info fpr(conf)
@info fnr(conf)
@info inform(conf)
@info mcc(conf)
```

get the data

```{julia}
using SpeciesDistributionToolkit
_layer_path = joinpath(@__DIR__, "..", "data", "general", "rangifer-layers.tiff")
bio1 = SpeciesDistributionToolkit._read_geotiff(_layer_path, SimpleSDMResponse; bandnumber=1)
bio2 = SpeciesDistributionToolkit._read_geotiff(_layer_path, SimpleSDMResponse; bandnumber=2)
bio15 = SpeciesDistributionToolkit._read_geotiff(_layer_path, SimpleSDMResponse; bandnumber=15)
```

make the predictions for every pixel

```{julia}
pr = convert(Float64, similar(bio1))
for k in keys(pr)
    x = [bio1[k], bio2[k], bio15[k]]
    pr[k] = predictor(x)
end
```

show the results on a map

```{julia}
#| label: fig-moving-rangemap
#| fig-cap: Range map of *Rangifer tarandus tarandus* as ...
f = Figure(; resolution=(500, 300))
ax = Axis(f[1,1]; xlabel="Longitude", ylabel="Latitude", xgridvisible=false, ygridvisible=false)
heatmap!(ax, pr .> τ, colormap=[:lightgrey, :grey])
presences = filter(r -> r.presence, presabs)
scatter!(ax, presences.longitude, presences.latitude, color=:black, markersize=2)
current_figure()
```

finally show the uncertainty, which is useful as a way to setup the rest of the modules

```{julia}
entropy = similar(pr)
for k in keys(pr)
    p = [pr[k], 1.0 - pr[k]]
    entropy[k] = -sum(p .* log2.(p))
end
```

```{julia}
#| label: fig-moving-uncertainty
#| fig-cap: Uncertainty (in bits) of the prediction made by the fully trained NBC. As we have seen in fig **TODO**, the model is really good at producing probabilities that are close to 0 and close to 1, and therefore the areas of high uncertainty are rather narrow in space. Note that the areas of higher uncertainty correspond, largely, to the areas of transition from predicted presence to predicted absence.
f = Figure(; resolution=(500, 300))
ax = Axis(f[1,1]; xlabel="Longitude", ylabel="Latitude", xgridvisible=false, ygridvisible=false)
heatmap!(ax, entropy, colormap=:lapaz)
current_figure()
```