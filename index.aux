\relax 
\providecommand*\new@tpo@label[2]{}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand*\HyPL@Entry[1]{}
\HyPL@Entry{0<</S/r>>}
\HyPL@Entry{4<</S/D>>}
\newlabel{preface}{{}{1}{}{chapter*.2}{}}
\@writefile{toc}{\contentsline {chapter}{Preface}{1}{chapter*.2}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{3}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lop}{\addvspace {10\p@ }}
\newlabel{introduction}{{1}{3}{Introduction}{chapter.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Core concepts in data science}{4}{section.1.1}\protected@file@percent }
\newlabel{core-concepts-in-data-science}{{1.1}{4}{Core concepts in data science}{section.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}EDA}{4}{subsection.1.1.1}\protected@file@percent }
\newlabel{eda}{{1.1.1}{4}{EDA}{subsection.1.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.2}Clustering and regression}{4}{subsection.1.1.2}\protected@file@percent }
\newlabel{clustering-and-regression}{{1.1.2}{4}{Clustering and regression}{subsection.1.1.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.3}Supervised and unsupervised}{4}{subsection.1.1.3}\protected@file@percent }
\newlabel{supervised-and-unsupervised}{{1.1.3}{4}{Supervised and unsupervised}{subsection.1.1.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.4}Training, testing, and validation}{4}{subsection.1.1.4}\protected@file@percent }
\newlabel{training-testing-and-validation}{{1.1.4}{4}{Training, testing, and validation}{subsection.1.1.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.5}Transformations and feature engineering}{4}{subsection.1.1.5}\protected@file@percent }
\newlabel{transformations-and-feature-engineering}{{1.1.5}{4}{Transformations and feature engineering}{subsection.1.1.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}An overview of the content}{4}{section.1.2}\protected@file@percent }
\newlabel{an-overview-of-the-content}{{1.2}{4}{An overview of the content}{section.1.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Some rules about this book}{5}{section.1.3}\protected@file@percent }
\newlabel{some-rules-about-this-book}{{1.3}{5}{Some rules about this book}{section.1.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.1}No code}{5}{subsection.1.3.1}\protected@file@percent }
\newlabel{no-code}{{1.3.1}{5}{No code}{subsection.1.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.2}No simulated data}{6}{subsection.1.3.2}\protected@file@percent }
\newlabel{no-simulated-data}{{1.3.2}{6}{No simulated data}{subsection.1.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.3}No model zoo}{6}{subsection.1.3.3}\protected@file@percent }
\newlabel{no-model-zoo}{{1.3.3}{6}{No model zoo}{subsection.1.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.4}No \texttt  {iris} dataset}{7}{subsection.1.3.4}\protected@file@percent }
\newlabel{no-iris-dataset}{{1.3.4}{7}{\texorpdfstring {No \texttt {iris} dataset}{No iris dataset}}{subsection.1.3.4}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Clustering}{9}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lop}{\addvspace {10\p@ }}
\newlabel{sec-clustering}{{2}{9}{Clustering}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}A digression: which birds are red?}{9}{section.2.1}\protected@file@percent }
\newlabel{a-digression-which-birds-are-red}{{2.1}{9}{A digression: which birds are red?}{section.2.1}{}}
\gdef \LT@i {\LT@entry 
    {1}{74.67896pt}\LT@entry 
    {1}{99.74341pt}\LT@entry 
    {1}{124.29347pt}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}The problem: classifying pixels from an image}{11}{section.2.2}\protected@file@percent }
\newlabel{the-problem-classifying-pixels-from-an-image}{{2.2}{11}{The problem: classifying pixels from an image}{section.2.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces Overview of the bands in a Landsat 9 scene. The data from this chapter were downloaded from \href  {https://landsatlook.usgs.gov}{LandsatLook}.}}{11}{table.2.1}\protected@file@percent }
\newmarginnote{note.16.1}{{16}{15945509sp}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces The Landsat 9 data are combined into the ``Natural Color'' image, in which the red, green, and blue bands are mapped to their respective channels (left). The other composites is a 6-5-4 image meant to show differences between urban areas, vegetations, and crops. Note that the true-color composite is slightly distored compared to the colors of the landscape we expect; this is because natural colors are difficult to reproduce accurately.}}{12}{figure.caption.3}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig-kmeans-composites}{{2.1}{12}{\@sidenotes@sidecaption@tof }{figure.caption.3}{}}
\newmarginnote{note.17.1}{{17}{4736286sp}}
\newmarginnote{note.18.1}{{18}{15945509sp}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces The pixels acquired from Landsat 9 exist in a space with many different dimensions (one for each band). Because we are interested in a landscape classification based on water and vegetation data, we use the NDVI, NDMI, and NDWI combinations of bands. These are \emph  {derived} data, and represent the creation of new features from the raw data. Darker colors indicate more pixels in this bin.}}{14}{figure.caption.4}\protected@file@percent }
\newlabel{fig-kmeans-hexbin}{{2.2}{14}{\@sidenotes@sidecaption@tof }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}The theory behind \emph  {k}-means clustering}{15}{section.2.3}\protected@file@percent }
\newlabel{the-theory-behind-k-means-clustering}{{2.3}{15}{\texorpdfstring {The theory behind \emph {k}-means clustering}{The theory behind k-means clustering}}{section.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Inputs and parameters}{15}{subsection.2.3.1}\protected@file@percent }
\newlabel{inputs-and-parameters}{{2.3.1}{15}{Inputs and parameters}{subsection.2.3.1}{}}
\newmarginnote{note.19.1}{{19}{4736286sp}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Assigning instances to classes}{15}{subsection.2.3.2}\protected@file@percent }
\newlabel{assigning-instances-to-classes}{{2.3.2}{15}{Assigning instances to classes}{subsection.2.3.2}{}}
\newmarginnote{note.19.2}{{19}{4736286sp}}
\newlabel{eq-clustering-onepoint}{{2.1}{15}{Assigning instances to classes}{section*.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Optimizing the centroids}{16}{subsection.2.3.3}\protected@file@percent }
\newlabel{optimizing-the-centroids}{{2.3.3}{16}{Optimizing the centroids}{subsection.2.3.3}{}}
\newlabel{eq-clustering-variance}{{2.2}{16}{Optimizing the centroids}{section*.6}{}}
\newlabel{eq-clustering-centroid-update}{{2.3}{16}{Optimizing the centroids}{section*.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.4}Updating the classes}{17}{subsection.2.3.4}\protected@file@percent }
\newlabel{updating-the-classes}{{2.3.4}{17}{Updating the classes}{subsection.2.3.4}{}}
\newmarginnote{note.20.1}{{21}{4736286sp}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.5}Identification of the optimal number of clusters}{17}{subsection.2.3.5}\protected@file@percent }
\newlabel{sec-clustering-optimality}{{2.3.5}{17}{Identification of the optimal number of clusters}{subsection.2.3.5}{}}
\newmarginnote{note.22.1}{{22}{15945509sp}}
\newmarginnote{note.23.1}{{23}{4736286sp}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces caption}}{19}{figure.caption.8}\protected@file@percent }
\newlabel{fig-kmeans-initial-landscape}{{2.3}{19}{\@sidenotes@sidecaption@tof }{figure.caption.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Application: optimal clustering of the satellite image data}{19}{section.2.4}\protected@file@percent }
\newlabel{application-optimal-clustering-of-the-satellite-image-data}{{2.4}{19}{Application: optimal clustering of the satellite image data}{section.2.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Initial run}{19}{subsection.2.4.1}\protected@file@percent }
\newlabel{sec-kmeans-initial}{{2.4.1}{19}{Initial run}{subsection.2.4.1}{}}
\newmarginnote{note.23.2}{{23}{4736286sp}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Optimal number of pixels}{20}{subsection.2.4.2}\protected@file@percent }
\newlabel{optimal-number-of-pixels}{{2.4.2}{20}{Optimal number of pixels}{subsection.2.4.2}{}}
\newmarginnote{note.24.1}{{24}{15945509sp}}
\newmarginnote{note.24.2}{{25}{4736286sp}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Results of running the \emph  {k}-means algorithm ten times for each number of clusters between 3 and 8. The average Davies-Bouldin and cost are reported, as well as the standard deviation. As expected, the total cost decreases with more clusters, but this is not necessarily the sign of a better clustering.}}{21}{figure.caption.9}\protected@file@percent }
\newlabel{fig-kmeans-tuning}{{2.4}{21}{\@sidenotes@sidecaption@tof }{figure.caption.9}{}}
\gdef \LT@ii {\LT@entry 
    {1}{37.4703pt}\LT@entry 
    {1}{37.99532pt}\LT@entry 
    {3}{40.28386pt}\LT@entry 
    {1}{41.1927pt}\LT@entry 
    {1}{35.1927pt}}
\newmarginnote{note.26.1}{{26}{15945509sp}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Results of the landscape clustering with k=5 clusters. This number of clusters gives us a good separation between different groups of pixels, and seems to capture features of the landscape as revealed with the false-color composites.}}{22}{figure.caption.10}\protected@file@percent }
\newlabel{fig-kmeans-optimal-landscape}{{2.5}{22}{\@sidenotes@sidecaption@tof }{figure.caption.10}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2.2}{\ignorespaces Summary of the values for the centers of the optimal clusters found in this image. The cover column gives the percentage of all pixels associated to this class. The clusters are sorted by the NDVI of their centroid.}}{22}{table.caption.11}\protected@file@percent }
\newlabel{tbl-clustering-centers}{{2.2}{22}{Summary of the values for the centers of the optimal clusters found in this image. The cover column gives the percentage of all pixels associated to this class. The clusters are sorted by the NDVI of their centroid}{table.caption.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.3}Clustering with optimal number of classes}{22}{subsection.2.4.3}\protected@file@percent }
\newlabel{clustering-with-optimal-number-of-classes}{{2.4.3}{22}{Clustering with optimal number of classes}{subsection.2.4.3}{}}
\newmarginnote{note.26.2}{{26}{15945509sp}}
\newmarginnote{note.26.3}{{26}{15945509sp}}
\newmarginnote{note.27.1}{{27}{4736286sp}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Visualisation of the clustering output as a function of the NDVI and NDMI values. Note that the limits between the clusters are lines (planes), and that each cluster covers about the same volume in the space of parameters.}}{23}{figure.caption.12}\protected@file@percent }
\newlabel{fig-kmeans-clustering}{{2.6}{23}{\@sidenotes@sidecaption@tof }{figure.caption.12}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Conclusion}{23}{section.2.5}\protected@file@percent }
\newlabel{conclusion}{{2.5}{23}{Conclusion}{section.2.5}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Gradient descent}{25}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lop}{\addvspace {10\p@ }}
\newlabel{sec-gradientdescent}{{3}{25}{Gradient descent}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}A digression: what is a trained model?}{25}{section.3.1}\protected@file@percent }
\newlabel{sec-gradientdescent-trainedmodel}{{3.1}{25}{A digression: what is a trained model?}{section.3.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}The problem: how many interactions in a food web?}{26}{section.3.2}\protected@file@percent }
\newlabel{sec-gradientdescent-problem}{{3.2}{26}{The problem: how many interactions in a food web?}{section.3.2}{}}
